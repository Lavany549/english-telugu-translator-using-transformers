{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_file.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3VCKnl0zL-n",
        "colab_type": "code",
        "outputId": "719626b7-6d3c-492e-f7f6-7c333abb6dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/ML"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ML\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5btkFHJv1hd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_wS-mGRcTxf",
        "colab_type": "code",
        "outputId": "7a431b32-4bf8-4783-f624-0c3d19df38d8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba7c2e59-0e36-481f-9bb4-6de312c41102\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ba7c2e59-0e36-481f-9bb4-6de312c41102\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RubDd3oHcvpT",
        "colab_type": "text"
      },
      "source": [
        "# **Machine Translation using Transformers**\n",
        "\n",
        "\n",
        "\n",
        "The model is based on:\n",
        "\n",
        "\"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. arXiv:1706.03762\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing Translation Data\n",
        "(from Translation_preprocess.py)\n",
        "\n",
        "Function for expanding English contractions\n",
        "source: https://gist.github.com/nealrs/96342d8231b75cf4bb82"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xmhoEoWzevB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from __future__ import division\n",
        "import io\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "\n",
        "#source: https://gist.github.com/nealrs/96342d8231b75cf4bb82\n",
        "cList = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return cList[match.group(0)]\n",
        "    return c_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIq9CmI_6VBF",
        "colab_type": "code",
        "outputId": "b7b20420-7470-4a47-f35b-91ac9112d9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiOp5nJudTHs",
        "colab_type": "text"
      },
      "source": [
        "## Loading Translation Data\n",
        "Splitting the data into eng and tel. eng will contain the list of English lines, and tel will contain the corresponding list of telugu lines.\n",
        "\n",
        "Source of data: http://www.manythings.org/anki/ (downloaded tel-eng) and saved as ben.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLmG1IwO5XNu",
        "colab_type": "code",
        "outputId": "b491fa52-f087-4df2-eded-9965ff4aa515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "filename = 'ben.txt'\n",
        "#Datasource: http://www.manythings.org/anki/\n",
        "    \n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    \n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(expandContractions(s.lower().strip()))\n",
        "    s = re.sub(r\"([.!?,\\\"])\", r\" \", s)\n",
        "    return s\n",
        "    \n",
        "def loaddata(filename):\n",
        "    file = io.open(filename,'r')\n",
        "    eng=[]\n",
        "    beng = []\n",
        "    for line in file.readlines():\n",
        "        lang_pair = line.split('\\t')\n",
        "        lang_pair[0] = normalizeString(lang_pair[0])\n",
        "        lang_pair[1] = normalizeString(lang_pair[1])\n",
        "        eng.append(word_tokenize(lang_pair[0]))\n",
        "        beng.append(word_tokenize(lang_pair[1]))\n",
        "    file.close()\n",
        "    return eng,beng\n",
        "\n",
        "eng,beng = loaddata(filename)\n",
        "#Example:\n",
        "sample = random.randint(0,len(eng))\n",
        "print( \"Example Sample #\"+str(sample)+\":\\n\")\n",
        "string = \"ENGLISH:\"\n",
        "for i in range(0,len(eng[sample])):\n",
        "    string+=\" \"+eng[sample][i]\n",
        "print (string)\n",
        "    \n",
        "string = \"\\nTELUGU:\"\n",
        "for i in range(0,len(beng[sample])):\n",
        "    string+=\" \"+beng[sample][i]\n",
        "print (string)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example Sample #19:\n",
            "\n",
            "ENGLISH: how are you coming\n",
            "\n",
            "BENGALI: నువవు ఎల వసతుననవ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNgFwBN9doWG",
        "colab_type": "text"
      },
      "source": [
        "# Creating separate vocabulary lists for English words and Telugu words\n",
        "The index of vocabulary will represent the numerical representation of the word which is stored at that index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS8oXhms2GX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab_eng=[]\n",
        "vocab_eng.append('<PAD>')\n",
        "vocab_eng.append('<EOS>')\n",
        "\n",
        "vocab_beng=[]\n",
        "vocab_beng.append('<PAD>')\n",
        "vocab_beng.append('<EOS>')\n",
        "\n",
        "#The index of vocab will serve as an integer representation of the word\n",
        "\n",
        "vectorized_eng = []\n",
        "vectorized_beng = []\n",
        "\n",
        "for i in range(len(eng)):\n",
        "    \n",
        "    vectorized_eng_line = []\n",
        "    for word in eng[i]:\n",
        "        if word not in vocab_eng:\n",
        "            vocab_eng.append(word)\n",
        "            vectorized_eng_line.append(vocab_eng.index(word))\n",
        "        else:\n",
        "            vectorized_eng_line.append(vocab_eng.index(word))\n",
        "    vectorized_eng.append(vectorized_eng_line)\n",
        "    \n",
        "    vectorized_beng_line = []\n",
        "    for word in beng[i]:\n",
        "        if word not in vocab_beng:\n",
        "            vocab_beng.append(word)\n",
        "            vectorized_beng_line.append(vocab_beng.index(word))\n",
        "        else:\n",
        "            vectorized_beng_line.append(vocab_beng.index(word))\n",
        "    vectorized_beng.append(vectorized_beng_line)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCHrGKE1d0Kv",
        "colab_type": "text"
      },
      "source": [
        "# Creating training dataset for word2vec embedding\n",
        " if the sentence is \"I am alright\"\n",
        "\n",
        "then for the word 'am', the context words with window size 1 will be \"I\" and \"alright\" i.e [\"I\",\"alright\"]\n",
        "\n",
        "For 'I' the context words will be \"PAD\" and \"am\"\n",
        "\n",
        "For 'alright' the context words will be \"am\" and \"PAD\"\n",
        "\n",
        "PAD represents empty and EOS represents end of sentence.\n",
        "\n",
        "Later lots of pads may be applied after the end of sentence to fit sequence length.\n",
        "\n",
        "So I also added the word PAD with context words being PADs, and PAD and EOS for embedding.\n",
        "\n",
        "\n",
        "\n",
        "In this way, first, from each sentence, I am creating a list of words, and a corresponding list of context words. I am doing the same thing for both English and Telugu lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUGUIcwn7BUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_eng = []\n",
        "contexts_eng = []\n",
        "\n",
        "words_beng = []\n",
        "contexts_beng = []\n",
        "\n",
        "words_eng.append(vocab_eng.index('<PAD>'))\n",
        "contexts_eng.append([vocab_eng.index('<EOS>'),vocab_eng.index('<PAD>')])\n",
        "words_eng.append(vocab_eng.index('<PAD>'))\n",
        "contexts_eng.append([vocab_eng.index('<PAD>'),vocab_eng.index('<PAD>')])\n",
        "\n",
        "words_beng.append(vocab_beng.index('<PAD>'))\n",
        "contexts_beng.append([vocab_beng.index('<EOS>'),vocab_beng.index('<PAD>')])\n",
        "words_beng.append(vocab_beng.index('<PAD>'))\n",
        "contexts_beng.append([vocab_beng.index('<PAD>'),vocab_beng.index('<PAD>')])\n",
        "\n",
        "\n",
        "for i in range(len(vectorized_eng)):\n",
        "    \n",
        "    for j in range(0,len(vectorized_eng[i])):\n",
        "        \n",
        "        context1=0\n",
        "        context2=0\n",
        "        \n",
        "        if j==0:\n",
        "            context1 = vocab_eng.index('<PAD>')\n",
        "            if j!=len(vectorized_eng[i])-1:\n",
        "                context2 = vectorized_eng[i][j+1]\n",
        "        if j==len(vectorized_eng[i])-1:\n",
        "            context2=vocab_eng.index('<EOS>')\n",
        "            if j!=0:\n",
        "                context1 = vectorized_eng[i][j-1]\n",
        "        if j>0 and j<len(vectorized_eng[i])-1:\n",
        "            context1 = vectorized_eng[i][j-1]\n",
        "            context2 = vectorized_eng[i][j+1]\n",
        "        \n",
        "        words_eng.append(vectorized_eng[i][j])\n",
        "        contexts_eng.append([context1,context2])\n",
        "    \n",
        "    rand = random.randint(0,3)\n",
        "    if rand == 1: #reduce the freuency of <EOS> for training data\n",
        "        words_eng.append(vocab_eng.index('<EOS>'))\n",
        "        context1 = vectorized_eng[i][len(vectorized_eng[i])-1]\n",
        "        context2 = vocab_eng.index('<PAD>')\n",
        "        contexts_eng.append([context1,context2])\n",
        "    \n",
        "    for j in range(0,len(vectorized_beng[i])):\n",
        "        \n",
        "        context1=0\n",
        "        context2=0\n",
        "        \n",
        "        if j==0:\n",
        "            context1 = vocab_beng.index('<PAD>')\n",
        "            if j!=len(vectorized_beng[i])-1:\n",
        "                context2 = vectorized_beng[i][j+1]\n",
        "        if j==len(vectorized_beng[i])-1:\n",
        "            context2=vocab_beng.index('<EOS>')\n",
        "            if j!=0:\n",
        "                context1 = vectorized_beng[i][j-1]\n",
        "        if j>0 and j<len(vectorized_beng[i])-1:\n",
        "            context1 = vectorized_beng[i][j-1]\n",
        "            context2 = vectorized_beng[i][j+1]\n",
        "        \n",
        "        words_beng.append(vectorized_beng[i][j])\n",
        "        contexts_beng.append([context1,context2])\n",
        "    \n",
        "    rand = random.randint(0,3)\n",
        "    if rand == 1: #reduce the freuency of <EOS> for training data\n",
        "        words_beng.append(vocab_beng.index('<EOS>'))\n",
        "        context1 = vectorized_beng[i][len(vectorized_beng[i])-1]\n",
        "        context2 = vocab_beng.index('<PAD>')\n",
        "        contexts_beng.append([context1,context2])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmyaFgkQePf3",
        "colab_type": "text"
      },
      "source": [
        "If word = \"am\" and context = [\"I\",\"alright\"], then, from this data I will create the following samples:\n",
        "\n",
        "input = \"am\" output = \"I\" and input = \"am\" label = \"alright\"\n",
        "\n",
        "Like this I will construct a list of all training inputs (words) and training outputs\\labels (context words)\n",
        "\n",
        "embd_inputs_eng will contain all the English training inputs. embd_labels_eng will contain all the English training labels.\n",
        "\n",
        "embd_inputs_beng will contain all the Bengali training inputs. embd_labels_beng will contain all the Bengali training labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0bdGNvd7L2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embd_inputs_eng = []\n",
        "embd_labels_eng = []\n",
        "for i in range(len(contexts_eng)):\n",
        "    for context in contexts_eng[i]:\n",
        "        embd_inputs_eng.append(words_eng[i])\n",
        "        embd_labels_eng.append(context)\n",
        "embd_inputs_eng = np.asarray(embd_inputs_eng,np.int32)\n",
        "embd_labels_eng = np.asarray(embd_labels_eng,np.int32)\n",
        "\n",
        "embd_inputs_beng = []\n",
        "embd_labels_beng = []\n",
        "for i in range(len(contexts_beng)):\n",
        "    for context in contexts_beng[i]:\n",
        "        embd_inputs_beng.append(words_beng[i])\n",
        "        embd_labels_beng.append(context)\n",
        "embd_inputs_beng = np.asarray(embd_inputs_beng,np.int32)\n",
        "embd_labels_beng = np.asarray(embd_labels_beng,np.int32)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otxr9RbRebrt",
        "colab_type": "text"
      },
      "source": [
        "# Function for generating mini-batches from the total training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHYQK4mq7Sjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "def generate_batch(inputs,labels,batch_size):\n",
        "    rand = random.sample(list((np.arange(len(inputs)))),batch_size)\n",
        "    batch_inputs=[]\n",
        "    batch_labels=[]\n",
        "    for i in range(batch_size):\n",
        "        batch_inputs.append(inputs[int(rand[i])])\n",
        "        batch_labels.append(labels[int(rand[i])])\n",
        "    batch_inputs = np.asarray(batch_inputs,np.int32)\n",
        "    batch_labels = np.asarray(batch_labels,np.int32)\n",
        "    return batch_inputs,batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REEwzHct7bKr",
        "colab_type": "text"
      },
      "source": [
        "# Preparing for word2vec embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs4WK33G7cGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "#https://www.tensorflow.org/tutorials/word2vec\n",
        "embedding_size = 256\n",
        "vocabulary_size_eng = len(vocab_eng)\n",
        "vocabulary_size_beng = len(vocab_beng)\n",
        "\n",
        "# Placeholders for inputs\n",
        "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsTGEWZuelZi",
        "colab_type": "text"
      },
      "source": [
        "# Training for word2vec embedding (For English words)\n",
        "See: https://www.tensorflow.org/tutorials/word2vec\n",
        "\n",
        "for details of word2vec and code description.\n",
        "\n",
        "Most of the word2vec code used here are from the Tensorflow tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFRLJND67pSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_eng = tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size_eng, embedding_size], -1.0, 1.0))\n",
        "\n",
        "nce_weights_eng = tf.Variable(\n",
        "  tf.truncated_normal([vocabulary_size_eng, embedding_size],\n",
        "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
        "nce_biases_eng = tf.Variable(tf.zeros([vocabulary_size_eng]))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4OLdWn17v3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_eng = tf.nn.embedding_lookup(embeddings_eng, train_inputs)\n",
        "\n",
        "# Compute the NCE loss, using a sample of the negative labels each time.\n",
        "loss = tf.reduce_mean(\n",
        "  tf.nn.nce_loss(weights=nce_weights_eng,\n",
        "                 biases=nce_biases_eng,\n",
        "                 labels=train_labels,\n",
        "                 inputs=embed_eng,\n",
        "                 num_sampled=10, \n",
        "                 num_classes=vocabulary_size_eng)) #num_sampled = no. of negative samples\n",
        "\n",
        "# We use the SGD optimizer.\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PAqeikQ771D",
        "colab_type": "code",
        "outputId": "8643b858-43b9-4b00-b29b-3bc76f7059bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    convergence_threshold = 0.5\n",
        "    training_iters = 500*(int((len(embd_inputs_eng))/batch_size))\n",
        "    step=0\n",
        "    n=5\n",
        "    last_n_losses = np.zeros((n),np.float32)\n",
        "    \n",
        "    \n",
        "    while step<training_iters:\n",
        "        \n",
        "        batch_inputs,batch_labels = generate_batch(embd_inputs_eng,embd_labels_eng,batch_size)\n",
        "        \n",
        "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels.reshape((-1,1))}\n",
        "        \n",
        "        _, np_embedding_eng, cur_loss = sess.run([optimizer, embeddings_eng, loss], feed_dict=feed_dict)\n",
        "        \n",
        "        print (\"Iter \"+str(step)+\", loss = \"+str(cur_loss))\n",
        "        \n",
        "        last_n_losses[step%n]=cur_loss\n",
        "        \n",
        "        if step>=n:\n",
        "            if np.mean(last_n_losses)<=convergence_threshold:\n",
        "                break\n",
        "        step+=1\n",
        "                \n",
        "print (\"\\nOptimization Finished\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iter 1504, loss = 1.5076728\n",
            "Iter 1505, loss = 0.97887653\n",
            "Iter 1506, loss = 0.91611886\n",
            "Iter 1507, loss = 0.9277456\n",
            "Iter 1508, loss = 1.1638627\n",
            "Iter 1509, loss = 0.8137457\n",
            "Iter 1510, loss = 1.3825965\n",
            "Iter 1511, loss = 0.9424311\n",
            "Iter 1512, loss = 1.5509691\n",
            "Iter 1513, loss = 1.1341801\n",
            "Iter 1514, loss = 1.1295986\n",
            "Iter 1515, loss = 1.2887391\n",
            "Iter 1516, loss = 1.0422226\n",
            "Iter 1517, loss = 0.98039025\n",
            "Iter 1518, loss = 0.9345323\n",
            "Iter 1519, loss = 1.0024459\n",
            "Iter 1520, loss = 1.2359478\n",
            "Iter 1521, loss = 1.1283454\n",
            "Iter 1522, loss = 1.4020624\n",
            "Iter 1523, loss = 1.6263297\n",
            "Iter 1524, loss = 0.8998445\n",
            "Iter 1525, loss = 0.9221674\n",
            "Iter 1526, loss = 0.85447073\n",
            "Iter 1527, loss = 1.0194697\n",
            "Iter 1528, loss = 0.8322352\n",
            "Iter 1529, loss = 1.1804745\n",
            "Iter 1530, loss = 1.2228968\n",
            "Iter 1531, loss = 0.99962485\n",
            "Iter 1532, loss = 1.2073774\n",
            "Iter 1533, loss = 1.2019453\n",
            "Iter 1534, loss = 0.8767539\n",
            "Iter 1535, loss = 1.0304575\n",
            "Iter 1536, loss = 0.99062884\n",
            "Iter 1537, loss = 1.1118383\n",
            "Iter 1538, loss = 1.072371\n",
            "Iter 1539, loss = 1.397444\n",
            "Iter 1540, loss = 1.0238036\n",
            "Iter 1541, loss = 1.1798052\n",
            "Iter 1542, loss = 0.92979145\n",
            "Iter 1543, loss = 1.059752\n",
            "Iter 1544, loss = 1.6195226\n",
            "Iter 1545, loss = 0.97947717\n",
            "Iter 1546, loss = 1.2026417\n",
            "Iter 1547, loss = 1.5851518\n",
            "Iter 1548, loss = 0.8183021\n",
            "Iter 1549, loss = 1.5813128\n",
            "Iter 1550, loss = 1.3234164\n",
            "Iter 1551, loss = 1.2818574\n",
            "Iter 1552, loss = 0.9161686\n",
            "Iter 1553, loss = 1.133573\n",
            "Iter 1554, loss = 1.4504427\n",
            "Iter 1555, loss = 1.1675701\n",
            "Iter 1556, loss = 0.8587552\n",
            "Iter 1557, loss = 1.208269\n",
            "Iter 1558, loss = 1.2409041\n",
            "Iter 1559, loss = 1.1627339\n",
            "Iter 1560, loss = 0.8452339\n",
            "Iter 1561, loss = 1.3686349\n",
            "Iter 1562, loss = 1.0782692\n",
            "Iter 1563, loss = 1.0531414\n",
            "Iter 1564, loss = 1.3529959\n",
            "Iter 1565, loss = 1.1625473\n",
            "Iter 1566, loss = 1.0216599\n",
            "Iter 1567, loss = 1.2696345\n",
            "Iter 1568, loss = 1.3104379\n",
            "Iter 1569, loss = 1.0638397\n",
            "Iter 1570, loss = 0.8471587\n",
            "Iter 1571, loss = 1.1512479\n",
            "Iter 1572, loss = 1.1766834\n",
            "Iter 1573, loss = 1.1349279\n",
            "Iter 1574, loss = 1.5205197\n",
            "Iter 1575, loss = 1.3805021\n",
            "Iter 1576, loss = 1.0896683\n",
            "Iter 1577, loss = 1.3600161\n",
            "Iter 1578, loss = 0.96101034\n",
            "Iter 1579, loss = 1.0630145\n",
            "Iter 1580, loss = 0.7680608\n",
            "Iter 1581, loss = 1.2157693\n",
            "Iter 1582, loss = 1.1620085\n",
            "Iter 1583, loss = 0.922593\n",
            "Iter 1584, loss = 1.1631339\n",
            "Iter 1585, loss = 1.0526491\n",
            "Iter 1586, loss = 1.3488164\n",
            "Iter 1587, loss = 1.093199\n",
            "Iter 1588, loss = 1.2419167\n",
            "Iter 1589, loss = 0.8378077\n",
            "Iter 1590, loss = 0.9255137\n",
            "Iter 1591, loss = 1.1522809\n",
            "Iter 1592, loss = 1.1672144\n",
            "Iter 1593, loss = 0.68484944\n",
            "Iter 1594, loss = 1.4901184\n",
            "Iter 1595, loss = 1.5946862\n",
            "Iter 1596, loss = 0.94195205\n",
            "Iter 1597, loss = 1.7869978\n",
            "Iter 1598, loss = 1.2021761\n",
            "Iter 1599, loss = 1.54648\n",
            "Iter 1600, loss = 1.0276315\n",
            "Iter 1601, loss = 1.0909429\n",
            "Iter 1602, loss = 0.855381\n",
            "Iter 1603, loss = 1.0848992\n",
            "Iter 1604, loss = 1.3837763\n",
            "Iter 1605, loss = 1.3859031\n",
            "Iter 1606, loss = 1.2662947\n",
            "Iter 1607, loss = 1.1226102\n",
            "Iter 1608, loss = 1.2255312\n",
            "Iter 1609, loss = 1.404172\n",
            "Iter 1610, loss = 1.3483342\n",
            "Iter 1611, loss = 1.2945435\n",
            "Iter 1612, loss = 1.0425675\n",
            "Iter 1613, loss = 1.5359153\n",
            "Iter 1614, loss = 1.0127095\n",
            "Iter 1615, loss = 0.999781\n",
            "Iter 1616, loss = 0.91933364\n",
            "Iter 1617, loss = 1.09675\n",
            "Iter 1618, loss = 0.92336535\n",
            "Iter 1619, loss = 0.88686204\n",
            "Iter 1620, loss = 1.3005145\n",
            "Iter 1621, loss = 0.8072055\n",
            "Iter 1622, loss = 0.8592906\n",
            "Iter 1623, loss = 1.111397\n",
            "Iter 1624, loss = 1.6975265\n",
            "Iter 1625, loss = 1.3851928\n",
            "Iter 1626, loss = 1.221221\n",
            "Iter 1627, loss = 1.0337489\n",
            "Iter 1628, loss = 1.1642714\n",
            "Iter 1629, loss = 1.1588318\n",
            "Iter 1630, loss = 1.1453168\n",
            "Iter 1631, loss = 1.4165385\n",
            "Iter 1632, loss = 1.4373665\n",
            "Iter 1633, loss = 1.270489\n",
            "Iter 1634, loss = 0.9163407\n",
            "Iter 1635, loss = 1.0961714\n",
            "Iter 1636, loss = 1.0188\n",
            "Iter 1637, loss = 1.144813\n",
            "Iter 1638, loss = 1.0156152\n",
            "Iter 1639, loss = 0.99755394\n",
            "Iter 1640, loss = 1.014435\n",
            "Iter 1641, loss = 1.2057719\n",
            "Iter 1642, loss = 1.9572165\n",
            "Iter 1643, loss = 1.2512045\n",
            "Iter 1644, loss = 1.0774753\n",
            "Iter 1645, loss = 1.126508\n",
            "Iter 1646, loss = 1.2589529\n",
            "Iter 1647, loss = 1.1405509\n",
            "Iter 1648, loss = 1.1330462\n",
            "Iter 1649, loss = 1.2277563\n",
            "Iter 1650, loss = 0.9985997\n",
            "Iter 1651, loss = 0.8777148\n",
            "Iter 1652, loss = 1.3866527\n",
            "Iter 1653, loss = 1.3130691\n",
            "Iter 1654, loss = 1.2739583\n",
            "Iter 1655, loss = 1.3044963\n",
            "Iter 1656, loss = 1.2957464\n",
            "Iter 1657, loss = 1.1596318\n",
            "Iter 1658, loss = 0.94246435\n",
            "Iter 1659, loss = 1.1253823\n",
            "Iter 1660, loss = 1.0680616\n",
            "Iter 1661, loss = 0.97692704\n",
            "Iter 1662, loss = 1.0278487\n",
            "Iter 1663, loss = 1.1955652\n",
            "Iter 1664, loss = 1.0171492\n",
            "Iter 1665, loss = 1.2864211\n",
            "Iter 1666, loss = 0.86670196\n",
            "Iter 1667, loss = 1.0194108\n",
            "Iter 1668, loss = 1.2207363\n",
            "Iter 1669, loss = 1.3740544\n",
            "Iter 1670, loss = 1.3372984\n",
            "Iter 1671, loss = 1.1091285\n",
            "Iter 1672, loss = 1.2780156\n",
            "Iter 1673, loss = 1.0359251\n",
            "Iter 1674, loss = 1.0405228\n",
            "Iter 1675, loss = 1.1238921\n",
            "Iter 1676, loss = 1.0172665\n",
            "Iter 1677, loss = 1.0447302\n",
            "Iter 1678, loss = 1.0983825\n",
            "Iter 1679, loss = 1.3075144\n",
            "Iter 1680, loss = 0.8904189\n",
            "Iter 1681, loss = 1.8378986\n",
            "Iter 1682, loss = 1.1123016\n",
            "Iter 1683, loss = 1.1939895\n",
            "Iter 1684, loss = 1.3037436\n",
            "Iter 1685, loss = 1.0855328\n",
            "Iter 1686, loss = 0.97461826\n",
            "Iter 1687, loss = 1.3503373\n",
            "Iter 1688, loss = 0.85887027\n",
            "Iter 1689, loss = 1.0496101\n",
            "Iter 1690, loss = 1.0748451\n",
            "Iter 1691, loss = 1.1163218\n",
            "Iter 1692, loss = 1.1368078\n",
            "Iter 1693, loss = 1.0395404\n",
            "Iter 1694, loss = 1.0179963\n",
            "Iter 1695, loss = 1.4139652\n",
            "Iter 1696, loss = 1.2215288\n",
            "Iter 1697, loss = 1.2059615\n",
            "Iter 1698, loss = 0.8258381\n",
            "Iter 1699, loss = 0.9137926\n",
            "Iter 1700, loss = 1.201112\n",
            "Iter 1701, loss = 1.0020151\n",
            "Iter 1702, loss = 1.2912645\n",
            "Iter 1703, loss = 1.1824175\n",
            "Iter 1704, loss = 1.1444168\n",
            "Iter 1705, loss = 1.1676145\n",
            "Iter 1706, loss = 1.0873665\n",
            "Iter 1707, loss = 1.3329678\n",
            "Iter 1708, loss = 1.3006887\n",
            "Iter 1709, loss = 0.9268783\n",
            "Iter 1710, loss = 1.0164678\n",
            "Iter 1711, loss = 1.2884276\n",
            "Iter 1712, loss = 1.270266\n",
            "Iter 1713, loss = 0.8028289\n",
            "Iter 1714, loss = 1.4865091\n",
            "Iter 1715, loss = 1.2533913\n",
            "Iter 1716, loss = 0.99179876\n",
            "Iter 1717, loss = 1.1484203\n",
            "Iter 1718, loss = 1.2304262\n",
            "Iter 1719, loss = 1.005552\n",
            "Iter 1720, loss = 0.86818004\n",
            "Iter 1721, loss = 1.1909823\n",
            "Iter 1722, loss = 0.98919207\n",
            "Iter 1723, loss = 1.1827797\n",
            "Iter 1724, loss = 1.0548576\n",
            "Iter 1725, loss = 0.8763707\n",
            "Iter 1726, loss = 0.8261238\n",
            "Iter 1727, loss = 1.037128\n",
            "Iter 1728, loss = 1.6760561\n",
            "Iter 1729, loss = 1.3182063\n",
            "Iter 1730, loss = 1.2153778\n",
            "Iter 1731, loss = 0.80138683\n",
            "Iter 1732, loss = 1.0627835\n",
            "Iter 1733, loss = 0.8917818\n",
            "Iter 1734, loss = 1.0952373\n",
            "Iter 1735, loss = 1.281045\n",
            "Iter 1736, loss = 1.2116468\n",
            "Iter 1737, loss = 1.1408913\n",
            "Iter 1738, loss = 1.0073273\n",
            "Iter 1739, loss = 1.1366031\n",
            "Iter 1740, loss = 1.2036153\n",
            "Iter 1741, loss = 1.4302111\n",
            "Iter 1742, loss = 1.5853226\n",
            "Iter 1743, loss = 1.5906065\n",
            "Iter 1744, loss = 1.3442907\n",
            "Iter 1745, loss = 1.0875287\n",
            "Iter 1746, loss = 1.0277274\n",
            "Iter 1747, loss = 1.2712026\n",
            "Iter 1748, loss = 0.79293835\n",
            "Iter 1749, loss = 0.92850846\n",
            "Iter 1750, loss = 1.6240256\n",
            "Iter 1751, loss = 0.7995088\n",
            "Iter 1752, loss = 0.81647044\n",
            "Iter 1753, loss = 1.2641723\n",
            "Iter 1754, loss = 1.081614\n",
            "Iter 1755, loss = 1.1913644\n",
            "Iter 1756, loss = 1.4069529\n",
            "Iter 1757, loss = 1.2769486\n",
            "Iter 1758, loss = 1.2230077\n",
            "Iter 1759, loss = 1.034265\n",
            "Iter 1760, loss = 1.0656757\n",
            "Iter 1761, loss = 1.0154366\n",
            "Iter 1762, loss = 1.0756226\n",
            "Iter 1763, loss = 1.0795481\n",
            "Iter 1764, loss = 0.9828272\n",
            "Iter 1765, loss = 0.8813516\n",
            "Iter 1766, loss = 0.82452583\n",
            "Iter 1767, loss = 1.1226698\n",
            "Iter 1768, loss = 0.78961056\n",
            "Iter 1769, loss = 1.3922108\n",
            "Iter 1770, loss = 1.4926233\n",
            "Iter 1771, loss = 1.3047845\n",
            "Iter 1772, loss = 1.3779457\n",
            "Iter 1773, loss = 1.3470314\n",
            "Iter 1774, loss = 0.97513294\n",
            "Iter 1775, loss = 1.0179632\n",
            "Iter 1776, loss = 1.1008422\n",
            "Iter 1777, loss = 0.90388095\n",
            "Iter 1778, loss = 1.1971978\n",
            "Iter 1779, loss = 1.0061721\n",
            "Iter 1780, loss = 1.4608004\n",
            "Iter 1781, loss = 1.2786695\n",
            "Iter 1782, loss = 0.9563336\n",
            "Iter 1783, loss = 1.2939136\n",
            "Iter 1784, loss = 1.0000349\n",
            "Iter 1785, loss = 1.0076745\n",
            "Iter 1786, loss = 0.82052284\n",
            "Iter 1787, loss = 1.3218724\n",
            "Iter 1788, loss = 1.3237627\n",
            "Iter 1789, loss = 0.7608428\n",
            "Iter 1790, loss = 1.2845404\n",
            "Iter 1791, loss = 1.3863233\n",
            "Iter 1792, loss = 1.0117614\n",
            "Iter 1793, loss = 1.2325583\n",
            "Iter 1794, loss = 1.3380761\n",
            "Iter 1795, loss = 1.0837487\n",
            "Iter 1796, loss = 0.8878433\n",
            "Iter 1797, loss = 1.2587222\n",
            "Iter 1798, loss = 0.96098065\n",
            "Iter 1799, loss = 1.0695878\n",
            "Iter 1800, loss = 1.0249455\n",
            "Iter 1801, loss = 0.94517326\n",
            "Iter 1802, loss = 0.9061147\n",
            "Iter 1803, loss = 1.1330053\n",
            "Iter 1804, loss = 1.4560759\n",
            "Iter 1805, loss = 1.7594103\n",
            "Iter 1806, loss = 1.1691811\n",
            "Iter 1807, loss = 1.3042256\n",
            "Iter 1808, loss = 1.0748925\n",
            "Iter 1809, loss = 1.6343879\n",
            "Iter 1810, loss = 1.4490187\n",
            "Iter 1811, loss = 0.86686504\n",
            "Iter 1812, loss = 1.0696877\n",
            "Iter 1813, loss = 1.3406751\n",
            "Iter 1814, loss = 1.2038625\n",
            "Iter 1815, loss = 1.1232858\n",
            "Iter 1816, loss = 1.0812892\n",
            "Iter 1817, loss = 0.99565756\n",
            "Iter 1818, loss = 1.233127\n",
            "Iter 1819, loss = 1.2374753\n",
            "Iter 1820, loss = 1.1815094\n",
            "Iter 1821, loss = 1.1461656\n",
            "Iter 1822, loss = 0.898497\n",
            "Iter 1823, loss = 0.9175741\n",
            "Iter 1824, loss = 0.8760317\n",
            "Iter 1825, loss = 1.05237\n",
            "Iter 1826, loss = 1.1910957\n",
            "Iter 1827, loss = 1.2186079\n",
            "Iter 1828, loss = 0.8962819\n",
            "Iter 1829, loss = 1.0642571\n",
            "Iter 1830, loss = 1.0048425\n",
            "Iter 1831, loss = 0.9899958\n",
            "Iter 1832, loss = 1.1125755\n",
            "Iter 1833, loss = 1.1181691\n",
            "Iter 1834, loss = 0.89824\n",
            "Iter 1835, loss = 1.245862\n",
            "Iter 1836, loss = 1.0581188\n",
            "Iter 1837, loss = 1.1708729\n",
            "Iter 1838, loss = 0.8652336\n",
            "Iter 1839, loss = 1.4069759\n",
            "Iter 1840, loss = 1.2250888\n",
            "Iter 1841, loss = 1.0148602\n",
            "Iter 1842, loss = 1.2917755\n",
            "Iter 1843, loss = 1.2068713\n",
            "Iter 1844, loss = 1.4623435\n",
            "Iter 1845, loss = 1.0949447\n",
            "Iter 1846, loss = 1.0730397\n",
            "Iter 1847, loss = 0.9778798\n",
            "Iter 1848, loss = 1.0924941\n",
            "Iter 1849, loss = 1.124562\n",
            "Iter 1850, loss = 0.9240111\n",
            "Iter 1851, loss = 1.1222107\n",
            "Iter 1852, loss = 1.0667698\n",
            "Iter 1853, loss = 1.3126683\n",
            "Iter 1854, loss = 0.7830106\n",
            "Iter 1855, loss = 1.1519941\n",
            "Iter 1856, loss = 1.9999914\n",
            "Iter 1857, loss = 0.8876922\n",
            "Iter 1858, loss = 1.101408\n",
            "Iter 1859, loss = 1.6397722\n",
            "Iter 1860, loss = 1.1439798\n",
            "Iter 1861, loss = 1.6258001\n",
            "Iter 1862, loss = 1.0984412\n",
            "Iter 1863, loss = 1.1174213\n",
            "Iter 1864, loss = 1.3213544\n",
            "Iter 1865, loss = 1.1187148\n",
            "Iter 1866, loss = 1.0269395\n",
            "Iter 1867, loss = 1.1453056\n",
            "Iter 1868, loss = 1.1398134\n",
            "Iter 1869, loss = 1.4788611\n",
            "Iter 1870, loss = 1.1756868\n",
            "Iter 1871, loss = 1.0485307\n",
            "Iter 1872, loss = 1.0251987\n",
            "Iter 1873, loss = 1.0426534\n",
            "Iter 1874, loss = 2.0528016\n",
            "Iter 1875, loss = 1.1112347\n",
            "Iter 1876, loss = 1.4770333\n",
            "Iter 1877, loss = 1.1443512\n",
            "Iter 1878, loss = 0.8712493\n",
            "Iter 1879, loss = 1.1662475\n",
            "Iter 1880, loss = 1.052079\n",
            "Iter 1881, loss = 1.1474853\n",
            "Iter 1882, loss = 0.85468376\n",
            "Iter 1883, loss = 0.91578233\n",
            "Iter 1884, loss = 1.1587434\n",
            "Iter 1885, loss = 1.1907294\n",
            "Iter 1886, loss = 1.0507451\n",
            "Iter 1887, loss = 1.2572461\n",
            "Iter 1888, loss = 0.95646286\n",
            "Iter 1889, loss = 0.9395135\n",
            "Iter 1890, loss = 1.0093427\n",
            "Iter 1891, loss = 1.0091219\n",
            "Iter 1892, loss = 1.5663614\n",
            "Iter 1893, loss = 0.9827143\n",
            "Iter 1894, loss = 0.9002103\n",
            "Iter 1895, loss = 1.1831411\n",
            "Iter 1896, loss = 1.096257\n",
            "Iter 1897, loss = 1.2466652\n",
            "Iter 1898, loss = 1.1760383\n",
            "Iter 1899, loss = 1.0076945\n",
            "Iter 1900, loss = 1.595682\n",
            "Iter 1901, loss = 1.0905143\n",
            "Iter 1902, loss = 1.3013484\n",
            "Iter 1903, loss = 0.9645889\n",
            "Iter 1904, loss = 1.1655648\n",
            "Iter 1905, loss = 1.0896142\n",
            "Iter 1906, loss = 1.1247755\n",
            "Iter 1907, loss = 1.2226441\n",
            "Iter 1908, loss = 0.9189209\n",
            "Iter 1909, loss = 1.045028\n",
            "Iter 1910, loss = 1.0304687\n",
            "Iter 1911, loss = 1.4456306\n",
            "Iter 1912, loss = 1.1538677\n",
            "Iter 1913, loss = 1.186241\n",
            "Iter 1914, loss = 1.0297216\n",
            "Iter 1915, loss = 0.8259667\n",
            "Iter 1916, loss = 1.0234996\n",
            "Iter 1917, loss = 0.94324195\n",
            "Iter 1918, loss = 1.0426702\n",
            "Iter 1919, loss = 1.3929869\n",
            "Iter 1920, loss = 1.3018506\n",
            "Iter 1921, loss = 1.0597453\n",
            "Iter 1922, loss = 0.82318276\n",
            "Iter 1923, loss = 1.38982\n",
            "Iter 1924, loss = 1.0475332\n",
            "Iter 1925, loss = 0.8549043\n",
            "Iter 1926, loss = 1.9652212\n",
            "Iter 1927, loss = 1.2643434\n",
            "Iter 1928, loss = 1.1331475\n",
            "Iter 1929, loss = 1.1804924\n",
            "Iter 1930, loss = 1.4655445\n",
            "Iter 1931, loss = 1.0262985\n",
            "Iter 1932, loss = 0.9174009\n",
            "Iter 1933, loss = 1.3970679\n",
            "Iter 1934, loss = 1.0199447\n",
            "Iter 1935, loss = 1.3155818\n",
            "Iter 1936, loss = 1.2437676\n",
            "Iter 1937, loss = 0.92219007\n",
            "Iter 1938, loss = 1.0196894\n",
            "Iter 1939, loss = 1.1895045\n",
            "Iter 1940, loss = 1.2011029\n",
            "Iter 1941, loss = 1.0671507\n",
            "Iter 1942, loss = 0.7746077\n",
            "Iter 1943, loss = 1.0995862\n",
            "Iter 1944, loss = 1.1335151\n",
            "Iter 1945, loss = 1.1673014\n",
            "Iter 1946, loss = 1.2483611\n",
            "Iter 1947, loss = 0.80259293\n",
            "Iter 1948, loss = 1.313578\n",
            "Iter 1949, loss = 1.2945642\n",
            "Iter 1950, loss = 1.2847208\n",
            "Iter 1951, loss = 1.0416094\n",
            "Iter 1952, loss = 1.0099537\n",
            "Iter 1953, loss = 0.9458935\n",
            "Iter 1954, loss = 1.0737597\n",
            "Iter 1955, loss = 0.9651655\n",
            "Iter 1956, loss = 1.3246455\n",
            "Iter 1957, loss = 0.84905183\n",
            "Iter 1958, loss = 1.0482929\n",
            "Iter 1959, loss = 1.046637\n",
            "Iter 1960, loss = 1.0515648\n",
            "Iter 1961, loss = 1.2553482\n",
            "Iter 1962, loss = 1.5137885\n",
            "Iter 1963, loss = 0.93853015\n",
            "Iter 1964, loss = 1.0798638\n",
            "Iter 1965, loss = 1.2285388\n",
            "Iter 1966, loss = 1.3327043\n",
            "Iter 1967, loss = 1.0923725\n",
            "Iter 1968, loss = 1.1543936\n",
            "Iter 1969, loss = 1.7448783\n",
            "Iter 1970, loss = 1.2838867\n",
            "Iter 1971, loss = 1.1427193\n",
            "Iter 1972, loss = 1.034079\n",
            "Iter 1973, loss = 1.6452622\n",
            "Iter 1974, loss = 0.9949564\n",
            "Iter 1975, loss = 1.3013841\n",
            "Iter 1976, loss = 0.8639407\n",
            "Iter 1977, loss = 1.2877543\n",
            "Iter 1978, loss = 1.8591847\n",
            "Iter 1979, loss = 1.2135985\n",
            "Iter 1980, loss = 0.9924784\n",
            "Iter 1981, loss = 1.2442658\n",
            "Iter 1982, loss = 1.1043134\n",
            "Iter 1983, loss = 1.0183177\n",
            "Iter 1984, loss = 0.84962356\n",
            "Iter 1985, loss = 1.7607551\n",
            "Iter 1986, loss = 1.0562193\n",
            "Iter 1987, loss = 1.132929\n",
            "Iter 1988, loss = 1.2272643\n",
            "Iter 1989, loss = 0.9406222\n",
            "Iter 1990, loss = 0.8940126\n",
            "Iter 1991, loss = 1.1727107\n",
            "Iter 1992, loss = 0.98592794\n",
            "Iter 1993, loss = 1.1963217\n",
            "Iter 1994, loss = 0.7821036\n",
            "Iter 1995, loss = 1.4491279\n",
            "Iter 1996, loss = 1.0814886\n",
            "Iter 1997, loss = 0.9973097\n",
            "Iter 1998, loss = 1.3626668\n",
            "Iter 1999, loss = 1.4963417\n",
            "Iter 2000, loss = 1.336782\n",
            "Iter 2001, loss = 1.0135517\n",
            "Iter 2002, loss = 0.9149034\n",
            "Iter 2003, loss = 1.1359134\n",
            "Iter 2004, loss = 1.2188874\n",
            "Iter 2005, loss = 0.98749006\n",
            "Iter 2006, loss = 0.7607955\n",
            "Iter 2007, loss = 0.93252456\n",
            "Iter 2008, loss = 1.0496049\n",
            "Iter 2009, loss = 0.9180683\n",
            "Iter 2010, loss = 1.2002951\n",
            "Iter 2011, loss = 1.1252761\n",
            "Iter 2012, loss = 1.0610709\n",
            "Iter 2013, loss = 1.1398613\n",
            "Iter 2014, loss = 1.4233954\n",
            "Iter 2015, loss = 1.4563847\n",
            "Iter 2016, loss = 1.4173763\n",
            "Iter 2017, loss = 1.3757145\n",
            "Iter 2018, loss = 1.070193\n",
            "Iter 2019, loss = 1.1651764\n",
            "Iter 2020, loss = 1.2487568\n",
            "Iter 2021, loss = 0.8871768\n",
            "Iter 2022, loss = 1.3101138\n",
            "Iter 2023, loss = 1.4824333\n",
            "Iter 2024, loss = 1.0549672\n",
            "Iter 2025, loss = 0.893422\n",
            "Iter 2026, loss = 0.94253004\n",
            "Iter 2027, loss = 0.92018455\n",
            "Iter 2028, loss = 1.0933588\n",
            "Iter 2029, loss = 1.0530541\n",
            "Iter 2030, loss = 0.92114687\n",
            "Iter 2031, loss = 1.4770671\n",
            "Iter 2032, loss = 0.9601093\n",
            "Iter 2033, loss = 1.0170468\n",
            "Iter 2034, loss = 0.87044215\n",
            "Iter 2035, loss = 1.4881856\n",
            "Iter 2036, loss = 1.3005282\n",
            "Iter 2037, loss = 1.593214\n",
            "Iter 2038, loss = 1.1123455\n",
            "Iter 2039, loss = 0.8900187\n",
            "Iter 2040, loss = 1.0282689\n",
            "Iter 2041, loss = 1.0763477\n",
            "Iter 2042, loss = 1.4221073\n",
            "Iter 2043, loss = 1.2496607\n",
            "Iter 2044, loss = 1.5872897\n",
            "Iter 2045, loss = 0.9308416\n",
            "Iter 2046, loss = 1.1759399\n",
            "Iter 2047, loss = 1.2844248\n",
            "Iter 2048, loss = 1.1540872\n",
            "Iter 2049, loss = 1.1728833\n",
            "Iter 2050, loss = 1.2044473\n",
            "Iter 2051, loss = 0.9768826\n",
            "Iter 2052, loss = 0.74904966\n",
            "Iter 2053, loss = 0.9790986\n",
            "Iter 2054, loss = 1.4184141\n",
            "Iter 2055, loss = 0.926412\n",
            "Iter 2056, loss = 0.9116259\n",
            "Iter 2057, loss = 1.1052772\n",
            "Iter 2058, loss = 1.0288994\n",
            "Iter 2059, loss = 0.98702884\n",
            "Iter 2060, loss = 1.2167566\n",
            "Iter 2061, loss = 1.3965156\n",
            "Iter 2062, loss = 0.98638856\n",
            "Iter 2063, loss = 1.0347157\n",
            "Iter 2064, loss = 0.92104745\n",
            "Iter 2065, loss = 1.3754641\n",
            "Iter 2066, loss = 0.95661974\n",
            "Iter 2067, loss = 1.2285427\n",
            "Iter 2068, loss = 1.1553303\n",
            "Iter 2069, loss = 0.97681266\n",
            "Iter 2070, loss = 0.93492174\n",
            "Iter 2071, loss = 1.0440731\n",
            "Iter 2072, loss = 1.3521508\n",
            "Iter 2073, loss = 1.346602\n",
            "Iter 2074, loss = 1.0108514\n",
            "Iter 2075, loss = 1.2400649\n",
            "Iter 2076, loss = 1.1041012\n",
            "Iter 2077, loss = 0.88519174\n",
            "Iter 2078, loss = 1.5243361\n",
            "Iter 2079, loss = 0.93695503\n",
            "Iter 2080, loss = 1.0302889\n",
            "Iter 2081, loss = 1.0285404\n",
            "Iter 2082, loss = 0.9079776\n",
            "Iter 2083, loss = 0.866796\n",
            "Iter 2084, loss = 1.0502173\n",
            "Iter 2085, loss = 1.0020962\n",
            "Iter 2086, loss = 1.1605676\n",
            "Iter 2087, loss = 1.051383\n",
            "Iter 2088, loss = 1.1617616\n",
            "Iter 2089, loss = 0.9641368\n",
            "Iter 2090, loss = 1.3786471\n",
            "Iter 2091, loss = 1.209158\n",
            "Iter 2092, loss = 1.0167638\n",
            "Iter 2093, loss = 1.1978881\n",
            "Iter 2094, loss = 1.1717902\n",
            "Iter 2095, loss = 1.1326022\n",
            "Iter 2096, loss = 1.5467604\n",
            "Iter 2097, loss = 0.831604\n",
            "Iter 2098, loss = 0.8669258\n",
            "Iter 2099, loss = 1.0660436\n",
            "Iter 2100, loss = 0.9843458\n",
            "Iter 2101, loss = 0.98090833\n",
            "Iter 2102, loss = 0.9764361\n",
            "Iter 2103, loss = 1.3702948\n",
            "Iter 2104, loss = 1.2798394\n",
            "Iter 2105, loss = 0.8029485\n",
            "Iter 2106, loss = 1.1996007\n",
            "Iter 2107, loss = 0.8695301\n",
            "Iter 2108, loss = 1.1917541\n",
            "Iter 2109, loss = 1.7963572\n",
            "Iter 2110, loss = 0.99868053\n",
            "Iter 2111, loss = 1.4307624\n",
            "Iter 2112, loss = 1.1463461\n",
            "Iter 2113, loss = 1.2943645\n",
            "Iter 2114, loss = 0.95249784\n",
            "Iter 2115, loss = 1.2993064\n",
            "Iter 2116, loss = 1.0095264\n",
            "Iter 2117, loss = 0.9994871\n",
            "Iter 2118, loss = 1.1085868\n",
            "Iter 2119, loss = 1.1264648\n",
            "Iter 2120, loss = 1.1342521\n",
            "Iter 2121, loss = 1.3316901\n",
            "Iter 2122, loss = 0.8324742\n",
            "Iter 2123, loss = 1.2556777\n",
            "Iter 2124, loss = 1.1901877\n",
            "Iter 2125, loss = 0.86029303\n",
            "Iter 2126, loss = 1.4885342\n",
            "Iter 2127, loss = 1.1832836\n",
            "Iter 2128, loss = 0.98984736\n",
            "Iter 2129, loss = 1.1097605\n",
            "Iter 2130, loss = 1.060874\n",
            "Iter 2131, loss = 0.97757685\n",
            "Iter 2132, loss = 1.2792821\n",
            "Iter 2133, loss = 1.0789768\n",
            "Iter 2134, loss = 1.2090607\n",
            "Iter 2135, loss = 1.1641527\n",
            "Iter 2136, loss = 1.1319561\n",
            "Iter 2137, loss = 1.1253855\n",
            "Iter 2138, loss = 1.1603287\n",
            "Iter 2139, loss = 1.273854\n",
            "Iter 2140, loss = 1.2120864\n",
            "Iter 2141, loss = 1.4397969\n",
            "Iter 2142, loss = 1.6301429\n",
            "Iter 2143, loss = 1.2174311\n",
            "Iter 2144, loss = 1.1678169\n",
            "Iter 2145, loss = 1.0836477\n",
            "Iter 2146, loss = 1.3225341\n",
            "Iter 2147, loss = 0.9740141\n",
            "Iter 2148, loss = 1.1010618\n",
            "Iter 2149, loss = 1.0598112\n",
            "Iter 2150, loss = 1.4180368\n",
            "Iter 2151, loss = 1.1088765\n",
            "Iter 2152, loss = 0.93723476\n",
            "Iter 2153, loss = 1.5779185\n",
            "Iter 2154, loss = 0.94288695\n",
            "Iter 2155, loss = 1.0965548\n",
            "Iter 2156, loss = 1.1284301\n",
            "Iter 2157, loss = 1.1241466\n",
            "Iter 2158, loss = 1.236987\n",
            "Iter 2159, loss = 0.92509395\n",
            "Iter 2160, loss = 1.1394435\n",
            "Iter 2161, loss = 1.1404355\n",
            "Iter 2162, loss = 1.3887324\n",
            "Iter 2163, loss = 0.79179233\n",
            "Iter 2164, loss = 1.2045629\n",
            "Iter 2165, loss = 1.0349059\n",
            "Iter 2166, loss = 1.4011631\n",
            "Iter 2167, loss = 1.1553742\n",
            "Iter 2168, loss = 1.4358997\n",
            "Iter 2169, loss = 1.1761878\n",
            "Iter 2170, loss = 1.1578727\n",
            "Iter 2171, loss = 1.1969748\n",
            "Iter 2172, loss = 0.9922043\n",
            "Iter 2173, loss = 1.137598\n",
            "Iter 2174, loss = 1.2058384\n",
            "Iter 2175, loss = 0.97888994\n",
            "Iter 2176, loss = 0.9913881\n",
            "Iter 2177, loss = 0.98724705\n",
            "Iter 2178, loss = 0.8257922\n",
            "Iter 2179, loss = 1.4007422\n",
            "Iter 2180, loss = 0.77485096\n",
            "Iter 2181, loss = 0.7126895\n",
            "Iter 2182, loss = 1.2429297\n",
            "Iter 2183, loss = 0.9018363\n",
            "Iter 2184, loss = 0.83350325\n",
            "Iter 2185, loss = 1.3764424\n",
            "Iter 2186, loss = 1.3086939\n",
            "Iter 2187, loss = 1.0852863\n",
            "Iter 2188, loss = 1.2759821\n",
            "Iter 2189, loss = 1.8883647\n",
            "Iter 2190, loss = 1.0306232\n",
            "Iter 2191, loss = 0.9465606\n",
            "Iter 2192, loss = 1.0778681\n",
            "Iter 2193, loss = 1.1756642\n",
            "Iter 2194, loss = 1.0626814\n",
            "Iter 2195, loss = 1.1080213\n",
            "Iter 2196, loss = 1.0184951\n",
            "Iter 2197, loss = 1.220417\n",
            "Iter 2198, loss = 1.3032733\n",
            "Iter 2199, loss = 1.4177761\n",
            "Iter 2200, loss = 0.9287064\n",
            "Iter 2201, loss = 1.191114\n",
            "Iter 2202, loss = 1.0907924\n",
            "Iter 2203, loss = 1.0250257\n",
            "Iter 2204, loss = 0.90895605\n",
            "Iter 2205, loss = 0.9516217\n",
            "Iter 2206, loss = 0.9197002\n",
            "Iter 2207, loss = 0.92409253\n",
            "Iter 2208, loss = 0.9963321\n",
            "Iter 2209, loss = 1.3048651\n",
            "Iter 2210, loss = 1.1779481\n",
            "Iter 2211, loss = 1.0688881\n",
            "Iter 2212, loss = 1.31415\n",
            "Iter 2213, loss = 1.1109076\n",
            "Iter 2214, loss = 1.0737283\n",
            "Iter 2215, loss = 1.3524419\n",
            "Iter 2216, loss = 1.3848366\n",
            "Iter 2217, loss = 1.3462341\n",
            "Iter 2218, loss = 1.0319614\n",
            "Iter 2219, loss = 0.8652125\n",
            "Iter 2220, loss = 1.1642909\n",
            "Iter 2221, loss = 0.9745602\n",
            "Iter 2222, loss = 0.9766354\n",
            "Iter 2223, loss = 0.96241224\n",
            "Iter 2224, loss = 1.1020555\n",
            "Iter 2225, loss = 1.1390697\n",
            "Iter 2226, loss = 1.0778672\n",
            "Iter 2227, loss = 1.2637918\n",
            "Iter 2228, loss = 0.95639324\n",
            "Iter 2229, loss = 1.0463146\n",
            "Iter 2230, loss = 1.236268\n",
            "Iter 2231, loss = 0.8638684\n",
            "Iter 2232, loss = 1.744794\n",
            "Iter 2233, loss = 1.0460663\n",
            "Iter 2234, loss = 1.6079847\n",
            "Iter 2235, loss = 1.1347153\n",
            "Iter 2236, loss = 0.96852577\n",
            "Iter 2237, loss = 1.1454388\n",
            "Iter 2238, loss = 1.2850853\n",
            "Iter 2239, loss = 1.1647809\n",
            "Iter 2240, loss = 1.3266652\n",
            "Iter 2241, loss = 1.0054257\n",
            "Iter 2242, loss = 0.8963963\n",
            "Iter 2243, loss = 1.097764\n",
            "Iter 2244, loss = 1.416486\n",
            "Iter 2245, loss = 0.9867399\n",
            "Iter 2246, loss = 0.8975948\n",
            "Iter 2247, loss = 1.1461384\n",
            "Iter 2248, loss = 0.8764398\n",
            "Iter 2249, loss = 0.7973005\n",
            "Iter 2250, loss = 1.0251122\n",
            "Iter 2251, loss = 1.4156032\n",
            "Iter 2252, loss = 1.3069754\n",
            "Iter 2253, loss = 1.3094066\n",
            "Iter 2254, loss = 1.0036862\n",
            "Iter 2255, loss = 1.0475128\n",
            "Iter 2256, loss = 1.6640041\n",
            "Iter 2257, loss = 1.3976653\n",
            "Iter 2258, loss = 1.3538309\n",
            "Iter 2259, loss = 1.0873525\n",
            "Iter 2260, loss = 1.3995166\n",
            "Iter 2261, loss = 1.1270002\n",
            "Iter 2262, loss = 1.1504735\n",
            "Iter 2263, loss = 1.1309438\n",
            "Iter 2264, loss = 1.1867335\n",
            "Iter 2265, loss = 0.89832443\n",
            "Iter 2266, loss = 1.1446733\n",
            "Iter 2267, loss = 0.8858565\n",
            "Iter 2268, loss = 0.98186636\n",
            "Iter 2269, loss = 1.4736251\n",
            "Iter 2270, loss = 1.1202006\n",
            "Iter 2271, loss = 1.1716833\n",
            "Iter 2272, loss = 1.3879586\n",
            "Iter 2273, loss = 1.2341027\n",
            "Iter 2274, loss = 1.132673\n",
            "Iter 2275, loss = 1.2248343\n",
            "Iter 2276, loss = 1.5540419\n",
            "Iter 2277, loss = 1.3875681\n",
            "Iter 2278, loss = 1.3800848\n",
            "Iter 2279, loss = 1.1403005\n",
            "Iter 2280, loss = 1.0696089\n",
            "Iter 2281, loss = 0.9171072\n",
            "Iter 2282, loss = 1.3032212\n",
            "Iter 2283, loss = 1.0060513\n",
            "Iter 2284, loss = 1.0318742\n",
            "Iter 2285, loss = 0.9075365\n",
            "Iter 2286, loss = 0.97254294\n",
            "Iter 2287, loss = 1.3931799\n",
            "Iter 2288, loss = 0.91821057\n",
            "Iter 2289, loss = 1.1011066\n",
            "Iter 2290, loss = 1.246394\n",
            "Iter 2291, loss = 1.1048489\n",
            "Iter 2292, loss = 1.0094919\n",
            "Iter 2293, loss = 1.3146324\n",
            "Iter 2294, loss = 0.97441316\n",
            "Iter 2295, loss = 1.1838546\n",
            "Iter 2296, loss = 1.1234442\n",
            "Iter 2297, loss = 0.7857093\n",
            "Iter 2298, loss = 0.8463106\n",
            "Iter 2299, loss = 1.3185843\n",
            "Iter 2300, loss = 1.4267923\n",
            "Iter 2301, loss = 1.2865846\n",
            "Iter 2302, loss = 1.0080758\n",
            "Iter 2303, loss = 1.3788755\n",
            "Iter 2304, loss = 1.0721803\n",
            "Iter 2305, loss = 1.4541395\n",
            "Iter 2306, loss = 1.2042142\n",
            "Iter 2307, loss = 0.9593503\n",
            "Iter 2308, loss = 1.3998611\n",
            "Iter 2309, loss = 1.2109025\n",
            "Iter 2310, loss = 1.0859613\n",
            "Iter 2311, loss = 1.0885222\n",
            "Iter 2312, loss = 1.4516459\n",
            "Iter 2313, loss = 1.3952911\n",
            "Iter 2314, loss = 1.1527631\n",
            "Iter 2315, loss = 1.3757732\n",
            "Iter 2316, loss = 0.99755526\n",
            "Iter 2317, loss = 0.8857994\n",
            "Iter 2318, loss = 0.89237314\n",
            "Iter 2319, loss = 1.4822059\n",
            "Iter 2320, loss = 1.3888288\n",
            "Iter 2321, loss = 1.1938944\n",
            "Iter 2322, loss = 1.1004962\n",
            "Iter 2323, loss = 0.8412775\n",
            "Iter 2324, loss = 1.4865445\n",
            "Iter 2325, loss = 1.1842765\n",
            "Iter 2326, loss = 0.904016\n",
            "Iter 2327, loss = 1.1664753\n",
            "Iter 2328, loss = 1.3204856\n",
            "Iter 2329, loss = 1.085614\n",
            "Iter 2330, loss = 1.020369\n",
            "Iter 2331, loss = 0.8182517\n",
            "Iter 2332, loss = 1.2473389\n",
            "Iter 2333, loss = 1.102587\n",
            "Iter 2334, loss = 1.5019207\n",
            "Iter 2335, loss = 1.1491147\n",
            "Iter 2336, loss = 1.2620008\n",
            "Iter 2337, loss = 1.1013858\n",
            "Iter 2338, loss = 0.9395206\n",
            "Iter 2339, loss = 1.47278\n",
            "Iter 2340, loss = 0.94487345\n",
            "Iter 2341, loss = 1.2914946\n",
            "Iter 2342, loss = 0.99942935\n",
            "Iter 2343, loss = 1.1956086\n",
            "Iter 2344, loss = 1.0761398\n",
            "Iter 2345, loss = 1.2997618\n",
            "Iter 2346, loss = 1.3167981\n",
            "Iter 2347, loss = 0.97914577\n",
            "Iter 2348, loss = 0.8842306\n",
            "Iter 2349, loss = 1.2499007\n",
            "Iter 2350, loss = 0.8615441\n",
            "Iter 2351, loss = 0.9148241\n",
            "Iter 2352, loss = 1.3384094\n",
            "Iter 2353, loss = 1.3800309\n",
            "Iter 2354, loss = 1.1061796\n",
            "Iter 2355, loss = 0.9703544\n",
            "Iter 2356, loss = 1.0603462\n",
            "Iter 2357, loss = 1.3673562\n",
            "Iter 2358, loss = 0.91226095\n",
            "Iter 2359, loss = 1.026355\n",
            "Iter 2360, loss = 1.1164123\n",
            "Iter 2361, loss = 1.0752674\n",
            "Iter 2362, loss = 0.93515813\n",
            "Iter 2363, loss = 1.4816554\n",
            "Iter 2364, loss = 0.9205029\n",
            "Iter 2365, loss = 1.0732423\n",
            "Iter 2366, loss = 1.3908536\n",
            "Iter 2367, loss = 1.0507112\n",
            "Iter 2368, loss = 1.1518638\n",
            "Iter 2369, loss = 0.8811873\n",
            "Iter 2370, loss = 1.4128263\n",
            "Iter 2371, loss = 1.0233846\n",
            "Iter 2372, loss = 0.9392562\n",
            "Iter 2373, loss = 0.97940993\n",
            "Iter 2374, loss = 1.047713\n",
            "Iter 2375, loss = 0.9163267\n",
            "Iter 2376, loss = 1.2658073\n",
            "Iter 2377, loss = 1.1823974\n",
            "Iter 2378, loss = 0.9822775\n",
            "Iter 2379, loss = 1.0573151\n",
            "Iter 2380, loss = 0.8149471\n",
            "Iter 2381, loss = 1.1256596\n",
            "Iter 2382, loss = 1.1985776\n",
            "Iter 2383, loss = 1.1171978\n",
            "Iter 2384, loss = 0.76761353\n",
            "Iter 2385, loss = 1.3982439\n",
            "Iter 2386, loss = 0.927268\n",
            "Iter 2387, loss = 1.0455081\n",
            "Iter 2388, loss = 0.8781631\n",
            "Iter 2389, loss = 1.0313022\n",
            "Iter 2390, loss = 0.8606263\n",
            "Iter 2391, loss = 1.2998136\n",
            "Iter 2392, loss = 1.0229104\n",
            "Iter 2393, loss = 1.106575\n",
            "Iter 2394, loss = 1.0109353\n",
            "Iter 2395, loss = 1.6707388\n",
            "Iter 2396, loss = 0.96200603\n",
            "Iter 2397, loss = 1.507224\n",
            "Iter 2398, loss = 1.35887\n",
            "Iter 2399, loss = 1.1311506\n",
            "Iter 2400, loss = 1.2165279\n",
            "Iter 2401, loss = 1.329067\n",
            "Iter 2402, loss = 1.0041394\n",
            "Iter 2403, loss = 0.7219006\n",
            "Iter 2404, loss = 1.0347189\n",
            "Iter 2405, loss = 1.2506021\n",
            "Iter 2406, loss = 0.95805395\n",
            "Iter 2407, loss = 1.4683385\n",
            "Iter 2408, loss = 0.9830347\n",
            "Iter 2409, loss = 1.5974958\n",
            "Iter 2410, loss = 1.1029418\n",
            "Iter 2411, loss = 1.4802481\n",
            "Iter 2412, loss = 1.4840864\n",
            "Iter 2413, loss = 1.2102866\n",
            "Iter 2414, loss = 0.94345593\n",
            "Iter 2415, loss = 1.4994826\n",
            "Iter 2416, loss = 0.8461496\n",
            "Iter 2417, loss = 1.4453917\n",
            "Iter 2418, loss = 1.1008115\n",
            "Iter 2419, loss = 1.1509781\n",
            "Iter 2420, loss = 0.93231994\n",
            "Iter 2421, loss = 1.1575549\n",
            "Iter 2422, loss = 1.0737463\n",
            "Iter 2423, loss = 1.2287191\n",
            "Iter 2424, loss = 1.1889654\n",
            "Iter 2425, loss = 1.1577048\n",
            "Iter 2426, loss = 1.2789499\n",
            "Iter 2427, loss = 0.97755027\n",
            "Iter 2428, loss = 1.099671\n",
            "Iter 2429, loss = 0.98979235\n",
            "Iter 2430, loss = 1.2964437\n",
            "Iter 2431, loss = 0.7958325\n",
            "Iter 2432, loss = 1.128044\n",
            "Iter 2433, loss = 1.0346854\n",
            "Iter 2434, loss = 1.2724552\n",
            "Iter 2435, loss = 1.0354528\n",
            "Iter 2436, loss = 1.0753152\n",
            "Iter 2437, loss = 1.2030318\n",
            "Iter 2438, loss = 1.4501952\n",
            "Iter 2439, loss = 1.4679797\n",
            "Iter 2440, loss = 1.5032518\n",
            "Iter 2441, loss = 1.338505\n",
            "Iter 2442, loss = 1.0049405\n",
            "Iter 2443, loss = 0.94898\n",
            "Iter 2444, loss = 1.0244584\n",
            "Iter 2445, loss = 1.5828797\n",
            "Iter 2446, loss = 1.3282986\n",
            "Iter 2447, loss = 0.89026976\n",
            "Iter 2448, loss = 0.8030672\n",
            "Iter 2449, loss = 1.8440032\n",
            "Iter 2450, loss = 1.3524693\n",
            "Iter 2451, loss = 1.1359732\n",
            "Iter 2452, loss = 1.4876167\n",
            "Iter 2453, loss = 0.858589\n",
            "Iter 2454, loss = 1.2983243\n",
            "Iter 2455, loss = 0.95925325\n",
            "Iter 2456, loss = 1.0098584\n",
            "Iter 2457, loss = 1.1920979\n",
            "Iter 2458, loss = 1.0391319\n",
            "Iter 2459, loss = 1.0810962\n",
            "Iter 2460, loss = 0.88048613\n",
            "Iter 2461, loss = 1.1420945\n",
            "Iter 2462, loss = 1.1026692\n",
            "Iter 2463, loss = 0.7826872\n",
            "Iter 2464, loss = 1.3931024\n",
            "Iter 2465, loss = 1.6116568\n",
            "Iter 2466, loss = 1.3719403\n",
            "Iter 2467, loss = 1.3428254\n",
            "Iter 2468, loss = 1.119168\n",
            "Iter 2469, loss = 0.95104605\n",
            "Iter 2470, loss = 1.2571106\n",
            "Iter 2471, loss = 1.1497107\n",
            "Iter 2472, loss = 1.1603491\n",
            "Iter 2473, loss = 1.050518\n",
            "Iter 2474, loss = 1.1170071\n",
            "Iter 2475, loss = 0.99476504\n",
            "Iter 2476, loss = 1.1341726\n",
            "Iter 2477, loss = 1.0764036\n",
            "Iter 2478, loss = 0.8074672\n",
            "Iter 2479, loss = 1.5550575\n",
            "Iter 2480, loss = 1.1265802\n",
            "Iter 2481, loss = 1.1973791\n",
            "Iter 2482, loss = 0.9726398\n",
            "Iter 2483, loss = 1.1603084\n",
            "Iter 2484, loss = 1.0527556\n",
            "Iter 2485, loss = 1.2942159\n",
            "Iter 2486, loss = 1.1275358\n",
            "Iter 2487, loss = 1.0343676\n",
            "Iter 2488, loss = 1.1726309\n",
            "Iter 2489, loss = 0.9790324\n",
            "Iter 2490, loss = 0.6976023\n",
            "Iter 2491, loss = 1.6086842\n",
            "Iter 2492, loss = 1.0314579\n",
            "Iter 2493, loss = 1.0552213\n",
            "Iter 2494, loss = 1.105007\n",
            "Iter 2495, loss = 1.125394\n",
            "Iter 2496, loss = 1.4297616\n",
            "Iter 2497, loss = 1.0502052\n",
            "Iter 2498, loss = 1.0220246\n",
            "Iter 2499, loss = 1.378335\n",
            "Iter 2500, loss = 1.2340865\n",
            "Iter 2501, loss = 1.1903691\n",
            "Iter 2502, loss = 1.1457994\n",
            "Iter 2503, loss = 0.8725488\n",
            "Iter 2504, loss = 1.0100497\n",
            "Iter 2505, loss = 1.1280324\n",
            "Iter 2506, loss = 1.4196455\n",
            "Iter 2507, loss = 0.90570915\n",
            "Iter 2508, loss = 1.3636156\n",
            "Iter 2509, loss = 1.4159087\n",
            "Iter 2510, loss = 1.1667325\n",
            "Iter 2511, loss = 1.0711105\n",
            "Iter 2512, loss = 1.1473607\n",
            "Iter 2513, loss = 0.9443166\n",
            "Iter 2514, loss = 1.1646216\n",
            "Iter 2515, loss = 1.2216434\n",
            "Iter 2516, loss = 1.314979\n",
            "Iter 2517, loss = 0.9920771\n",
            "Iter 2518, loss = 1.1324959\n",
            "Iter 2519, loss = 1.1824824\n",
            "Iter 2520, loss = 1.0385138\n",
            "Iter 2521, loss = 1.1209414\n",
            "Iter 2522, loss = 0.9834722\n",
            "Iter 2523, loss = 0.9266057\n",
            "Iter 2524, loss = 1.1462638\n",
            "Iter 2525, loss = 1.1555073\n",
            "Iter 2526, loss = 1.083435\n",
            "Iter 2527, loss = 1.1152171\n",
            "Iter 2528, loss = 1.3249803\n",
            "Iter 2529, loss = 1.0384101\n",
            "Iter 2530, loss = 0.6552666\n",
            "Iter 2531, loss = 0.76225454\n",
            "Iter 2532, loss = 1.127655\n",
            "Iter 2533, loss = 0.7912574\n",
            "Iter 2534, loss = 1.1898952\n",
            "Iter 2535, loss = 1.6403458\n",
            "Iter 2536, loss = 1.290057\n",
            "Iter 2537, loss = 1.3892932\n",
            "Iter 2538, loss = 0.9766027\n",
            "Iter 2539, loss = 0.9794137\n",
            "Iter 2540, loss = 0.94797647\n",
            "Iter 2541, loss = 1.1894662\n",
            "Iter 2542, loss = 1.3850747\n",
            "Iter 2543, loss = 0.95771563\n",
            "Iter 2544, loss = 1.078952\n",
            "Iter 2545, loss = 1.002917\n",
            "Iter 2546, loss = 0.91588414\n",
            "Iter 2547, loss = 1.2247312\n",
            "Iter 2548, loss = 1.1006718\n",
            "Iter 2549, loss = 0.7848687\n",
            "Iter 2550, loss = 1.3631939\n",
            "Iter 2551, loss = 1.1794527\n",
            "Iter 2552, loss = 1.0738759\n",
            "Iter 2553, loss = 0.9827295\n",
            "Iter 2554, loss = 1.199206\n",
            "Iter 2555, loss = 1.2086583\n",
            "Iter 2556, loss = 1.3379607\n",
            "Iter 2557, loss = 1.3525524\n",
            "Iter 2558, loss = 1.0788512\n",
            "Iter 2559, loss = 1.4654529\n",
            "Iter 2560, loss = 1.1933234\n",
            "Iter 2561, loss = 1.0909731\n",
            "Iter 2562, loss = 0.81330144\n",
            "Iter 2563, loss = 0.97756755\n",
            "Iter 2564, loss = 0.99231726\n",
            "Iter 2565, loss = 0.9940783\n",
            "Iter 2566, loss = 1.276417\n",
            "Iter 2567, loss = 1.0867424\n",
            "Iter 2568, loss = 0.81885135\n",
            "Iter 2569, loss = 0.9910481\n",
            "Iter 2570, loss = 1.7755318\n",
            "Iter 2571, loss = 0.89695346\n",
            "Iter 2572, loss = 1.3475678\n",
            "Iter 2573, loss = 1.08355\n",
            "Iter 2574, loss = 1.2200111\n",
            "Iter 2575, loss = 1.0005255\n",
            "Iter 2576, loss = 0.96404827\n",
            "Iter 2577, loss = 1.1882906\n",
            "Iter 2578, loss = 1.1810043\n",
            "Iter 2579, loss = 0.9072083\n",
            "Iter 2580, loss = 1.1704559\n",
            "Iter 2581, loss = 1.3257947\n",
            "Iter 2582, loss = 0.92606276\n",
            "Iter 2583, loss = 1.1021054\n",
            "Iter 2584, loss = 1.2490438\n",
            "Iter 2585, loss = 1.2162547\n",
            "Iter 2586, loss = 1.149516\n",
            "Iter 2587, loss = 1.2349149\n",
            "Iter 2588, loss = 1.3205867\n",
            "Iter 2589, loss = 0.97773707\n",
            "Iter 2590, loss = 1.0144459\n",
            "Iter 2591, loss = 1.0533339\n",
            "Iter 2592, loss = 1.0432265\n",
            "Iter 2593, loss = 1.1321877\n",
            "Iter 2594, loss = 1.1478503\n",
            "Iter 2595, loss = 1.3740301\n",
            "Iter 2596, loss = 0.95188564\n",
            "Iter 2597, loss = 1.5498357\n",
            "Iter 2598, loss = 1.06253\n",
            "Iter 2599, loss = 1.3073635\n",
            "Iter 2600, loss = 0.8562403\n",
            "Iter 2601, loss = 1.7753465\n",
            "Iter 2602, loss = 1.4795465\n",
            "Iter 2603, loss = 0.9755477\n",
            "Iter 2604, loss = 1.0440574\n",
            "Iter 2605, loss = 1.0445495\n",
            "Iter 2606, loss = 0.8417113\n",
            "Iter 2607, loss = 0.92078704\n",
            "Iter 2608, loss = 0.96175617\n",
            "Iter 2609, loss = 0.8567363\n",
            "Iter 2610, loss = 0.9783014\n",
            "Iter 2611, loss = 1.0006051\n",
            "Iter 2612, loss = 1.0411652\n",
            "Iter 2613, loss = 1.0301653\n",
            "Iter 2614, loss = 1.065721\n",
            "Iter 2615, loss = 0.8839283\n",
            "Iter 2616, loss = 0.8193972\n",
            "Iter 2617, loss = 0.9327159\n",
            "Iter 2618, loss = 1.1225436\n",
            "Iter 2619, loss = 1.3592522\n",
            "Iter 2620, loss = 0.8303561\n",
            "Iter 2621, loss = 0.9336873\n",
            "Iter 2622, loss = 1.0649164\n",
            "Iter 2623, loss = 1.1049984\n",
            "Iter 2624, loss = 1.3783534\n",
            "Iter 2625, loss = 1.1026694\n",
            "Iter 2626, loss = 1.3909445\n",
            "Iter 2627, loss = 1.1397406\n",
            "Iter 2628, loss = 1.4658947\n",
            "Iter 2629, loss = 1.0973326\n",
            "Iter 2630, loss = 0.8991462\n",
            "Iter 2631, loss = 1.0520636\n",
            "Iter 2632, loss = 1.4488503\n",
            "Iter 2633, loss = 1.2248628\n",
            "Iter 2634, loss = 0.9286395\n",
            "Iter 2635, loss = 0.7817957\n",
            "Iter 2636, loss = 0.9252291\n",
            "Iter 2637, loss = 1.2433994\n",
            "Iter 2638, loss = 0.95685196\n",
            "Iter 2639, loss = 1.1191859\n",
            "Iter 2640, loss = 1.17255\n",
            "Iter 2641, loss = 1.1253402\n",
            "Iter 2642, loss = 1.0618875\n",
            "Iter 2643, loss = 1.0505793\n",
            "Iter 2644, loss = 1.527931\n",
            "Iter 2645, loss = 0.90518683\n",
            "Iter 2646, loss = 1.0928155\n",
            "Iter 2647, loss = 1.1005692\n",
            "Iter 2648, loss = 0.8798472\n",
            "Iter 2649, loss = 1.0841918\n",
            "Iter 2650, loss = 1.4248987\n",
            "Iter 2651, loss = 0.8429124\n",
            "Iter 2652, loss = 1.6336058\n",
            "Iter 2653, loss = 1.0231501\n",
            "Iter 2654, loss = 1.4273727\n",
            "Iter 2655, loss = 1.0316181\n",
            "Iter 2656, loss = 0.8946725\n",
            "Iter 2657, loss = 0.7973454\n",
            "Iter 2658, loss = 0.9529392\n",
            "Iter 2659, loss = 0.8607859\n",
            "Iter 2660, loss = 1.5414948\n",
            "Iter 2661, loss = 1.0070351\n",
            "Iter 2662, loss = 0.841817\n",
            "Iter 2663, loss = 1.3055078\n",
            "Iter 2664, loss = 0.9324365\n",
            "Iter 2665, loss = 0.8880408\n",
            "Iter 2666, loss = 1.3307722\n",
            "Iter 2667, loss = 0.8303261\n",
            "Iter 2668, loss = 1.7973213\n",
            "Iter 2669, loss = 1.4546622\n",
            "Iter 2670, loss = 1.1288878\n",
            "Iter 2671, loss = 1.1508036\n",
            "Iter 2672, loss = 0.9862033\n",
            "Iter 2673, loss = 1.025192\n",
            "Iter 2674, loss = 1.0710115\n",
            "Iter 2675, loss = 1.7206318\n",
            "Iter 2676, loss = 1.1541076\n",
            "Iter 2677, loss = 1.5136535\n",
            "Iter 2678, loss = 1.01935\n",
            "Iter 2679, loss = 1.1285946\n",
            "Iter 2680, loss = 1.1742718\n",
            "Iter 2681, loss = 0.9731394\n",
            "Iter 2682, loss = 0.9405491\n",
            "Iter 2683, loss = 1.1731822\n",
            "Iter 2684, loss = 1.0473477\n",
            "Iter 2685, loss = 0.87147593\n",
            "Iter 2686, loss = 1.3413696\n",
            "Iter 2687, loss = 1.0270245\n",
            "Iter 2688, loss = 1.2539958\n",
            "Iter 2689, loss = 1.2842271\n",
            "Iter 2690, loss = 0.99436736\n",
            "Iter 2691, loss = 0.83722425\n",
            "Iter 2692, loss = 0.9440038\n",
            "Iter 2693, loss = 1.6294928\n",
            "Iter 2694, loss = 1.1309557\n",
            "Iter 2695, loss = 1.1501043\n",
            "Iter 2696, loss = 1.5622727\n",
            "Iter 2697, loss = 1.2438502\n",
            "Iter 2698, loss = 0.9635564\n",
            "Iter 2699, loss = 1.2025533\n",
            "Iter 2700, loss = 1.2958405\n",
            "Iter 2701, loss = 0.9978446\n",
            "Iter 2702, loss = 1.0666933\n",
            "Iter 2703, loss = 1.281699\n",
            "Iter 2704, loss = 1.0577965\n",
            "Iter 2705, loss = 1.128849\n",
            "Iter 2706, loss = 1.0176611\n",
            "Iter 2707, loss = 1.2587394\n",
            "Iter 2708, loss = 1.1883622\n",
            "Iter 2709, loss = 1.2898495\n",
            "Iter 2710, loss = 1.1213713\n",
            "Iter 2711, loss = 1.2138321\n",
            "Iter 2712, loss = 0.87056625\n",
            "Iter 2713, loss = 0.89601076\n",
            "Iter 2714, loss = 0.97652245\n",
            "Iter 2715, loss = 1.0527406\n",
            "Iter 2716, loss = 1.0598412\n",
            "Iter 2717, loss = 0.8352001\n",
            "Iter 2718, loss = 0.819757\n",
            "Iter 2719, loss = 0.9677414\n",
            "Iter 2720, loss = 0.98808616\n",
            "Iter 2721, loss = 0.99564457\n",
            "Iter 2722, loss = 0.81759083\n",
            "Iter 2723, loss = 0.97478634\n",
            "Iter 2724, loss = 0.9457007\n",
            "Iter 2725, loss = 1.1209567\n",
            "Iter 2726, loss = 1.1008773\n",
            "Iter 2727, loss = 0.98533803\n",
            "Iter 2728, loss = 1.0847156\n",
            "Iter 2729, loss = 1.3301897\n",
            "Iter 2730, loss = 0.89522445\n",
            "Iter 2731, loss = 1.6260595\n",
            "Iter 2732, loss = 1.0603019\n",
            "Iter 2733, loss = 1.0451987\n",
            "Iter 2734, loss = 1.1908579\n",
            "Iter 2735, loss = 1.0872241\n",
            "Iter 2736, loss = 1.062583\n",
            "Iter 2737, loss = 1.5122528\n",
            "Iter 2738, loss = 0.86626124\n",
            "Iter 2739, loss = 1.2790751\n",
            "Iter 2740, loss = 0.9139377\n",
            "Iter 2741, loss = 1.3624395\n",
            "Iter 2742, loss = 1.2202542\n",
            "Iter 2743, loss = 1.3107989\n",
            "Iter 2744, loss = 0.8123416\n",
            "Iter 2745, loss = 1.3359466\n",
            "Iter 2746, loss = 1.0346967\n",
            "Iter 2747, loss = 1.1260781\n",
            "Iter 2748, loss = 1.4753766\n",
            "Iter 2749, loss = 0.95468014\n",
            "Iter 2750, loss = 1.7972269\n",
            "Iter 2751, loss = 1.4403574\n",
            "Iter 2752, loss = 1.4303734\n",
            "Iter 2753, loss = 1.1020043\n",
            "Iter 2754, loss = 1.1927544\n",
            "Iter 2755, loss = 0.83411336\n",
            "Iter 2756, loss = 0.9183536\n",
            "Iter 2757, loss = 0.8977196\n",
            "Iter 2758, loss = 1.0142663\n",
            "Iter 2759, loss = 1.3246194\n",
            "Iter 2760, loss = 0.95050436\n",
            "Iter 2761, loss = 1.395041\n",
            "Iter 2762, loss = 1.1462817\n",
            "Iter 2763, loss = 0.9930862\n",
            "Iter 2764, loss = 0.9615054\n",
            "Iter 2765, loss = 1.1358168\n",
            "Iter 2766, loss = 0.9906958\n",
            "Iter 2767, loss = 0.73668945\n",
            "Iter 2768, loss = 1.4077597\n",
            "Iter 2769, loss = 0.82704306\n",
            "Iter 2770, loss = 1.6254718\n",
            "Iter 2771, loss = 1.322139\n",
            "Iter 2772, loss = 0.87177455\n",
            "Iter 2773, loss = 0.84822947\n",
            "Iter 2774, loss = 1.2242903\n",
            "Iter 2775, loss = 1.205619\n",
            "Iter 2776, loss = 1.2761872\n",
            "Iter 2777, loss = 1.1469362\n",
            "Iter 2778, loss = 1.2737472\n",
            "Iter 2779, loss = 1.4788787\n",
            "Iter 2780, loss = 0.9896134\n",
            "Iter 2781, loss = 1.439004\n",
            "Iter 2782, loss = 0.8216738\n",
            "Iter 2783, loss = 1.1520185\n",
            "Iter 2784, loss = 1.3732221\n",
            "Iter 2785, loss = 1.0742879\n",
            "Iter 2786, loss = 0.9134079\n",
            "Iter 2787, loss = 1.0401514\n",
            "Iter 2788, loss = 1.0226034\n",
            "Iter 2789, loss = 1.2103688\n",
            "Iter 2790, loss = 0.98909706\n",
            "Iter 2791, loss = 1.3818023\n",
            "Iter 2792, loss = 0.76903725\n",
            "Iter 2793, loss = 1.0318547\n",
            "Iter 2794, loss = 1.0586631\n",
            "Iter 2795, loss = 0.72976613\n",
            "Iter 2796, loss = 1.2060733\n",
            "Iter 2797, loss = 0.9155689\n",
            "Iter 2798, loss = 0.93571675\n",
            "Iter 2799, loss = 1.0391805\n",
            "Iter 2800, loss = 0.9720754\n",
            "Iter 2801, loss = 0.9691348\n",
            "Iter 2802, loss = 1.0405521\n",
            "Iter 2803, loss = 1.9275975\n",
            "Iter 2804, loss = 1.1568575\n",
            "Iter 2805, loss = 1.1271334\n",
            "Iter 2806, loss = 1.0246943\n",
            "Iter 2807, loss = 0.8509971\n",
            "Iter 2808, loss = 1.3389311\n",
            "Iter 2809, loss = 1.2513418\n",
            "Iter 2810, loss = 1.0014986\n",
            "Iter 2811, loss = 1.0580035\n",
            "Iter 2812, loss = 1.1616489\n",
            "Iter 2813, loss = 1.0911931\n",
            "Iter 2814, loss = 0.9565953\n",
            "Iter 2815, loss = 0.7356782\n",
            "Iter 2816, loss = 0.93581474\n",
            "Iter 2817, loss = 1.3929822\n",
            "Iter 2818, loss = 1.2163548\n",
            "Iter 2819, loss = 0.96644\n",
            "Iter 2820, loss = 0.9483217\n",
            "Iter 2821, loss = 1.2062765\n",
            "Iter 2822, loss = 0.8566779\n",
            "Iter 2823, loss = 1.1596087\n",
            "Iter 2824, loss = 0.9483299\n",
            "Iter 2825, loss = 0.93701315\n",
            "Iter 2826, loss = 1.1188467\n",
            "Iter 2827, loss = 1.1935344\n",
            "Iter 2828, loss = 1.31893\n",
            "Iter 2829, loss = 1.0313928\n",
            "Iter 2830, loss = 1.0737492\n",
            "Iter 2831, loss = 0.94897604\n",
            "Iter 2832, loss = 1.1474848\n",
            "Iter 2833, loss = 0.96491647\n",
            "Iter 2834, loss = 1.1873631\n",
            "Iter 2835, loss = 0.84151185\n",
            "Iter 2836, loss = 0.76602155\n",
            "Iter 2837, loss = 1.1550487\n",
            "Iter 2838, loss = 0.85042036\n",
            "Iter 2839, loss = 1.617789\n",
            "Iter 2840, loss = 1.1013856\n",
            "Iter 2841, loss = 0.9998875\n",
            "Iter 2842, loss = 1.2229435\n",
            "Iter 2843, loss = 1.0886251\n",
            "Iter 2844, loss = 1.5300453\n",
            "Iter 2845, loss = 1.210299\n",
            "Iter 2846, loss = 1.5686622\n",
            "Iter 2847, loss = 1.1294489\n",
            "Iter 2848, loss = 0.8230475\n",
            "Iter 2849, loss = 1.1610057\n",
            "Iter 2850, loss = 1.1746783\n",
            "Iter 2851, loss = 0.7775222\n",
            "Iter 2852, loss = 1.146849\n",
            "Iter 2853, loss = 1.0052156\n",
            "Iter 2854, loss = 1.7778356\n",
            "Iter 2855, loss = 1.2008197\n",
            "Iter 2856, loss = 1.3443676\n",
            "Iter 2857, loss = 1.1732311\n",
            "Iter 2858, loss = 1.0205743\n",
            "Iter 2859, loss = 0.83362657\n",
            "Iter 2860, loss = 1.4103116\n",
            "Iter 2861, loss = 1.2307183\n",
            "Iter 2862, loss = 1.1263553\n",
            "Iter 2863, loss = 1.2182944\n",
            "Iter 2864, loss = 0.7699249\n",
            "Iter 2865, loss = 1.6733625\n",
            "Iter 2866, loss = 1.3610489\n",
            "Iter 2867, loss = 0.9954579\n",
            "Iter 2868, loss = 1.4016862\n",
            "Iter 2869, loss = 1.1189007\n",
            "Iter 2870, loss = 1.1018991\n",
            "Iter 2871, loss = 1.2151875\n",
            "Iter 2872, loss = 1.0427101\n",
            "Iter 2873, loss = 1.0390341\n",
            "Iter 2874, loss = 1.4958321\n",
            "Iter 2875, loss = 1.1244882\n",
            "Iter 2876, loss = 1.1856189\n",
            "Iter 2877, loss = 1.1465268\n",
            "Iter 2878, loss = 0.89858294\n",
            "Iter 2879, loss = 1.2263205\n",
            "Iter 2880, loss = 1.1520132\n",
            "Iter 2881, loss = 0.78448415\n",
            "Iter 2882, loss = 0.9224917\n",
            "Iter 2883, loss = 1.0849193\n",
            "Iter 2884, loss = 1.4141833\n",
            "Iter 2885, loss = 1.1430209\n",
            "Iter 2886, loss = 1.163421\n",
            "Iter 2887, loss = 0.85352397\n",
            "Iter 2888, loss = 1.1446009\n",
            "Iter 2889, loss = 1.3719448\n",
            "Iter 2890, loss = 1.1851218\n",
            "Iter 2891, loss = 1.5577395\n",
            "Iter 2892, loss = 1.3144801\n",
            "Iter 2893, loss = 1.1685648\n",
            "Iter 2894, loss = 1.243547\n",
            "Iter 2895, loss = 1.1578956\n",
            "Iter 2896, loss = 1.3408923\n",
            "Iter 2897, loss = 0.9107499\n",
            "Iter 2898, loss = 1.0268711\n",
            "Iter 2899, loss = 1.3594959\n",
            "Iter 2900, loss = 0.8971568\n",
            "Iter 2901, loss = 1.1490428\n",
            "Iter 2902, loss = 1.2191579\n",
            "Iter 2903, loss = 1.1376367\n",
            "Iter 2904, loss = 1.4331267\n",
            "Iter 2905, loss = 1.0175481\n",
            "Iter 2906, loss = 1.2189854\n",
            "Iter 2907, loss = 0.9325721\n",
            "Iter 2908, loss = 1.6968582\n",
            "Iter 2909, loss = 1.029694\n",
            "Iter 2910, loss = 1.0558093\n",
            "Iter 2911, loss = 1.2405512\n",
            "Iter 2912, loss = 1.4521276\n",
            "Iter 2913, loss = 1.0773461\n",
            "Iter 2914, loss = 1.1872327\n",
            "Iter 2915, loss = 1.0588703\n",
            "Iter 2916, loss = 0.9824307\n",
            "Iter 2917, loss = 1.2854679\n",
            "Iter 2918, loss = 1.8962681\n",
            "Iter 2919, loss = 1.0587628\n",
            "Iter 2920, loss = 1.0945265\n",
            "Iter 2921, loss = 1.1775007\n",
            "Iter 2922, loss = 1.2739604\n",
            "Iter 2923, loss = 1.2132609\n",
            "Iter 2924, loss = 0.7307055\n",
            "Iter 2925, loss = 0.69683367\n",
            "Iter 2926, loss = 0.7871824\n",
            "Iter 2927, loss = 1.3419476\n",
            "Iter 2928, loss = 1.5365462\n",
            "Iter 2929, loss = 0.9572346\n",
            "Iter 2930, loss = 1.1432022\n",
            "Iter 2931, loss = 0.876566\n",
            "Iter 2932, loss = 1.181648\n",
            "Iter 2933, loss = 1.2158685\n",
            "Iter 2934, loss = 1.0789895\n",
            "Iter 2935, loss = 0.99057907\n",
            "Iter 2936, loss = 0.84807575\n",
            "Iter 2937, loss = 1.1040746\n",
            "Iter 2938, loss = 1.0350517\n",
            "Iter 2939, loss = 1.0422511\n",
            "Iter 2940, loss = 1.2037287\n",
            "Iter 2941, loss = 1.2013326\n",
            "Iter 2942, loss = 1.1510396\n",
            "Iter 2943, loss = 0.99510825\n",
            "Iter 2944, loss = 0.9670154\n",
            "Iter 2945, loss = 0.94048756\n",
            "Iter 2946, loss = 1.1837242\n",
            "Iter 2947, loss = 1.2621489\n",
            "Iter 2948, loss = 1.1568105\n",
            "Iter 2949, loss = 0.9628784\n",
            "Iter 2950, loss = 0.9753343\n",
            "Iter 2951, loss = 0.9075316\n",
            "Iter 2952, loss = 1.1554779\n",
            "Iter 2953, loss = 1.0464461\n",
            "Iter 2954, loss = 1.1798989\n",
            "Iter 2955, loss = 0.88815403\n",
            "Iter 2956, loss = 1.1521046\n",
            "Iter 2957, loss = 0.9885014\n",
            "Iter 2958, loss = 1.1080543\n",
            "Iter 2959, loss = 1.0267044\n",
            "Iter 2960, loss = 0.8279343\n",
            "Iter 2961, loss = 0.8550042\n",
            "Iter 2962, loss = 1.2447264\n",
            "Iter 2963, loss = 1.4128019\n",
            "Iter 2964, loss = 1.0833958\n",
            "Iter 2965, loss = 0.96719474\n",
            "Iter 2966, loss = 0.69558704\n",
            "Iter 2967, loss = 0.9164711\n",
            "Iter 2968, loss = 1.2213819\n",
            "Iter 2969, loss = 1.3461778\n",
            "Iter 2970, loss = 1.1458404\n",
            "Iter 2971, loss = 0.92164505\n",
            "Iter 2972, loss = 0.8571768\n",
            "Iter 2973, loss = 0.8755522\n",
            "Iter 2974, loss = 1.1686754\n",
            "Iter 2975, loss = 1.1711023\n",
            "Iter 2976, loss = 0.8993485\n",
            "Iter 2977, loss = 1.8272797\n",
            "Iter 2978, loss = 1.6309979\n",
            "Iter 2979, loss = 1.1468318\n",
            "Iter 2980, loss = 0.9749244\n",
            "Iter 2981, loss = 1.3053557\n",
            "Iter 2982, loss = 1.0391717\n",
            "Iter 2983, loss = 1.5748689\n",
            "Iter 2984, loss = 0.9338608\n",
            "Iter 2985, loss = 0.96275216\n",
            "Iter 2986, loss = 1.3241634\n",
            "Iter 2987, loss = 0.81981087\n",
            "Iter 2988, loss = 1.3286768\n",
            "Iter 2989, loss = 0.9307295\n",
            "Iter 2990, loss = 1.2204244\n",
            "Iter 2991, loss = 1.1013566\n",
            "Iter 2992, loss = 1.006855\n",
            "Iter 2993, loss = 1.112952\n",
            "Iter 2994, loss = 1.1885169\n",
            "Iter 2995, loss = 0.8665942\n",
            "Iter 2996, loss = 0.86415094\n",
            "Iter 2997, loss = 0.77403504\n",
            "Iter 2998, loss = 1.5203959\n",
            "Iter 2999, loss = 1.1747285\n",
            "Iter 3000, loss = 1.6880255\n",
            "Iter 3001, loss = 0.9909575\n",
            "Iter 3002, loss = 1.2689748\n",
            "Iter 3003, loss = 1.1723313\n",
            "Iter 3004, loss = 0.9848765\n",
            "Iter 3005, loss = 1.0183916\n",
            "Iter 3006, loss = 0.9747447\n",
            "Iter 3007, loss = 1.0060291\n",
            "Iter 3008, loss = 1.1968856\n",
            "Iter 3009, loss = 1.1134048\n",
            "Iter 3010, loss = 1.22935\n",
            "Iter 3011, loss = 1.1500809\n",
            "Iter 3012, loss = 1.4440262\n",
            "Iter 3013, loss = 1.3056135\n",
            "Iter 3014, loss = 1.3787937\n",
            "Iter 3015, loss = 1.3267391\n",
            "Iter 3016, loss = 1.2338927\n",
            "Iter 3017, loss = 1.4174514\n",
            "Iter 3018, loss = 1.0789037\n",
            "Iter 3019, loss = 1.1269898\n",
            "Iter 3020, loss = 1.1274552\n",
            "Iter 3021, loss = 1.128805\n",
            "Iter 3022, loss = 1.3076245\n",
            "Iter 3023, loss = 1.0764294\n",
            "Iter 3024, loss = 1.0788944\n",
            "Iter 3025, loss = 1.2839823\n",
            "Iter 3026, loss = 1.0020708\n",
            "Iter 3027, loss = 0.9796277\n",
            "Iter 3028, loss = 1.0227479\n",
            "Iter 3029, loss = 1.191881\n",
            "Iter 3030, loss = 1.0695103\n",
            "Iter 3031, loss = 0.9101249\n",
            "Iter 3032, loss = 1.119404\n",
            "Iter 3033, loss = 0.8409275\n",
            "Iter 3034, loss = 1.2630312\n",
            "Iter 3035, loss = 1.138094\n",
            "Iter 3036, loss = 0.9575484\n",
            "Iter 3037, loss = 1.247128\n",
            "Iter 3038, loss = 1.1592555\n",
            "Iter 3039, loss = 1.1242085\n",
            "Iter 3040, loss = 1.2757623\n",
            "Iter 3041, loss = 0.8181956\n",
            "Iter 3042, loss = 1.2499027\n",
            "Iter 3043, loss = 0.89917415\n",
            "Iter 3044, loss = 0.9883042\n",
            "Iter 3045, loss = 0.8076292\n",
            "Iter 3046, loss = 0.79582953\n",
            "Iter 3047, loss = 1.0387328\n",
            "Iter 3048, loss = 1.1447697\n",
            "Iter 3049, loss = 0.9164197\n",
            "Iter 3050, loss = 0.9120619\n",
            "Iter 3051, loss = 1.2939618\n",
            "Iter 3052, loss = 1.1452851\n",
            "Iter 3053, loss = 1.1049099\n",
            "Iter 3054, loss = 0.83768815\n",
            "Iter 3055, loss = 1.3148971\n",
            "Iter 3056, loss = 0.7951726\n",
            "Iter 3057, loss = 1.6293973\n",
            "Iter 3058, loss = 1.2283931\n",
            "Iter 3059, loss = 1.1295602\n",
            "Iter 3060, loss = 1.2145338\n",
            "Iter 3061, loss = 1.2722753\n",
            "Iter 3062, loss = 0.90897465\n",
            "Iter 3063, loss = 1.5041748\n",
            "Iter 3064, loss = 0.8903507\n",
            "Iter 3065, loss = 1.2227321\n",
            "Iter 3066, loss = 1.0032434\n",
            "Iter 3067, loss = 1.3101878\n",
            "Iter 3068, loss = 1.3580344\n",
            "Iter 3069, loss = 1.4339792\n",
            "Iter 3070, loss = 0.73844314\n",
            "Iter 3071, loss = 0.96472454\n",
            "Iter 3072, loss = 1.3207622\n",
            "Iter 3073, loss = 0.81077135\n",
            "Iter 3074, loss = 0.9997802\n",
            "Iter 3075, loss = 1.052917\n",
            "Iter 3076, loss = 0.8893606\n",
            "Iter 3077, loss = 1.4166697\n",
            "Iter 3078, loss = 0.9879768\n",
            "Iter 3079, loss = 0.9503623\n",
            "Iter 3080, loss = 1.5240715\n",
            "Iter 3081, loss = 1.0252812\n",
            "Iter 3082, loss = 1.2630587\n",
            "Iter 3083, loss = 1.124079\n",
            "Iter 3084, loss = 1.0412672\n",
            "Iter 3085, loss = 1.1178045\n",
            "Iter 3086, loss = 1.0155407\n",
            "Iter 3087, loss = 1.1239719\n",
            "Iter 3088, loss = 1.3010231\n",
            "Iter 3089, loss = 1.231977\n",
            "Iter 3090, loss = 1.0683231\n",
            "Iter 3091, loss = 1.0223631\n",
            "Iter 3092, loss = 1.1761632\n",
            "Iter 3093, loss = 1.0692546\n",
            "Iter 3094, loss = 1.1041633\n",
            "Iter 3095, loss = 0.81009895\n",
            "Iter 3096, loss = 0.8806865\n",
            "Iter 3097, loss = 1.4282563\n",
            "Iter 3098, loss = 0.98813415\n",
            "Iter 3099, loss = 0.91252273\n",
            "Iter 3100, loss = 0.7770184\n",
            "Iter 3101, loss = 1.8370419\n",
            "Iter 3102, loss = 1.6452441\n",
            "Iter 3103, loss = 1.3573718\n",
            "Iter 3104, loss = 0.88399374\n",
            "Iter 3105, loss = 1.2196765\n",
            "Iter 3106, loss = 1.2802514\n",
            "Iter 3107, loss = 1.4051545\n",
            "Iter 3108, loss = 1.1789771\n",
            "Iter 3109, loss = 0.94529974\n",
            "Iter 3110, loss = 1.1834238\n",
            "Iter 3111, loss = 0.76556456\n",
            "Iter 3112, loss = 0.9349755\n",
            "Iter 3113, loss = 1.3758379\n",
            "Iter 3114, loss = 0.9046236\n",
            "Iter 3115, loss = 1.5928476\n",
            "Iter 3116, loss = 1.0240171\n",
            "Iter 3117, loss = 1.0012763\n",
            "Iter 3118, loss = 1.2020587\n",
            "Iter 3119, loss = 0.78576815\n",
            "Iter 3120, loss = 1.028543\n",
            "Iter 3121, loss = 1.3636713\n",
            "Iter 3122, loss = 1.1447741\n",
            "Iter 3123, loss = 1.0797255\n",
            "Iter 3124, loss = 1.276366\n",
            "Iter 3125, loss = 1.2206535\n",
            "Iter 3126, loss = 0.91113675\n",
            "Iter 3127, loss = 0.80681604\n",
            "Iter 3128, loss = 1.0151566\n",
            "Iter 3129, loss = 1.3704613\n",
            "Iter 3130, loss = 1.028157\n",
            "Iter 3131, loss = 1.1738472\n",
            "Iter 3132, loss = 1.139931\n",
            "Iter 3133, loss = 0.7585782\n",
            "Iter 3134, loss = 1.1224197\n",
            "Iter 3135, loss = 1.1818675\n",
            "Iter 3136, loss = 1.4337599\n",
            "Iter 3137, loss = 0.90365994\n",
            "Iter 3138, loss = 1.2094746\n",
            "Iter 3139, loss = 0.76133376\n",
            "Iter 3140, loss = 0.9311909\n",
            "Iter 3141, loss = 1.1154141\n",
            "Iter 3142, loss = 1.0553048\n",
            "Iter 3143, loss = 1.0701028\n",
            "Iter 3144, loss = 1.6780511\n",
            "Iter 3145, loss = 0.89661646\n",
            "Iter 3146, loss = 1.5629927\n",
            "Iter 3147, loss = 1.0256014\n",
            "Iter 3148, loss = 1.3075955\n",
            "Iter 3149, loss = 0.98942864\n",
            "Iter 3150, loss = 0.90644765\n",
            "Iter 3151, loss = 0.9500934\n",
            "Iter 3152, loss = 1.0300009\n",
            "Iter 3153, loss = 0.8189393\n",
            "Iter 3154, loss = 0.89738494\n",
            "Iter 3155, loss = 1.0649539\n",
            "Iter 3156, loss = 1.0992347\n",
            "Iter 3157, loss = 1.1199776\n",
            "Iter 3158, loss = 0.712331\n",
            "Iter 3159, loss = 1.0693774\n",
            "Iter 3160, loss = 1.0058992\n",
            "Iter 3161, loss = 1.0318561\n",
            "Iter 3162, loss = 1.5984519\n",
            "Iter 3163, loss = 1.1252289\n",
            "Iter 3164, loss = 1.1711363\n",
            "Iter 3165, loss = 0.8082648\n",
            "Iter 3166, loss = 1.3450177\n",
            "Iter 3167, loss = 1.0851281\n",
            "Iter 3168, loss = 1.0874431\n",
            "Iter 3169, loss = 1.6327322\n",
            "Iter 3170, loss = 1.0739014\n",
            "Iter 3171, loss = 1.0796667\n",
            "Iter 3172, loss = 1.0971496\n",
            "Iter 3173, loss = 0.9368886\n",
            "Iter 3174, loss = 0.9638564\n",
            "Iter 3175, loss = 1.1881316\n",
            "Iter 3176, loss = 1.0947995\n",
            "Iter 3177, loss = 1.4897109\n",
            "Iter 3178, loss = 0.9318218\n",
            "Iter 3179, loss = 0.9903856\n",
            "Iter 3180, loss = 1.4752169\n",
            "Iter 3181, loss = 0.7657414\n",
            "Iter 3182, loss = 1.0934775\n",
            "Iter 3183, loss = 1.2207127\n",
            "Iter 3184, loss = 1.0738561\n",
            "Iter 3185, loss = 1.2076107\n",
            "Iter 3186, loss = 1.1056888\n",
            "Iter 3187, loss = 0.9503194\n",
            "Iter 3188, loss = 1.0553372\n",
            "Iter 3189, loss = 1.3224899\n",
            "Iter 3190, loss = 1.1347666\n",
            "Iter 3191, loss = 1.0936668\n",
            "Iter 3192, loss = 1.1202145\n",
            "Iter 3193, loss = 1.1847103\n",
            "Iter 3194, loss = 1.2708502\n",
            "Iter 3195, loss = 1.5652323\n",
            "Iter 3196, loss = 1.1704502\n",
            "Iter 3197, loss = 1.4278474\n",
            "Iter 3198, loss = 0.96768117\n",
            "Iter 3199, loss = 1.2043576\n",
            "Iter 3200, loss = 0.85857093\n",
            "Iter 3201, loss = 1.0265738\n",
            "Iter 3202, loss = 1.776506\n",
            "Iter 3203, loss = 0.9404544\n",
            "Iter 3204, loss = 1.5241023\n",
            "Iter 3205, loss = 1.475947\n",
            "Iter 3206, loss = 0.85502577\n",
            "Iter 3207, loss = 1.1198704\n",
            "Iter 3208, loss = 0.986543\n",
            "Iter 3209, loss = 1.0802684\n",
            "Iter 3210, loss = 1.0868154\n",
            "Iter 3211, loss = 0.94679534\n",
            "Iter 3212, loss = 0.73091984\n",
            "Iter 3213, loss = 1.2105802\n",
            "Iter 3214, loss = 1.1022611\n",
            "Iter 3215, loss = 0.9992622\n",
            "Iter 3216, loss = 1.51707\n",
            "Iter 3217, loss = 1.1418955\n",
            "Iter 3218, loss = 1.3017337\n",
            "Iter 3219, loss = 1.0868421\n",
            "Iter 3220, loss = 1.169174\n",
            "Iter 3221, loss = 1.0196425\n",
            "Iter 3222, loss = 1.0010782\n",
            "Iter 3223, loss = 1.2389032\n",
            "Iter 3224, loss = 1.4902282\n",
            "Iter 3225, loss = 0.9217413\n",
            "Iter 3226, loss = 1.4184997\n",
            "Iter 3227, loss = 1.5213662\n",
            "Iter 3228, loss = 0.95437235\n",
            "Iter 3229, loss = 0.81840575\n",
            "Iter 3230, loss = 1.0106916\n",
            "Iter 3231, loss = 0.851215\n",
            "Iter 3232, loss = 0.9473974\n",
            "Iter 3233, loss = 1.148283\n",
            "Iter 3234, loss = 1.7069876\n",
            "Iter 3235, loss = 1.2369522\n",
            "Iter 3236, loss = 1.53718\n",
            "Iter 3237, loss = 0.9441314\n",
            "Iter 3238, loss = 1.7584043\n",
            "Iter 3239, loss = 0.992756\n",
            "Iter 3240, loss = 1.0055497\n",
            "Iter 3241, loss = 1.4377129\n",
            "Iter 3242, loss = 0.9011987\n",
            "Iter 3243, loss = 1.149091\n",
            "Iter 3244, loss = 1.0816022\n",
            "Iter 3245, loss = 1.6982269\n",
            "Iter 3246, loss = 1.7260168\n",
            "Iter 3247, loss = 1.0570658\n",
            "Iter 3248, loss = 1.3279285\n",
            "Iter 3249, loss = 0.97982454\n",
            "Iter 3250, loss = 1.1337535\n",
            "Iter 3251, loss = 1.1144749\n",
            "Iter 3252, loss = 0.89924246\n",
            "Iter 3253, loss = 0.8474823\n",
            "Iter 3254, loss = 1.1619449\n",
            "Iter 3255, loss = 1.3095944\n",
            "Iter 3256, loss = 0.8178736\n",
            "Iter 3257, loss = 1.3225982\n",
            "Iter 3258, loss = 1.2891469\n",
            "Iter 3259, loss = 1.0869594\n",
            "Iter 3260, loss = 0.93490994\n",
            "Iter 3261, loss = 1.1220293\n",
            "Iter 3262, loss = 0.9247532\n",
            "Iter 3263, loss = 1.5242751\n",
            "Iter 3264, loss = 1.0109941\n",
            "Iter 3265, loss = 1.2502725\n",
            "Iter 3266, loss = 0.8432647\n",
            "Iter 3267, loss = 1.5233786\n",
            "Iter 3268, loss = 1.0059128\n",
            "Iter 3269, loss = 1.1061218\n",
            "Iter 3270, loss = 1.320596\n",
            "Iter 3271, loss = 1.0943396\n",
            "Iter 3272, loss = 1.1087604\n",
            "Iter 3273, loss = 1.1296748\n",
            "Iter 3274, loss = 0.89807856\n",
            "Iter 3275, loss = 0.9655348\n",
            "Iter 3276, loss = 1.3120322\n",
            "Iter 3277, loss = 1.9568815\n",
            "Iter 3278, loss = 0.8598732\n",
            "Iter 3279, loss = 1.0502756\n",
            "Iter 3280, loss = 0.9685449\n",
            "Iter 3281, loss = 1.2032188\n",
            "Iter 3282, loss = 0.92234766\n",
            "Iter 3283, loss = 0.9341996\n",
            "Iter 3284, loss = 1.2049568\n",
            "Iter 3285, loss = 1.07388\n",
            "Iter 3286, loss = 0.7763923\n",
            "Iter 3287, loss = 1.107462\n",
            "Iter 3288, loss = 0.86004907\n",
            "Iter 3289, loss = 1.1264429\n",
            "Iter 3290, loss = 0.8367125\n",
            "Iter 3291, loss = 0.93554795\n",
            "Iter 3292, loss = 1.1091926\n",
            "Iter 3293, loss = 1.0114692\n",
            "Iter 3294, loss = 0.9006805\n",
            "Iter 3295, loss = 0.8639325\n",
            "Iter 3296, loss = 0.99233884\n",
            "Iter 3297, loss = 1.1812128\n",
            "Iter 3298, loss = 0.9475535\n",
            "Iter 3299, loss = 1.3525662\n",
            "Iter 3300, loss = 1.0208923\n",
            "Iter 3301, loss = 1.0576767\n",
            "Iter 3302, loss = 1.3275123\n",
            "Iter 3303, loss = 1.035677\n",
            "Iter 3304, loss = 1.0884836\n",
            "Iter 3305, loss = 1.0566082\n",
            "Iter 3306, loss = 1.036125\n",
            "Iter 3307, loss = 0.92843187\n",
            "Iter 3308, loss = 1.416744\n",
            "Iter 3309, loss = 1.4215698\n",
            "Iter 3310, loss = 1.2216492\n",
            "Iter 3311, loss = 1.5103068\n",
            "Iter 3312, loss = 1.261448\n",
            "Iter 3313, loss = 1.1012527\n",
            "Iter 3314, loss = 1.2087936\n",
            "Iter 3315, loss = 0.9789664\n",
            "Iter 3316, loss = 1.2178397\n",
            "Iter 3317, loss = 1.1448861\n",
            "Iter 3318, loss = 0.8656467\n",
            "Iter 3319, loss = 1.786745\n",
            "Iter 3320, loss = 1.406565\n",
            "Iter 3321, loss = 1.2809348\n",
            "Iter 3322, loss = 0.9722082\n",
            "Iter 3323, loss = 1.0835521\n",
            "Iter 3324, loss = 1.0122812\n",
            "Iter 3325, loss = 1.044625\n",
            "Iter 3326, loss = 1.00182\n",
            "Iter 3327, loss = 0.8895575\n",
            "Iter 3328, loss = 1.0745506\n",
            "Iter 3329, loss = 1.3796711\n",
            "Iter 3330, loss = 1.3889081\n",
            "Iter 3331, loss = 1.28529\n",
            "Iter 3332, loss = 0.9413341\n",
            "Iter 3333, loss = 0.8157588\n",
            "Iter 3334, loss = 1.4713271\n",
            "Iter 3335, loss = 0.67532414\n",
            "Iter 3336, loss = 0.9138849\n",
            "Iter 3337, loss = 0.7774114\n",
            "Iter 3338, loss = 1.1649771\n",
            "Iter 3339, loss = 1.0209272\n",
            "Iter 3340, loss = 0.9682951\n",
            "Iter 3341, loss = 1.4975109\n",
            "Iter 3342, loss = 1.0995094\n",
            "Iter 3343, loss = 1.1123583\n",
            "Iter 3344, loss = 1.4536266\n",
            "Iter 3345, loss = 1.3513147\n",
            "Iter 3346, loss = 1.0774813\n",
            "Iter 3347, loss = 0.942051\n",
            "Iter 3348, loss = 1.4216783\n",
            "Iter 3349, loss = 1.1285291\n",
            "Iter 3350, loss = 0.97352624\n",
            "Iter 3351, loss = 1.1903076\n",
            "Iter 3352, loss = 1.2187665\n",
            "Iter 3353, loss = 0.80625284\n",
            "Iter 3354, loss = 1.1535459\n",
            "Iter 3355, loss = 0.9829936\n",
            "Iter 3356, loss = 1.2053487\n",
            "Iter 3357, loss = 0.9513154\n",
            "Iter 3358, loss = 1.2586849\n",
            "Iter 3359, loss = 1.2914497\n",
            "Iter 3360, loss = 0.9076593\n",
            "Iter 3361, loss = 0.8023569\n",
            "Iter 3362, loss = 1.1774216\n",
            "Iter 3363, loss = 1.0668852\n",
            "Iter 3364, loss = 1.0817378\n",
            "Iter 3365, loss = 1.0756109\n",
            "Iter 3366, loss = 0.9649579\n",
            "Iter 3367, loss = 1.2506096\n",
            "Iter 3368, loss = 0.6787423\n",
            "Iter 3369, loss = 1.2631242\n",
            "Iter 3370, loss = 0.97967815\n",
            "Iter 3371, loss = 1.1785505\n",
            "Iter 3372, loss = 1.1330829\n",
            "Iter 3373, loss = 1.2606665\n",
            "Iter 3374, loss = 1.3106338\n",
            "Iter 3375, loss = 1.1420289\n",
            "Iter 3376, loss = 1.0647223\n",
            "Iter 3377, loss = 1.169481\n",
            "Iter 3378, loss = 0.8509203\n",
            "Iter 3379, loss = 1.447922\n",
            "Iter 3380, loss = 1.3329408\n",
            "Iter 3381, loss = 0.9067931\n",
            "Iter 3382, loss = 0.84822726\n",
            "Iter 3383, loss = 1.0885134\n",
            "Iter 3384, loss = 1.0686029\n",
            "Iter 3385, loss = 1.2840948\n",
            "Iter 3386, loss = 0.8788798\n",
            "Iter 3387, loss = 0.96641916\n",
            "Iter 3388, loss = 0.94313717\n",
            "Iter 3389, loss = 1.1791418\n",
            "Iter 3390, loss = 0.85235345\n",
            "Iter 3391, loss = 0.65615237\n",
            "Iter 3392, loss = 0.8521154\n",
            "Iter 3393, loss = 1.2239182\n",
            "Iter 3394, loss = 1.3272312\n",
            "Iter 3395, loss = 1.1650566\n",
            "Iter 3396, loss = 1.3309113\n",
            "Iter 3397, loss = 1.0745687\n",
            "Iter 3398, loss = 1.4203706\n",
            "Iter 3399, loss = 0.96911526\n",
            "Iter 3400, loss = 0.9741559\n",
            "Iter 3401, loss = 0.8665019\n",
            "Iter 3402, loss = 1.0095854\n",
            "Iter 3403, loss = 1.0754964\n",
            "Iter 3404, loss = 1.1276994\n",
            "Iter 3405, loss = 1.1520352\n",
            "Iter 3406, loss = 0.8033239\n",
            "Iter 3407, loss = 0.886104\n",
            "Iter 3408, loss = 0.79570323\n",
            "Iter 3409, loss = 0.9643556\n",
            "Iter 3410, loss = 1.324583\n",
            "Iter 3411, loss = 0.76734346\n",
            "Iter 3412, loss = 1.9552859\n",
            "Iter 3413, loss = 1.2950242\n",
            "Iter 3414, loss = 1.1316237\n",
            "Iter 3415, loss = 1.2152728\n",
            "Iter 3416, loss = 1.0827289\n",
            "Iter 3417, loss = 1.0014138\n",
            "Iter 3418, loss = 0.7220123\n",
            "Iter 3419, loss = 1.21394\n",
            "Iter 3420, loss = 1.4184377\n",
            "Iter 3421, loss = 0.8511965\n",
            "Iter 3422, loss = 1.3264229\n",
            "Iter 3423, loss = 1.1104491\n",
            "Iter 3424, loss = 0.9137097\n",
            "Iter 3425, loss = 0.91288364\n",
            "Iter 3426, loss = 0.62652445\n",
            "Iter 3427, loss = 1.017442\n",
            "Iter 3428, loss = 0.9816468\n",
            "Iter 3429, loss = 1.2924962\n",
            "Iter 3430, loss = 1.0609372\n",
            "Iter 3431, loss = 1.3159773\n",
            "Iter 3432, loss = 0.9280708\n",
            "Iter 3433, loss = 0.97443295\n",
            "Iter 3434, loss = 0.9352168\n",
            "Iter 3435, loss = 0.8742117\n",
            "Iter 3436, loss = 1.4697939\n",
            "Iter 3437, loss = 0.98602253\n",
            "Iter 3438, loss = 1.7078172\n",
            "Iter 3439, loss = 1.4476678\n",
            "Iter 3440, loss = 1.76508\n",
            "Iter 3441, loss = 0.9989388\n",
            "Iter 3442, loss = 1.1132636\n",
            "Iter 3443, loss = 1.4762323\n",
            "Iter 3444, loss = 0.85153735\n",
            "Iter 3445, loss = 1.5127275\n",
            "Iter 3446, loss = 1.0270655\n",
            "Iter 3447, loss = 0.975314\n",
            "Iter 3448, loss = 1.1592357\n",
            "Iter 3449, loss = 1.1562026\n",
            "Iter 3450, loss = 1.0778946\n",
            "Iter 3451, loss = 1.0473627\n",
            "Iter 3452, loss = 0.9354124\n",
            "Iter 3453, loss = 1.3273821\n",
            "Iter 3454, loss = 0.9849327\n",
            "Iter 3455, loss = 1.1360725\n",
            "Iter 3456, loss = 0.9046052\n",
            "Iter 3457, loss = 1.3465827\n",
            "Iter 3458, loss = 1.0864847\n",
            "Iter 3459, loss = 1.4000885\n",
            "Iter 3460, loss = 1.258492\n",
            "Iter 3461, loss = 1.1828904\n",
            "Iter 3462, loss = 0.7426461\n",
            "Iter 3463, loss = 0.98634505\n",
            "Iter 3464, loss = 1.0980486\n",
            "Iter 3465, loss = 1.0721118\n",
            "Iter 3466, loss = 1.2096456\n",
            "Iter 3467, loss = 1.1546125\n",
            "Iter 3468, loss = 0.7902055\n",
            "Iter 3469, loss = 0.7981054\n",
            "Iter 3470, loss = 1.1729462\n",
            "Iter 3471, loss = 1.1499839\n",
            "Iter 3472, loss = 0.8540112\n",
            "Iter 3473, loss = 1.1610544\n",
            "Iter 3474, loss = 0.9685476\n",
            "Iter 3475, loss = 0.96007496\n",
            "Iter 3476, loss = 1.3348626\n",
            "Iter 3477, loss = 1.116014\n",
            "Iter 3478, loss = 1.0874238\n",
            "Iter 3479, loss = 1.3172442\n",
            "Iter 3480, loss = 0.9917453\n",
            "Iter 3481, loss = 1.2018712\n",
            "Iter 3482, loss = 1.1585057\n",
            "Iter 3483, loss = 0.81154364\n",
            "Iter 3484, loss = 0.8784697\n",
            "Iter 3485, loss = 1.163093\n",
            "Iter 3486, loss = 0.91272223\n",
            "Iter 3487, loss = 0.98654395\n",
            "Iter 3488, loss = 1.1367459\n",
            "Iter 3489, loss = 0.65148306\n",
            "Iter 3490, loss = 1.5659962\n",
            "Iter 3491, loss = 1.0202862\n",
            "Iter 3492, loss = 1.2784567\n",
            "Iter 3493, loss = 0.982952\n",
            "Iter 3494, loss = 1.092917\n",
            "Iter 3495, loss = 1.3148407\n",
            "Iter 3496, loss = 1.0080734\n",
            "Iter 3497, loss = 0.90418416\n",
            "Iter 3498, loss = 1.1559522\n",
            "Iter 3499, loss = 0.7713756\n",
            "Iter 3500, loss = 1.4110967\n",
            "Iter 3501, loss = 1.0163201\n",
            "Iter 3502, loss = 1.186477\n",
            "Iter 3503, loss = 1.1359909\n",
            "Iter 3504, loss = 1.0960125\n",
            "Iter 3505, loss = 1.888685\n",
            "Iter 3506, loss = 1.1324987\n",
            "Iter 3507, loss = 1.3735125\n",
            "Iter 3508, loss = 1.263536\n",
            "Iter 3509, loss = 1.3771336\n",
            "Iter 3510, loss = 1.1855545\n",
            "Iter 3511, loss = 1.2356224\n",
            "Iter 3512, loss = 1.1981735\n",
            "Iter 3513, loss = 1.0496458\n",
            "Iter 3514, loss = 1.177196\n",
            "Iter 3515, loss = 1.1362741\n",
            "Iter 3516, loss = 1.0710183\n",
            "Iter 3517, loss = 1.561212\n",
            "Iter 3518, loss = 1.4238751\n",
            "Iter 3519, loss = 1.2693177\n",
            "Iter 3520, loss = 0.9794738\n",
            "Iter 3521, loss = 0.8358383\n",
            "Iter 3522, loss = 0.9890076\n",
            "Iter 3523, loss = 1.1202106\n",
            "Iter 3524, loss = 1.0994775\n",
            "Iter 3525, loss = 1.1617976\n",
            "Iter 3526, loss = 1.2404515\n",
            "Iter 3527, loss = 1.2233558\n",
            "Iter 3528, loss = 1.1360459\n",
            "Iter 3529, loss = 1.0618228\n",
            "Iter 3530, loss = 1.0557773\n",
            "Iter 3531, loss = 1.0739847\n",
            "Iter 3532, loss = 1.094579\n",
            "Iter 3533, loss = 0.955714\n",
            "Iter 3534, loss = 1.1329623\n",
            "Iter 3535, loss = 1.1186874\n",
            "Iter 3536, loss = 1.1069071\n",
            "Iter 3537, loss = 1.3648615\n",
            "Iter 3538, loss = 1.151774\n",
            "Iter 3539, loss = 1.3793072\n",
            "Iter 3540, loss = 1.1583517\n",
            "Iter 3541, loss = 1.2869929\n",
            "Iter 3542, loss = 1.0326452\n",
            "Iter 3543, loss = 0.8088518\n",
            "Iter 3544, loss = 1.1227112\n",
            "Iter 3545, loss = 1.1043293\n",
            "Iter 3546, loss = 1.4026029\n",
            "Iter 3547, loss = 1.2122898\n",
            "Iter 3548, loss = 0.87825215\n",
            "Iter 3549, loss = 1.0438092\n",
            "Iter 3550, loss = 1.0597956\n",
            "Iter 3551, loss = 1.0744598\n",
            "Iter 3552, loss = 0.93799764\n",
            "Iter 3553, loss = 0.88217497\n",
            "Iter 3554, loss = 0.96370494\n",
            "Iter 3555, loss = 1.1650283\n",
            "Iter 3556, loss = 0.9049098\n",
            "Iter 3557, loss = 1.0492887\n",
            "Iter 3558, loss = 0.91376555\n",
            "Iter 3559, loss = 1.2134371\n",
            "Iter 3560, loss = 1.2101865\n",
            "Iter 3561, loss = 0.97464156\n",
            "Iter 3562, loss = 1.5137601\n",
            "Iter 3563, loss = 0.90096074\n",
            "Iter 3564, loss = 1.2418969\n",
            "Iter 3565, loss = 1.1846337\n",
            "Iter 3566, loss = 0.92971426\n",
            "Iter 3567, loss = 1.0387928\n",
            "Iter 3568, loss = 1.1683507\n",
            "Iter 3569, loss = 1.0188737\n",
            "Iter 3570, loss = 1.606828\n",
            "Iter 3571, loss = 1.1394386\n",
            "Iter 3572, loss = 0.91847783\n",
            "Iter 3573, loss = 1.1564031\n",
            "Iter 3574, loss = 1.1522827\n",
            "Iter 3575, loss = 1.1637864\n",
            "Iter 3576, loss = 0.95295596\n",
            "Iter 3577, loss = 1.0248022\n",
            "Iter 3578, loss = 1.0270928\n",
            "Iter 3579, loss = 1.1089485\n",
            "Iter 3580, loss = 1.3241041\n",
            "Iter 3581, loss = 1.0291376\n",
            "Iter 3582, loss = 0.7918813\n",
            "Iter 3583, loss = 1.2138715\n",
            "Iter 3584, loss = 1.1512444\n",
            "Iter 3585, loss = 1.530033\n",
            "Iter 3586, loss = 0.97680336\n",
            "Iter 3587, loss = 1.0349141\n",
            "Iter 3588, loss = 1.2050922\n",
            "Iter 3589, loss = 1.0843544\n",
            "Iter 3590, loss = 1.3145113\n",
            "Iter 3591, loss = 1.000386\n",
            "Iter 3592, loss = 1.1076119\n",
            "Iter 3593, loss = 0.99596775\n",
            "Iter 3594, loss = 1.1374927\n",
            "Iter 3595, loss = 1.4308147\n",
            "Iter 3596, loss = 1.0940418\n",
            "Iter 3597, loss = 1.2003093\n",
            "Iter 3598, loss = 0.9907888\n",
            "Iter 3599, loss = 1.1416225\n",
            "Iter 3600, loss = 0.96487665\n",
            "Iter 3601, loss = 1.2526525\n",
            "Iter 3602, loss = 1.4015105\n",
            "Iter 3603, loss = 0.9866791\n",
            "Iter 3604, loss = 0.97278464\n",
            "Iter 3605, loss = 1.1962395\n",
            "Iter 3606, loss = 1.4358143\n",
            "Iter 3607, loss = 1.0285004\n",
            "Iter 3608, loss = 1.2821021\n",
            "Iter 3609, loss = 1.143743\n",
            "Iter 3610, loss = 0.9071862\n",
            "Iter 3611, loss = 1.3975494\n",
            "Iter 3612, loss = 1.2537788\n",
            "Iter 3613, loss = 0.98110163\n",
            "Iter 3614, loss = 0.87246615\n",
            "Iter 3615, loss = 1.2694566\n",
            "Iter 3616, loss = 1.0285728\n",
            "Iter 3617, loss = 0.89773744\n",
            "Iter 3618, loss = 0.95563066\n",
            "Iter 3619, loss = 1.2923628\n",
            "Iter 3620, loss = 0.7980299\n",
            "Iter 3621, loss = 0.8147846\n",
            "Iter 3622, loss = 0.950653\n",
            "Iter 3623, loss = 0.9860176\n",
            "Iter 3624, loss = 1.120634\n",
            "Iter 3625, loss = 1.0760223\n",
            "Iter 3626, loss = 1.0172836\n",
            "Iter 3627, loss = 0.8695086\n",
            "Iter 3628, loss = 1.3236732\n",
            "Iter 3629, loss = 0.8662883\n",
            "Iter 3630, loss = 0.75352645\n",
            "Iter 3631, loss = 1.1127002\n",
            "Iter 3632, loss = 1.0454619\n",
            "Iter 3633, loss = 1.3194087\n",
            "Iter 3634, loss = 1.2594455\n",
            "Iter 3635, loss = 1.3028183\n",
            "Iter 3636, loss = 1.3330868\n",
            "Iter 3637, loss = 0.7425612\n",
            "Iter 3638, loss = 1.1042254\n",
            "Iter 3639, loss = 1.6750495\n",
            "Iter 3640, loss = 0.9978287\n",
            "Iter 3641, loss = 0.93578905\n",
            "Iter 3642, loss = 1.0251722\n",
            "Iter 3643, loss = 1.0978714\n",
            "Iter 3644, loss = 0.97189796\n",
            "Iter 3645, loss = 1.1382129\n",
            "Iter 3646, loss = 0.88230824\n",
            "Iter 3647, loss = 1.9204538\n",
            "Iter 3648, loss = 1.2607703\n",
            "Iter 3649, loss = 1.0232848\n",
            "Iter 3650, loss = 1.061624\n",
            "Iter 3651, loss = 1.0236045\n",
            "Iter 3652, loss = 0.94316715\n",
            "Iter 3653, loss = 0.93756545\n",
            "Iter 3654, loss = 1.3033838\n",
            "Iter 3655, loss = 1.1141226\n",
            "Iter 3656, loss = 1.028832\n",
            "Iter 3657, loss = 2.0810533\n",
            "Iter 3658, loss = 1.1973954\n",
            "Iter 3659, loss = 1.1276609\n",
            "Iter 3660, loss = 0.87437636\n",
            "Iter 3661, loss = 0.8441387\n",
            "Iter 3662, loss = 1.2965868\n",
            "Iter 3663, loss = 0.7786987\n",
            "Iter 3664, loss = 1.152263\n",
            "Iter 3665, loss = 0.94362074\n",
            "Iter 3666, loss = 0.9848999\n",
            "Iter 3667, loss = 2.1925485\n",
            "Iter 3668, loss = 1.182931\n",
            "Iter 3669, loss = 1.1425395\n",
            "Iter 3670, loss = 1.0692654\n",
            "Iter 3671, loss = 1.1663575\n",
            "Iter 3672, loss = 1.0656645\n",
            "Iter 3673, loss = 0.8224951\n",
            "Iter 3674, loss = 1.0442212\n",
            "Iter 3675, loss = 1.2827464\n",
            "Iter 3676, loss = 1.2629223\n",
            "Iter 3677, loss = 1.0483049\n",
            "Iter 3678, loss = 0.816813\n",
            "Iter 3679, loss = 1.125376\n",
            "Iter 3680, loss = 1.10306\n",
            "Iter 3681, loss = 0.8914122\n",
            "Iter 3682, loss = 1.0670109\n",
            "Iter 3683, loss = 1.0752075\n",
            "Iter 3684, loss = 1.1379967\n",
            "Iter 3685, loss = 0.96168876\n",
            "Iter 3686, loss = 0.9295842\n",
            "Iter 3687, loss = 1.1735892\n",
            "Iter 3688, loss = 1.1199937\n",
            "Iter 3689, loss = 0.83191633\n",
            "Iter 3690, loss = 1.196132\n",
            "Iter 3691, loss = 1.1481459\n",
            "Iter 3692, loss = 1.1117237\n",
            "Iter 3693, loss = 0.9938899\n",
            "Iter 3694, loss = 0.86017406\n",
            "Iter 3695, loss = 1.5591624\n",
            "Iter 3696, loss = 1.054672\n",
            "Iter 3697, loss = 1.3494353\n",
            "Iter 3698, loss = 1.1147507\n",
            "Iter 3699, loss = 0.97113216\n",
            "Iter 3700, loss = 1.0611\n",
            "Iter 3701, loss = 0.95271254\n",
            "Iter 3702, loss = 1.336144\n",
            "Iter 3703, loss = 0.94467485\n",
            "Iter 3704, loss = 1.2916586\n",
            "Iter 3705, loss = 0.928525\n",
            "Iter 3706, loss = 1.2348881\n",
            "Iter 3707, loss = 1.2668383\n",
            "Iter 3708, loss = 0.8127239\n",
            "Iter 3709, loss = 0.98991275\n",
            "Iter 3710, loss = 1.1370144\n",
            "Iter 3711, loss = 1.2442248\n",
            "Iter 3712, loss = 1.1301427\n",
            "Iter 3713, loss = 1.0253344\n",
            "Iter 3714, loss = 1.3449392\n",
            "Iter 3715, loss = 0.9534687\n",
            "Iter 3716, loss = 1.1229954\n",
            "Iter 3717, loss = 1.0310117\n",
            "Iter 3718, loss = 1.0143749\n",
            "Iter 3719, loss = 1.0166667\n",
            "Iter 3720, loss = 1.01414\n",
            "Iter 3721, loss = 0.6942114\n",
            "Iter 3722, loss = 0.98313797\n",
            "Iter 3723, loss = 0.93077624\n",
            "Iter 3724, loss = 1.6283921\n",
            "Iter 3725, loss = 0.9619293\n",
            "Iter 3726, loss = 1.166759\n",
            "Iter 3727, loss = 1.3920875\n",
            "Iter 3728, loss = 1.1174643\n",
            "Iter 3729, loss = 1.399992\n",
            "Iter 3730, loss = 1.1234274\n",
            "Iter 3731, loss = 0.96504575\n",
            "Iter 3732, loss = 0.91131777\n",
            "Iter 3733, loss = 0.9859466\n",
            "Iter 3734, loss = 1.1256797\n",
            "Iter 3735, loss = 1.2700639\n",
            "Iter 3736, loss = 0.93936044\n",
            "Iter 3737, loss = 0.8156895\n",
            "Iter 3738, loss = 0.9991413\n",
            "Iter 3739, loss = 0.8590666\n",
            "Iter 3740, loss = 1.492164\n",
            "Iter 3741, loss = 1.1125833\n",
            "Iter 3742, loss = 0.8167132\n",
            "Iter 3743, loss = 1.06995\n",
            "Iter 3744, loss = 0.9504881\n",
            "Iter 3745, loss = 1.2071338\n",
            "Iter 3746, loss = 0.918317\n",
            "Iter 3747, loss = 1.060739\n",
            "Iter 3748, loss = 0.9691276\n",
            "Iter 3749, loss = 1.0473742\n",
            "Iter 3750, loss = 1.0221329\n",
            "Iter 3751, loss = 1.154804\n",
            "Iter 3752, loss = 1.0730267\n",
            "Iter 3753, loss = 1.1359427\n",
            "Iter 3754, loss = 1.3395584\n",
            "Iter 3755, loss = 1.3531296\n",
            "Iter 3756, loss = 0.907267\n",
            "Iter 3757, loss = 1.1521101\n",
            "Iter 3758, loss = 1.218801\n",
            "Iter 3759, loss = 1.1089442\n",
            "Iter 3760, loss = 0.75725305\n",
            "Iter 3761, loss = 0.8460554\n",
            "Iter 3762, loss = 0.81855965\n",
            "Iter 3763, loss = 1.0438828\n",
            "Iter 3764, loss = 1.0966548\n",
            "Iter 3765, loss = 0.97084963\n",
            "Iter 3766, loss = 0.79595405\n",
            "Iter 3767, loss = 1.0837204\n",
            "Iter 3768, loss = 1.1254184\n",
            "Iter 3769, loss = 0.7982518\n",
            "Iter 3770, loss = 1.1050345\n",
            "Iter 3771, loss = 1.5354586\n",
            "Iter 3772, loss = 1.4584994\n",
            "Iter 3773, loss = 0.99436307\n",
            "Iter 3774, loss = 1.2303623\n",
            "Iter 3775, loss = 0.9503006\n",
            "Iter 3776, loss = 0.8546867\n",
            "Iter 3777, loss = 0.8317374\n",
            "Iter 3778, loss = 0.9730423\n",
            "Iter 3779, loss = 1.1821258\n",
            "Iter 3780, loss = 1.0255362\n",
            "Iter 3781, loss = 1.0144696\n",
            "Iter 3782, loss = 0.90657836\n",
            "Iter 3783, loss = 0.8983822\n",
            "Iter 3784, loss = 1.3323243\n",
            "Iter 3785, loss = 1.38624\n",
            "Iter 3786, loss = 1.1477288\n",
            "Iter 3787, loss = 1.0750213\n",
            "Iter 3788, loss = 1.2338929\n",
            "Iter 3789, loss = 1.1242497\n",
            "Iter 3790, loss = 0.7953538\n",
            "Iter 3791, loss = 1.3343387\n",
            "Iter 3792, loss = 1.2276735\n",
            "Iter 3793, loss = 1.4307146\n",
            "Iter 3794, loss = 1.2087756\n",
            "Iter 3795, loss = 0.91371\n",
            "Iter 3796, loss = 0.8648206\n",
            "Iter 3797, loss = 1.3668458\n",
            "Iter 3798, loss = 1.22208\n",
            "Iter 3799, loss = 1.1199276\n",
            "Iter 3800, loss = 1.1554855\n",
            "Iter 3801, loss = 0.88508594\n",
            "Iter 3802, loss = 1.0508935\n",
            "Iter 3803, loss = 1.7316165\n",
            "Iter 3804, loss = 1.0873271\n",
            "Iter 3805, loss = 1.0569175\n",
            "Iter 3806, loss = 0.89550406\n",
            "Iter 3807, loss = 1.1996115\n",
            "Iter 3808, loss = 1.2236924\n",
            "Iter 3809, loss = 1.4512398\n",
            "Iter 3810, loss = 1.0570706\n",
            "Iter 3811, loss = 1.212863\n",
            "Iter 3812, loss = 0.8744415\n",
            "Iter 3813, loss = 1.0857902\n",
            "Iter 3814, loss = 1.1417931\n",
            "Iter 3815, loss = 1.54773\n",
            "Iter 3816, loss = 1.1761053\n",
            "Iter 3817, loss = 1.1350002\n",
            "Iter 3818, loss = 0.948228\n",
            "Iter 3819, loss = 1.2209903\n",
            "Iter 3820, loss = 0.95716405\n",
            "Iter 3821, loss = 1.1809124\n",
            "Iter 3822, loss = 1.4097383\n",
            "Iter 3823, loss = 0.915195\n",
            "Iter 3824, loss = 1.3656121\n",
            "Iter 3825, loss = 1.093688\n",
            "Iter 3826, loss = 1.0507056\n",
            "Iter 3827, loss = 1.1605262\n",
            "Iter 3828, loss = 1.0613451\n",
            "Iter 3829, loss = 0.8347206\n",
            "Iter 3830, loss = 0.9428764\n",
            "Iter 3831, loss = 0.81637037\n",
            "Iter 3832, loss = 1.1977767\n",
            "Iter 3833, loss = 1.5887363\n",
            "Iter 3834, loss = 1.0296521\n",
            "Iter 3835, loss = 1.1149987\n",
            "Iter 3836, loss = 0.739362\n",
            "Iter 3837, loss = 1.1500382\n",
            "Iter 3838, loss = 1.1463093\n",
            "Iter 3839, loss = 1.0949455\n",
            "Iter 3840, loss = 1.3954089\n",
            "Iter 3841, loss = 0.8826214\n",
            "Iter 3842, loss = 1.2907296\n",
            "Iter 3843, loss = 1.0451527\n",
            "Iter 3844, loss = 0.929481\n",
            "Iter 3845, loss = 0.901775\n",
            "Iter 3846, loss = 1.0328729\n",
            "Iter 3847, loss = 0.99662805\n",
            "Iter 3848, loss = 1.2649235\n",
            "Iter 3849, loss = 1.1634854\n",
            "Iter 3850, loss = 1.0158215\n",
            "Iter 3851, loss = 1.2907169\n",
            "Iter 3852, loss = 1.0937768\n",
            "Iter 3853, loss = 1.060202\n",
            "Iter 3854, loss = 1.5839581\n",
            "Iter 3855, loss = 1.0339371\n",
            "Iter 3856, loss = 0.91882527\n",
            "Iter 3857, loss = 1.1477742\n",
            "Iter 3858, loss = 0.9981508\n",
            "Iter 3859, loss = 0.9656573\n",
            "Iter 3860, loss = 0.9904897\n",
            "Iter 3861, loss = 1.1715727\n",
            "Iter 3862, loss = 1.2147412\n",
            "Iter 3863, loss = 1.134357\n",
            "Iter 3864, loss = 1.1382853\n",
            "Iter 3865, loss = 1.5440792\n",
            "Iter 3866, loss = 1.1013889\n",
            "Iter 3867, loss = 0.84805197\n",
            "Iter 3868, loss = 0.98217845\n",
            "Iter 3869, loss = 1.3161118\n",
            "Iter 3870, loss = 0.74363285\n",
            "Iter 3871, loss = 1.2783117\n",
            "Iter 3872, loss = 1.2973309\n",
            "Iter 3873, loss = 0.86926115\n",
            "Iter 3874, loss = 1.1397974\n",
            "Iter 3875, loss = 1.2814631\n",
            "Iter 3876, loss = 1.3882393\n",
            "Iter 3877, loss = 1.2421508\n",
            "Iter 3878, loss = 1.0813094\n",
            "Iter 3879, loss = 1.2002249\n",
            "Iter 3880, loss = 1.0620103\n",
            "Iter 3881, loss = 1.1226479\n",
            "Iter 3882, loss = 1.2188783\n",
            "Iter 3883, loss = 1.1066439\n",
            "Iter 3884, loss = 1.20702\n",
            "Iter 3885, loss = 1.1206427\n",
            "Iter 3886, loss = 1.2342424\n",
            "Iter 3887, loss = 1.3207562\n",
            "Iter 3888, loss = 1.28977\n",
            "Iter 3889, loss = 1.2002014\n",
            "Iter 3890, loss = 0.9289392\n",
            "Iter 3891, loss = 1.0987817\n",
            "Iter 3892, loss = 0.89769053\n",
            "Iter 3893, loss = 1.2964462\n",
            "Iter 3894, loss = 1.1801659\n",
            "Iter 3895, loss = 0.90004563\n",
            "Iter 3896, loss = 1.1029601\n",
            "Iter 3897, loss = 0.9125394\n",
            "Iter 3898, loss = 1.2926285\n",
            "Iter 3899, loss = 1.0263897\n",
            "Iter 3900, loss = 1.1600957\n",
            "Iter 3901, loss = 0.77745736\n",
            "Iter 3902, loss = 0.9064087\n",
            "Iter 3903, loss = 1.1126926\n",
            "Iter 3904, loss = 1.2801406\n",
            "Iter 3905, loss = 0.9798855\n",
            "Iter 3906, loss = 1.359705\n",
            "Iter 3907, loss = 1.3805449\n",
            "Iter 3908, loss = 1.3992826\n",
            "Iter 3909, loss = 0.9152839\n",
            "Iter 3910, loss = 0.92154574\n",
            "Iter 3911, loss = 0.8287794\n",
            "Iter 3912, loss = 0.9296948\n",
            "Iter 3913, loss = 1.0126407\n",
            "Iter 3914, loss = 1.1024446\n",
            "Iter 3915, loss = 0.853454\n",
            "Iter 3916, loss = 1.1099805\n",
            "Iter 3917, loss = 0.92072517\n",
            "Iter 3918, loss = 1.5117016\n",
            "Iter 3919, loss = 1.1491305\n",
            "Iter 3920, loss = 1.4266834\n",
            "Iter 3921, loss = 1.1386758\n",
            "Iter 3922, loss = 1.2374707\n",
            "Iter 3923, loss = 1.0280397\n",
            "Iter 3924, loss = 1.1616654\n",
            "Iter 3925, loss = 1.1323748\n",
            "Iter 3926, loss = 1.0707281\n",
            "Iter 3927, loss = 1.014998\n",
            "Iter 3928, loss = 0.99711406\n",
            "Iter 3929, loss = 1.2356883\n",
            "Iter 3930, loss = 1.0008309\n",
            "Iter 3931, loss = 0.9454677\n",
            "Iter 3932, loss = 1.1021703\n",
            "Iter 3933, loss = 1.3162153\n",
            "Iter 3934, loss = 1.0638436\n",
            "Iter 3935, loss = 0.94956255\n",
            "Iter 3936, loss = 1.1144971\n",
            "Iter 3937, loss = 0.89787626\n",
            "Iter 3938, loss = 1.1880648\n",
            "Iter 3939, loss = 1.0084168\n",
            "Iter 3940, loss = 1.0701632\n",
            "Iter 3941, loss = 1.2674898\n",
            "Iter 3942, loss = 0.8823541\n",
            "Iter 3943, loss = 1.0796517\n",
            "Iter 3944, loss = 1.1064763\n",
            "Iter 3945, loss = 1.2063624\n",
            "Iter 3946, loss = 1.2023634\n",
            "Iter 3947, loss = 1.7484486\n",
            "Iter 3948, loss = 1.0697618\n",
            "Iter 3949, loss = 0.8889103\n",
            "Iter 3950, loss = 1.0597934\n",
            "Iter 3951, loss = 0.7751057\n",
            "Iter 3952, loss = 1.000593\n",
            "Iter 3953, loss = 1.0902325\n",
            "Iter 3954, loss = 1.5260851\n",
            "Iter 3955, loss = 1.7982376\n",
            "Iter 3956, loss = 1.1847545\n",
            "Iter 3957, loss = 1.1297107\n",
            "Iter 3958, loss = 0.96808386\n",
            "Iter 3959, loss = 1.0060322\n",
            "Iter 3960, loss = 1.0726517\n",
            "Iter 3961, loss = 0.9600611\n",
            "Iter 3962, loss = 0.943336\n",
            "Iter 3963, loss = 1.0681257\n",
            "Iter 3964, loss = 0.98073804\n",
            "Iter 3965, loss = 1.0223799\n",
            "Iter 3966, loss = 0.8158208\n",
            "Iter 3967, loss = 1.0160112\n",
            "Iter 3968, loss = 1.2882377\n",
            "Iter 3969, loss = 0.9346353\n",
            "Iter 3970, loss = 0.94218385\n",
            "Iter 3971, loss = 1.2049465\n",
            "Iter 3972, loss = 0.73122275\n",
            "Iter 3973, loss = 1.0336572\n",
            "Iter 3974, loss = 1.6191914\n",
            "Iter 3975, loss = 1.1971232\n",
            "Iter 3976, loss = 0.95592105\n",
            "Iter 3977, loss = 1.2735488\n",
            "Iter 3978, loss = 1.1760736\n",
            "Iter 3979, loss = 1.5333312\n",
            "Iter 3980, loss = 0.91060853\n",
            "Iter 3981, loss = 1.4073107\n",
            "Iter 3982, loss = 1.1308324\n",
            "Iter 3983, loss = 1.0528667\n",
            "Iter 3984, loss = 1.1025258\n",
            "Iter 3985, loss = 1.4280105\n",
            "Iter 3986, loss = 1.5374129\n",
            "Iter 3987, loss = 1.1516573\n",
            "Iter 3988, loss = 1.1636766\n",
            "Iter 3989, loss = 1.1359317\n",
            "Iter 3990, loss = 1.1783745\n",
            "Iter 3991, loss = 0.8900976\n",
            "Iter 3992, loss = 0.94910675\n",
            "Iter 3993, loss = 0.98550797\n",
            "Iter 3994, loss = 1.3987073\n",
            "Iter 3995, loss = 0.9301474\n",
            "Iter 3996, loss = 1.3453324\n",
            "Iter 3997, loss = 1.1225154\n",
            "Iter 3998, loss = 1.1611246\n",
            "Iter 3999, loss = 1.5667781\n",
            "Iter 4000, loss = 1.0038534\n",
            "Iter 4001, loss = 0.9349861\n",
            "Iter 4002, loss = 1.3050907\n",
            "Iter 4003, loss = 0.88910997\n",
            "Iter 4004, loss = 1.6306481\n",
            "Iter 4005, loss = 1.3024936\n",
            "Iter 4006, loss = 1.2311014\n",
            "Iter 4007, loss = 0.9049958\n",
            "Iter 4008, loss = 1.0722954\n",
            "Iter 4009, loss = 1.4929731\n",
            "Iter 4010, loss = 1.1078553\n",
            "Iter 4011, loss = 1.1569972\n",
            "Iter 4012, loss = 0.7942772\n",
            "Iter 4013, loss = 1.2734712\n",
            "Iter 4014, loss = 0.92952883\n",
            "Iter 4015, loss = 0.9304711\n",
            "Iter 4016, loss = 1.1479948\n",
            "Iter 4017, loss = 1.2194695\n",
            "Iter 4018, loss = 1.8704414\n",
            "Iter 4019, loss = 0.9901626\n",
            "Iter 4020, loss = 1.3206705\n",
            "Iter 4021, loss = 0.9279045\n",
            "Iter 4022, loss = 1.3074814\n",
            "Iter 4023, loss = 1.1151426\n",
            "Iter 4024, loss = 1.0939338\n",
            "Iter 4025, loss = 1.0226302\n",
            "Iter 4026, loss = 0.9777023\n",
            "Iter 4027, loss = 1.2368264\n",
            "Iter 4028, loss = 1.0844878\n",
            "Iter 4029, loss = 1.1806552\n",
            "Iter 4030, loss = 1.0993024\n",
            "Iter 4031, loss = 1.2114141\n",
            "Iter 4032, loss = 0.7867223\n",
            "Iter 4033, loss = 0.74446654\n",
            "Iter 4034, loss = 1.0816514\n",
            "Iter 4035, loss = 1.1624436\n",
            "Iter 4036, loss = 0.9073746\n",
            "Iter 4037, loss = 1.4615473\n",
            "Iter 4038, loss = 1.3388418\n",
            "Iter 4039, loss = 1.1036421\n",
            "Iter 4040, loss = 1.2569262\n",
            "Iter 4041, loss = 0.81511086\n",
            "Iter 4042, loss = 1.2065687\n",
            "Iter 4043, loss = 0.86171174\n",
            "Iter 4044, loss = 0.8903768\n",
            "Iter 4045, loss = 1.575252\n",
            "Iter 4046, loss = 1.0755172\n",
            "Iter 4047, loss = 0.9551587\n",
            "Iter 4048, loss = 1.2229298\n",
            "Iter 4049, loss = 0.84476364\n",
            "Iter 4050, loss = 0.99086\n",
            "Iter 4051, loss = 0.8150247\n",
            "Iter 4052, loss = 0.77471393\n",
            "Iter 4053, loss = 1.3307142\n",
            "Iter 4054, loss = 1.0260079\n",
            "Iter 4055, loss = 0.9544704\n",
            "Iter 4056, loss = 1.0301144\n",
            "Iter 4057, loss = 1.2613618\n",
            "Iter 4058, loss = 1.6135509\n",
            "Iter 4059, loss = 0.90903413\n",
            "Iter 4060, loss = 1.109726\n",
            "Iter 4061, loss = 1.2230843\n",
            "Iter 4062, loss = 0.88249314\n",
            "Iter 4063, loss = 0.96243584\n",
            "Iter 4064, loss = 0.95493996\n",
            "Iter 4065, loss = 1.4097977\n",
            "Iter 4066, loss = 1.2000811\n",
            "Iter 4067, loss = 1.4338801\n",
            "Iter 4068, loss = 1.016225\n",
            "Iter 4069, loss = 0.73502445\n",
            "Iter 4070, loss = 1.1395721\n",
            "Iter 4071, loss = 1.3908756\n",
            "Iter 4072, loss = 2.1274014\n",
            "Iter 4073, loss = 0.7419282\n",
            "Iter 4074, loss = 0.74719316\n",
            "Iter 4075, loss = 1.1497604\n",
            "Iter 4076, loss = 0.87086964\n",
            "Iter 4077, loss = 1.1526129\n",
            "Iter 4078, loss = 0.87610155\n",
            "Iter 4079, loss = 1.3867092\n",
            "Iter 4080, loss = 1.2208626\n",
            "Iter 4081, loss = 1.1978846\n",
            "Iter 4082, loss = 1.0515214\n",
            "Iter 4083, loss = 1.2561766\n",
            "Iter 4084, loss = 1.338625\n",
            "Iter 4085, loss = 0.84815884\n",
            "Iter 4086, loss = 1.3254248\n",
            "Iter 4087, loss = 0.910041\n",
            "Iter 4088, loss = 1.1330087\n",
            "Iter 4089, loss = 1.1364105\n",
            "Iter 4090, loss = 1.0698543\n",
            "Iter 4091, loss = 1.3034763\n",
            "Iter 4092, loss = 0.7951004\n",
            "Iter 4093, loss = 1.3581144\n",
            "Iter 4094, loss = 1.1641579\n",
            "Iter 4095, loss = 1.0970774\n",
            "Iter 4096, loss = 1.1954324\n",
            "Iter 4097, loss = 1.1416855\n",
            "Iter 4098, loss = 1.5184066\n",
            "Iter 4099, loss = 0.87237984\n",
            "Iter 4100, loss = 1.1377864\n",
            "Iter 4101, loss = 0.94457054\n",
            "Iter 4102, loss = 1.3012693\n",
            "Iter 4103, loss = 1.357511\n",
            "Iter 4104, loss = 1.3864405\n",
            "Iter 4105, loss = 1.0738766\n",
            "Iter 4106, loss = 0.9602352\n",
            "Iter 4107, loss = 1.0934727\n",
            "Iter 4108, loss = 0.9696939\n",
            "Iter 4109, loss = 0.9452411\n",
            "Iter 4110, loss = 1.1039321\n",
            "Iter 4111, loss = 0.939259\n",
            "Iter 4112, loss = 0.97564495\n",
            "Iter 4113, loss = 1.2297125\n",
            "Iter 4114, loss = 1.1008639\n",
            "Iter 4115, loss = 1.1396161\n",
            "Iter 4116, loss = 0.7030536\n",
            "Iter 4117, loss = 0.98563254\n",
            "Iter 4118, loss = 1.1227195\n",
            "Iter 4119, loss = 1.2347867\n",
            "Iter 4120, loss = 1.6413496\n",
            "Iter 4121, loss = 1.2085104\n",
            "Iter 4122, loss = 1.1697533\n",
            "Iter 4123, loss = 0.9742166\n",
            "Iter 4124, loss = 0.9016708\n",
            "Iter 4125, loss = 0.9858793\n",
            "Iter 4126, loss = 1.3627173\n",
            "Iter 4127, loss = 1.2761745\n",
            "Iter 4128, loss = 1.1884137\n",
            "Iter 4129, loss = 0.79362184\n",
            "Iter 4130, loss = 0.83700585\n",
            "Iter 4131, loss = 1.4086046\n",
            "Iter 4132, loss = 0.9219049\n",
            "Iter 4133, loss = 0.88501513\n",
            "Iter 4134, loss = 0.7151979\n",
            "Iter 4135, loss = 1.1134112\n",
            "Iter 4136, loss = 1.150296\n",
            "Iter 4137, loss = 1.1485165\n",
            "Iter 4138, loss = 1.055272\n",
            "Iter 4139, loss = 1.4205077\n",
            "Iter 4140, loss = 0.8669297\n",
            "Iter 4141, loss = 1.2519583\n",
            "Iter 4142, loss = 1.1237862\n",
            "Iter 4143, loss = 1.0873814\n",
            "Iter 4144, loss = 1.1843705\n",
            "Iter 4145, loss = 1.2654656\n",
            "Iter 4146, loss = 0.66369224\n",
            "Iter 4147, loss = 0.9845559\n",
            "Iter 4148, loss = 1.5123775\n",
            "Iter 4149, loss = 0.96771044\n",
            "Iter 4150, loss = 1.0254358\n",
            "Iter 4151, loss = 1.0619593\n",
            "Iter 4152, loss = 1.1580532\n",
            "Iter 4153, loss = 1.1988034\n",
            "Iter 4154, loss = 1.0652559\n",
            "Iter 4155, loss = 1.0243748\n",
            "Iter 4156, loss = 1.0138198\n",
            "Iter 4157, loss = 0.85026515\n",
            "Iter 4158, loss = 1.3052647\n",
            "Iter 4159, loss = 0.9097502\n",
            "Iter 4160, loss = 0.8272493\n",
            "Iter 4161, loss = 1.0500467\n",
            "Iter 4162, loss = 1.5212023\n",
            "Iter 4163, loss = 1.0613968\n",
            "Iter 4164, loss = 1.2486883\n",
            "Iter 4165, loss = 0.80421174\n",
            "Iter 4166, loss = 1.0613112\n",
            "Iter 4167, loss = 0.79988706\n",
            "Iter 4168, loss = 1.0505304\n",
            "Iter 4169, loss = 0.8122009\n",
            "Iter 4170, loss = 0.8291358\n",
            "Iter 4171, loss = 0.91261125\n",
            "Iter 4172, loss = 0.99680114\n",
            "Iter 4173, loss = 1.4430003\n",
            "Iter 4174, loss = 1.2470686\n",
            "Iter 4175, loss = 1.644812\n",
            "Iter 4176, loss = 1.4171507\n",
            "Iter 4177, loss = 0.7134906\n",
            "Iter 4178, loss = 1.1790735\n",
            "Iter 4179, loss = 1.1898632\n",
            "Iter 4180, loss = 2.0410745\n",
            "Iter 4181, loss = 0.9892472\n",
            "Iter 4182, loss = 0.7890738\n",
            "Iter 4183, loss = 1.2802134\n",
            "Iter 4184, loss = 1.270741\n",
            "Iter 4185, loss = 0.858271\n",
            "Iter 4186, loss = 1.067881\n",
            "Iter 4187, loss = 0.9094714\n",
            "Iter 4188, loss = 0.9385\n",
            "Iter 4189, loss = 1.0689712\n",
            "Iter 4190, loss = 1.0959\n",
            "Iter 4191, loss = 1.1436377\n",
            "Iter 4192, loss = 1.0360413\n",
            "Iter 4193, loss = 1.1747565\n",
            "Iter 4194, loss = 1.0161111\n",
            "Iter 4195, loss = 1.2533121\n",
            "Iter 4196, loss = 1.2751143\n",
            "Iter 4197, loss = 1.3808011\n",
            "Iter 4198, loss = 1.1360497\n",
            "Iter 4199, loss = 1.1351464\n",
            "Iter 4200, loss = 0.9947941\n",
            "Iter 4201, loss = 1.0381536\n",
            "Iter 4202, loss = 1.0818245\n",
            "Iter 4203, loss = 0.87351537\n",
            "Iter 4204, loss = 1.2056706\n",
            "Iter 4205, loss = 1.7514131\n",
            "Iter 4206, loss = 0.9060422\n",
            "Iter 4207, loss = 1.1482326\n",
            "Iter 4208, loss = 1.0885289\n",
            "Iter 4209, loss = 1.0011954\n",
            "Iter 4210, loss = 1.4866481\n",
            "Iter 4211, loss = 0.96163356\n",
            "Iter 4212, loss = 0.90764225\n",
            "Iter 4213, loss = 1.1722988\n",
            "Iter 4214, loss = 0.8015312\n",
            "Iter 4215, loss = 1.0509894\n",
            "Iter 4216, loss = 0.77323043\n",
            "Iter 4217, loss = 1.220197\n",
            "Iter 4218, loss = 1.1784048\n",
            "Iter 4219, loss = 0.85338026\n",
            "Iter 4220, loss = 0.9076597\n",
            "Iter 4221, loss = 1.4638908\n",
            "Iter 4222, loss = 1.1777365\n",
            "Iter 4223, loss = 1.0640152\n",
            "Iter 4224, loss = 0.7928734\n",
            "Iter 4225, loss = 1.0066609\n",
            "Iter 4226, loss = 1.1242707\n",
            "Iter 4227, loss = 1.0895503\n",
            "Iter 4228, loss = 0.77950764\n",
            "Iter 4229, loss = 0.88815016\n",
            "Iter 4230, loss = 1.4102521\n",
            "Iter 4231, loss = 1.2434658\n",
            "Iter 4232, loss = 1.0277677\n",
            "Iter 4233, loss = 1.0456314\n",
            "Iter 4234, loss = 0.97452545\n",
            "Iter 4235, loss = 1.0363353\n",
            "Iter 4236, loss = 1.0478821\n",
            "Iter 4237, loss = 1.005347\n",
            "Iter 4238, loss = 0.9656621\n",
            "Iter 4239, loss = 1.0683613\n",
            "Iter 4240, loss = 0.92309844\n",
            "Iter 4241, loss = 1.191667\n",
            "Iter 4242, loss = 1.322351\n",
            "Iter 4243, loss = 0.94445693\n",
            "Iter 4244, loss = 1.1228933\n",
            "Iter 4245, loss = 0.9352344\n",
            "Iter 4246, loss = 0.87731326\n",
            "Iter 4247, loss = 0.9762825\n",
            "Iter 4248, loss = 0.976511\n",
            "Iter 4249, loss = 1.0932168\n",
            "Iter 4250, loss = 1.2070196\n",
            "Iter 4251, loss = 1.1808581\n",
            "Iter 4252, loss = 0.9011207\n",
            "Iter 4253, loss = 1.1965983\n",
            "Iter 4254, loss = 1.2954869\n",
            "Iter 4255, loss = 0.8663432\n",
            "Iter 4256, loss = 1.022619\n",
            "Iter 4257, loss = 0.9092297\n",
            "Iter 4258, loss = 1.3272713\n",
            "Iter 4259, loss = 1.0082786\n",
            "Iter 4260, loss = 1.1250108\n",
            "Iter 4261, loss = 0.844519\n",
            "Iter 4262, loss = 1.1070728\n",
            "Iter 4263, loss = 0.99979115\n",
            "Iter 4264, loss = 0.97840524\n",
            "Iter 4265, loss = 1.406122\n",
            "Iter 4266, loss = 0.89508474\n",
            "Iter 4267, loss = 1.0260155\n",
            "Iter 4268, loss = 0.8039651\n",
            "Iter 4269, loss = 1.3711066\n",
            "Iter 4270, loss = 1.1327161\n",
            "Iter 4271, loss = 0.82105076\n",
            "Iter 4272, loss = 0.76264584\n",
            "Iter 4273, loss = 1.0070446\n",
            "Iter 4274, loss = 1.2205594\n",
            "Iter 4275, loss = 1.1667278\n",
            "Iter 4276, loss = 0.9994354\n",
            "Iter 4277, loss = 1.0725341\n",
            "Iter 4278, loss = 1.7160816\n",
            "Iter 4279, loss = 1.5017942\n",
            "Iter 4280, loss = 1.0191705\n",
            "Iter 4281, loss = 1.0530456\n",
            "Iter 4282, loss = 1.4947653\n",
            "Iter 4283, loss = 1.5614665\n",
            "Iter 4284, loss = 1.0407513\n",
            "Iter 4285, loss = 0.8228229\n",
            "Iter 4286, loss = 1.1462185\n",
            "Iter 4287, loss = 1.04699\n",
            "Iter 4288, loss = 1.071048\n",
            "Iter 4289, loss = 0.8624733\n",
            "Iter 4290, loss = 1.1291294\n",
            "Iter 4291, loss = 1.0032653\n",
            "Iter 4292, loss = 0.81155306\n",
            "Iter 4293, loss = 1.0568254\n",
            "Iter 4294, loss = 0.87559474\n",
            "Iter 4295, loss = 1.1280768\n",
            "Iter 4296, loss = 0.8538816\n",
            "Iter 4297, loss = 0.8385914\n",
            "Iter 4298, loss = 1.1049187\n",
            "Iter 4299, loss = 1.024677\n",
            "Iter 4300, loss = 1.0644927\n",
            "Iter 4301, loss = 1.0819778\n",
            "Iter 4302, loss = 1.1553547\n",
            "Iter 4303, loss = 0.80423707\n",
            "Iter 4304, loss = 1.1274211\n",
            "Iter 4305, loss = 0.91157085\n",
            "Iter 4306, loss = 0.82486975\n",
            "Iter 4307, loss = 1.1648937\n",
            "Iter 4308, loss = 1.4073961\n",
            "Iter 4309, loss = 1.2802243\n",
            "Iter 4310, loss = 0.8902785\n",
            "Iter 4311, loss = 1.3114944\n",
            "Iter 4312, loss = 0.99968016\n",
            "Iter 4313, loss = 0.84934294\n",
            "Iter 4314, loss = 0.7843791\n",
            "Iter 4315, loss = 1.1485415\n",
            "Iter 4316, loss = 1.0415986\n",
            "Iter 4317, loss = 0.9303644\n",
            "Iter 4318, loss = 1.0650687\n",
            "Iter 4319, loss = 1.4112432\n",
            "Iter 4320, loss = 1.2967119\n",
            "Iter 4321, loss = 1.2724165\n",
            "Iter 4322, loss = 1.617213\n",
            "Iter 4323, loss = 1.2430987\n",
            "Iter 4324, loss = 1.1558814\n",
            "Iter 4325, loss = 0.92896736\n",
            "Iter 4326, loss = 1.115534\n",
            "Iter 4327, loss = 0.79934096\n",
            "Iter 4328, loss = 0.9113499\n",
            "Iter 4329, loss = 1.3617082\n",
            "Iter 4330, loss = 1.0864317\n",
            "Iter 4331, loss = 1.13124\n",
            "Iter 4332, loss = 1.3540599\n",
            "Iter 4333, loss = 0.9422421\n",
            "Iter 4334, loss = 0.93208337\n",
            "Iter 4335, loss = 0.87928605\n",
            "Iter 4336, loss = 1.1182953\n",
            "Iter 4337, loss = 1.3639191\n",
            "Iter 4338, loss = 1.9871517\n",
            "Iter 4339, loss = 0.9453296\n",
            "Iter 4340, loss = 1.0650901\n",
            "Iter 4341, loss = 0.96522\n",
            "Iter 4342, loss = 1.0455071\n",
            "Iter 4343, loss = 1.2552947\n",
            "Iter 4344, loss = 1.2459927\n",
            "Iter 4345, loss = 1.0018274\n",
            "Iter 4346, loss = 0.73920274\n",
            "Iter 4347, loss = 1.0839441\n",
            "Iter 4348, loss = 0.91586316\n",
            "Iter 4349, loss = 1.3585382\n",
            "Iter 4350, loss = 1.1303462\n",
            "Iter 4351, loss = 0.98764735\n",
            "Iter 4352, loss = 0.8958387\n",
            "Iter 4353, loss = 1.1116581\n",
            "Iter 4354, loss = 1.1722361\n",
            "Iter 4355, loss = 1.1438446\n",
            "Iter 4356, loss = 1.424921\n",
            "Iter 4357, loss = 1.0334455\n",
            "Iter 4358, loss = 1.1081328\n",
            "Iter 4359, loss = 1.1800079\n",
            "Iter 4360, loss = 0.96359044\n",
            "Iter 4361, loss = 1.4073693\n",
            "Iter 4362, loss = 1.1625905\n",
            "Iter 4363, loss = 1.8620704\n",
            "Iter 4364, loss = 0.9019067\n",
            "Iter 4365, loss = 1.1395543\n",
            "Iter 4366, loss = 0.9306866\n",
            "Iter 4367, loss = 1.3519915\n",
            "Iter 4368, loss = 1.1183529\n",
            "Iter 4369, loss = 1.4585104\n",
            "Iter 4370, loss = 1.0669436\n",
            "Iter 4371, loss = 0.9340171\n",
            "Iter 4372, loss = 0.91658056\n",
            "Iter 4373, loss = 1.3623152\n",
            "Iter 4374, loss = 0.90771025\n",
            "Iter 4375, loss = 1.1362166\n",
            "Iter 4376, loss = 1.1339126\n",
            "Iter 4377, loss = 0.88211656\n",
            "Iter 4378, loss = 1.5267822\n",
            "Iter 4379, loss = 1.1164161\n",
            "Iter 4380, loss = 0.9747335\n",
            "Iter 4381, loss = 0.9437696\n",
            "Iter 4382, loss = 1.5236163\n",
            "Iter 4383, loss = 1.0198361\n",
            "Iter 4384, loss = 0.93370163\n",
            "Iter 4385, loss = 0.954136\n",
            "Iter 4386, loss = 0.9682717\n",
            "Iter 4387, loss = 1.5178591\n",
            "Iter 4388, loss = 1.1053314\n",
            "Iter 4389, loss = 0.9566163\n",
            "Iter 4390, loss = 1.210562\n",
            "Iter 4391, loss = 0.87305397\n",
            "Iter 4392, loss = 0.82109606\n",
            "Iter 4393, loss = 1.0339713\n",
            "Iter 4394, loss = 1.6462839\n",
            "Iter 4395, loss = 1.4429505\n",
            "Iter 4396, loss = 1.0014665\n",
            "Iter 4397, loss = 0.9237583\n",
            "Iter 4398, loss = 1.1574514\n",
            "Iter 4399, loss = 1.2202618\n",
            "Iter 4400, loss = 0.9465367\n",
            "Iter 4401, loss = 0.84316164\n",
            "Iter 4402, loss = 1.6092918\n",
            "Iter 4403, loss = 1.0309281\n",
            "Iter 4404, loss = 0.8822654\n",
            "Iter 4405, loss = 0.9891318\n",
            "Iter 4406, loss = 1.2861195\n",
            "Iter 4407, loss = 1.068855\n",
            "Iter 4408, loss = 1.0415587\n",
            "Iter 4409, loss = 0.9533732\n",
            "Iter 4410, loss = 0.97313344\n",
            "Iter 4411, loss = 0.9693497\n",
            "Iter 4412, loss = 1.1710631\n",
            "Iter 4413, loss = 1.0339327\n",
            "Iter 4414, loss = 1.4029437\n",
            "Iter 4415, loss = 1.0648282\n",
            "Iter 4416, loss = 1.4013706\n",
            "Iter 4417, loss = 0.99516237\n",
            "Iter 4418, loss = 0.8921359\n",
            "Iter 4419, loss = 1.1101177\n",
            "Iter 4420, loss = 0.9653237\n",
            "Iter 4421, loss = 0.9953491\n",
            "Iter 4422, loss = 1.4465413\n",
            "Iter 4423, loss = 0.71963346\n",
            "Iter 4424, loss = 1.009253\n",
            "Iter 4425, loss = 1.0907066\n",
            "Iter 4426, loss = 0.89076555\n",
            "Iter 4427, loss = 0.80146813\n",
            "Iter 4428, loss = 1.3348278\n",
            "Iter 4429, loss = 0.86587286\n",
            "Iter 4430, loss = 1.2866194\n",
            "Iter 4431, loss = 1.5871797\n",
            "Iter 4432, loss = 1.3337607\n",
            "Iter 4433, loss = 1.1098602\n",
            "Iter 4434, loss = 1.0609145\n",
            "Iter 4435, loss = 1.0236007\n",
            "Iter 4436, loss = 1.3149421\n",
            "Iter 4437, loss = 1.0773467\n",
            "Iter 4438, loss = 0.90726244\n",
            "Iter 4439, loss = 0.9525257\n",
            "Iter 4440, loss = 0.9971293\n",
            "Iter 4441, loss = 0.8679433\n",
            "Iter 4442, loss = 1.325864\n",
            "Iter 4443, loss = 1.4220208\n",
            "Iter 4444, loss = 1.4469538\n",
            "Iter 4445, loss = 0.9708552\n",
            "Iter 4446, loss = 0.8204731\n",
            "Iter 4447, loss = 0.92732537\n",
            "Iter 4448, loss = 1.216752\n",
            "Iter 4449, loss = 1.083871\n",
            "Iter 4450, loss = 0.9983195\n",
            "Iter 4451, loss = 1.099365\n",
            "Iter 4452, loss = 0.9228731\n",
            "Iter 4453, loss = 1.3507335\n",
            "Iter 4454, loss = 0.88134813\n",
            "Iter 4455, loss = 1.1022117\n",
            "Iter 4456, loss = 0.8848189\n",
            "Iter 4457, loss = 1.2127161\n",
            "Iter 4458, loss = 1.2481135\n",
            "Iter 4459, loss = 1.0907636\n",
            "Iter 4460, loss = 1.0886425\n",
            "Iter 4461, loss = 0.89965427\n",
            "Iter 4462, loss = 1.2735229\n",
            "Iter 4463, loss = 1.006774\n",
            "Iter 4464, loss = 1.6645288\n",
            "Iter 4465, loss = 1.2010882\n",
            "Iter 4466, loss = 0.83123827\n",
            "Iter 4467, loss = 1.1416945\n",
            "Iter 4468, loss = 1.2214102\n",
            "Iter 4469, loss = 1.232316\n",
            "Iter 4470, loss = 0.8967611\n",
            "Iter 4471, loss = 1.1944766\n",
            "Iter 4472, loss = 1.1343986\n",
            "Iter 4473, loss = 0.9315559\n",
            "Iter 4474, loss = 1.0129027\n",
            "Iter 4475, loss = 1.4570117\n",
            "Iter 4476, loss = 1.1510609\n",
            "Iter 4477, loss = 0.91039705\n",
            "Iter 4478, loss = 1.2405767\n",
            "Iter 4479, loss = 1.1458538\n",
            "Iter 4480, loss = 1.039273\n",
            "Iter 4481, loss = 1.2953633\n",
            "Iter 4482, loss = 1.5263519\n",
            "Iter 4483, loss = 1.0699484\n",
            "Iter 4484, loss = 1.3124058\n",
            "Iter 4485, loss = 1.0713692\n",
            "Iter 4486, loss = 0.862314\n",
            "Iter 4487, loss = 1.2756473\n",
            "Iter 4488, loss = 1.1551182\n",
            "Iter 4489, loss = 0.9631108\n",
            "Iter 4490, loss = 1.2494881\n",
            "Iter 4491, loss = 0.9735121\n",
            "Iter 4492, loss = 1.2251097\n",
            "Iter 4493, loss = 0.970444\n",
            "Iter 4494, loss = 0.9450766\n",
            "Iter 4495, loss = 1.2862511\n",
            "Iter 4496, loss = 1.1916738\n",
            "Iter 4497, loss = 0.9961324\n",
            "Iter 4498, loss = 1.1441059\n",
            "Iter 4499, loss = 1.0683919\n",
            "Iter 4500, loss = 1.1909399\n",
            "Iter 4501, loss = 1.1532404\n",
            "Iter 4502, loss = 1.0128548\n",
            "Iter 4503, loss = 1.4669585\n",
            "Iter 4504, loss = 1.049515\n",
            "Iter 4505, loss = 0.8209231\n",
            "Iter 4506, loss = 0.9764869\n",
            "Iter 4507, loss = 1.1390107\n",
            "Iter 4508, loss = 1.0689342\n",
            "Iter 4509, loss = 1.2864922\n",
            "Iter 4510, loss = 1.146564\n",
            "Iter 4511, loss = 0.8166344\n",
            "Iter 4512, loss = 1.2012224\n",
            "Iter 4513, loss = 0.92688143\n",
            "Iter 4514, loss = 0.93268013\n",
            "Iter 4515, loss = 1.2282368\n",
            "Iter 4516, loss = 1.4091991\n",
            "Iter 4517, loss = 1.0865827\n",
            "Iter 4518, loss = 0.8018415\n",
            "Iter 4519, loss = 0.8808803\n",
            "Iter 4520, loss = 0.76112795\n",
            "Iter 4521, loss = 1.2220006\n",
            "Iter 4522, loss = 1.1538825\n",
            "Iter 4523, loss = 1.1991014\n",
            "Iter 4524, loss = 1.2864497\n",
            "Iter 4525, loss = 0.9419751\n",
            "Iter 4526, loss = 0.9095603\n",
            "Iter 4527, loss = 0.72306395\n",
            "Iter 4528, loss = 1.0591657\n",
            "Iter 4529, loss = 1.5106578\n",
            "Iter 4530, loss = 1.125056\n",
            "Iter 4531, loss = 0.99520224\n",
            "Iter 4532, loss = 1.4297262\n",
            "Iter 4533, loss = 1.1061726\n",
            "Iter 4534, loss = 1.0196872\n",
            "Iter 4535, loss = 1.241715\n",
            "Iter 4536, loss = 0.7966109\n",
            "Iter 4537, loss = 1.0906222\n",
            "Iter 4538, loss = 0.7745019\n",
            "Iter 4539, loss = 0.96950585\n",
            "Iter 4540, loss = 1.0194446\n",
            "Iter 4541, loss = 1.2465928\n",
            "Iter 4542, loss = 1.0013857\n",
            "Iter 4543, loss = 1.2085178\n",
            "Iter 4544, loss = 1.2251344\n",
            "Iter 4545, loss = 0.8202824\n",
            "Iter 4546, loss = 1.2272002\n",
            "Iter 4547, loss = 0.9619378\n",
            "Iter 4548, loss = 0.861573\n",
            "Iter 4549, loss = 0.97773343\n",
            "Iter 4550, loss = 0.71635413\n",
            "Iter 4551, loss = 0.9546901\n",
            "Iter 4552, loss = 1.0051427\n",
            "Iter 4553, loss = 0.8632298\n",
            "Iter 4554, loss = 1.0764787\n",
            "Iter 4555, loss = 1.1940229\n",
            "Iter 4556, loss = 1.3223021\n",
            "Iter 4557, loss = 1.0669842\n",
            "Iter 4558, loss = 1.2043381\n",
            "Iter 4559, loss = 0.9953119\n",
            "Iter 4560, loss = 0.91371584\n",
            "Iter 4561, loss = 0.93618995\n",
            "Iter 4562, loss = 1.0055041\n",
            "Iter 4563, loss = 1.4935696\n",
            "Iter 4564, loss = 1.0079015\n",
            "Iter 4565, loss = 0.66829836\n",
            "Iter 4566, loss = 0.97200835\n",
            "Iter 4567, loss = 0.88795173\n",
            "Iter 4568, loss = 0.7789645\n",
            "Iter 4569, loss = 0.87036765\n",
            "Iter 4570, loss = 1.3659825\n",
            "Iter 4571, loss = 1.0900854\n",
            "Iter 4572, loss = 1.2302558\n",
            "Iter 4573, loss = 0.8620784\n",
            "Iter 4574, loss = 0.8747324\n",
            "Iter 4575, loss = 1.1515026\n",
            "Iter 4576, loss = 1.0439085\n",
            "Iter 4577, loss = 0.784794\n",
            "Iter 4578, loss = 1.0488377\n",
            "Iter 4579, loss = 1.174551\n",
            "Iter 4580, loss = 1.0080718\n",
            "Iter 4581, loss = 0.88831824\n",
            "Iter 4582, loss = 0.95267856\n",
            "Iter 4583, loss = 1.0752741\n",
            "Iter 4584, loss = 1.4806647\n",
            "Iter 4585, loss = 0.8325074\n",
            "Iter 4586, loss = 1.0074283\n",
            "Iter 4587, loss = 1.2496948\n",
            "Iter 4588, loss = 1.1349055\n",
            "Iter 4589, loss = 1.0937998\n",
            "Iter 4590, loss = 1.4181871\n",
            "Iter 4591, loss = 0.94757897\n",
            "Iter 4592, loss = 1.1390133\n",
            "Iter 4593, loss = 1.2372398\n",
            "Iter 4594, loss = 0.9076059\n",
            "Iter 4595, loss = 0.96466005\n",
            "Iter 4596, loss = 0.9099443\n",
            "Iter 4597, loss = 1.5154082\n",
            "Iter 4598, loss = 1.6522794\n",
            "Iter 4599, loss = 0.99135983\n",
            "Iter 4600, loss = 0.8197452\n",
            "Iter 4601, loss = 1.4508183\n",
            "Iter 4602, loss = 1.1029936\n",
            "Iter 4603, loss = 0.7889508\n",
            "Iter 4604, loss = 0.67521274\n",
            "Iter 4605, loss = 0.9854121\n",
            "Iter 4606, loss = 0.9658572\n",
            "Iter 4607, loss = 0.92931694\n",
            "Iter 4608, loss = 1.0299189\n",
            "Iter 4609, loss = 0.91200995\n",
            "Iter 4610, loss = 1.2736367\n",
            "Iter 4611, loss = 1.0830785\n",
            "Iter 4612, loss = 1.0429515\n",
            "Iter 4613, loss = 1.4501709\n",
            "Iter 4614, loss = 0.8911084\n",
            "Iter 4615, loss = 1.2380874\n",
            "Iter 4616, loss = 0.93950105\n",
            "Iter 4617, loss = 1.4170489\n",
            "Iter 4618, loss = 0.9642403\n",
            "Iter 4619, loss = 1.1854718\n",
            "Iter 4620, loss = 1.3341329\n",
            "Iter 4621, loss = 1.4302404\n",
            "Iter 4622, loss = 0.92511606\n",
            "Iter 4623, loss = 1.3197439\n",
            "Iter 4624, loss = 0.89735126\n",
            "Iter 4625, loss = 1.1359153\n",
            "Iter 4626, loss = 1.3749636\n",
            "Iter 4627, loss = 1.2630048\n",
            "Iter 4628, loss = 1.1856148\n",
            "Iter 4629, loss = 1.2923154\n",
            "Iter 4630, loss = 0.8148942\n",
            "Iter 4631, loss = 1.1074572\n",
            "Iter 4632, loss = 1.5384122\n",
            "Iter 4633, loss = 0.9211546\n",
            "Iter 4634, loss = 1.0268204\n",
            "Iter 4635, loss = 1.0169417\n",
            "Iter 4636, loss = 0.9456454\n",
            "Iter 4637, loss = 0.96994007\n",
            "Iter 4638, loss = 1.2170378\n",
            "Iter 4639, loss = 0.8871314\n",
            "Iter 4640, loss = 0.6620841\n",
            "Iter 4641, loss = 1.0480819\n",
            "Iter 4642, loss = 0.7726445\n",
            "Iter 4643, loss = 1.9638546\n",
            "Iter 4644, loss = 0.95459867\n",
            "Iter 4645, loss = 0.80699956\n",
            "Iter 4646, loss = 1.1446042\n",
            "Iter 4647, loss = 1.114073\n",
            "Iter 4648, loss = 1.0279988\n",
            "Iter 4649, loss = 1.1935027\n",
            "Iter 4650, loss = 1.1260154\n",
            "Iter 4651, loss = 0.8237716\n",
            "Iter 4652, loss = 1.100589\n",
            "Iter 4653, loss = 1.2479568\n",
            "Iter 4654, loss = 1.0401539\n",
            "Iter 4655, loss = 1.1392101\n",
            "Iter 4656, loss = 1.14194\n",
            "Iter 4657, loss = 1.2908442\n",
            "Iter 4658, loss = 0.8162868\n",
            "Iter 4659, loss = 0.9393404\n",
            "Iter 4660, loss = 0.99811935\n",
            "Iter 4661, loss = 1.1624248\n",
            "Iter 4662, loss = 1.0559372\n",
            "Iter 4663, loss = 0.9792862\n",
            "Iter 4664, loss = 1.36556\n",
            "Iter 4665, loss = 1.5043237\n",
            "Iter 4666, loss = 1.0935999\n",
            "Iter 4667, loss = 0.850329\n",
            "Iter 4668, loss = 1.1588221\n",
            "Iter 4669, loss = 1.6001043\n",
            "Iter 4670, loss = 1.3335478\n",
            "Iter 4671, loss = 1.0326874\n",
            "Iter 4672, loss = 1.0374787\n",
            "Iter 4673, loss = 0.986164\n",
            "Iter 4674, loss = 0.86186475\n",
            "Iter 4675, loss = 1.2046112\n",
            "Iter 4676, loss = 1.2604017\n",
            "Iter 4677, loss = 1.2610451\n",
            "Iter 4678, loss = 1.1028829\n",
            "Iter 4679, loss = 1.0333169\n",
            "Iter 4680, loss = 0.9842369\n",
            "Iter 4681, loss = 0.9956064\n",
            "Iter 4682, loss = 0.9269345\n",
            "Iter 4683, loss = 0.9868067\n",
            "Iter 4684, loss = 0.9973879\n",
            "Iter 4685, loss = 1.3863311\n",
            "Iter 4686, loss = 1.775524\n",
            "Iter 4687, loss = 0.9015706\n",
            "Iter 4688, loss = 0.9280269\n",
            "Iter 4689, loss = 1.3112519\n",
            "Iter 4690, loss = 1.157206\n",
            "Iter 4691, loss = 0.8623446\n",
            "Iter 4692, loss = 1.1188803\n",
            "Iter 4693, loss = 0.92810297\n",
            "Iter 4694, loss = 1.1157519\n",
            "Iter 4695, loss = 1.0731342\n",
            "Iter 4696, loss = 0.9123627\n",
            "Iter 4697, loss = 1.5641322\n",
            "Iter 4698, loss = 0.96614146\n",
            "Iter 4699, loss = 1.1128387\n",
            "Iter 4700, loss = 0.8050724\n",
            "Iter 4701, loss = 1.4657271\n",
            "Iter 4702, loss = 1.1296084\n",
            "Iter 4703, loss = 0.7835507\n",
            "Iter 4704, loss = 1.1681657\n",
            "Iter 4705, loss = 1.2730572\n",
            "Iter 4706, loss = 1.1658504\n",
            "Iter 4707, loss = 1.1002684\n",
            "Iter 4708, loss = 1.0558231\n",
            "Iter 4709, loss = 1.3937924\n",
            "Iter 4710, loss = 1.3982222\n",
            "Iter 4711, loss = 1.3287649\n",
            "Iter 4712, loss = 1.2041279\n",
            "Iter 4713, loss = 1.0459979\n",
            "Iter 4714, loss = 1.404999\n",
            "Iter 4715, loss = 0.88813853\n",
            "Iter 4716, loss = 0.8525996\n",
            "Iter 4717, loss = 0.913628\n",
            "Iter 4718, loss = 1.0769439\n",
            "Iter 4719, loss = 0.97492754\n",
            "Iter 4720, loss = 1.104897\n",
            "Iter 4721, loss = 0.8179216\n",
            "Iter 4722, loss = 1.0394632\n",
            "Iter 4723, loss = 1.1183038\n",
            "Iter 4724, loss = 1.2458006\n",
            "Iter 4725, loss = 0.9495064\n",
            "Iter 4726, loss = 1.338578\n",
            "Iter 4727, loss = 1.1340985\n",
            "Iter 4728, loss = 0.95075566\n",
            "Iter 4729, loss = 0.9729066\n",
            "Iter 4730, loss = 1.1946263\n",
            "Iter 4731, loss = 1.7714822\n",
            "Iter 4732, loss = 1.0141425\n",
            "Iter 4733, loss = 0.93600154\n",
            "Iter 4734, loss = 1.1520534\n",
            "Iter 4735, loss = 0.8325805\n",
            "Iter 4736, loss = 1.169188\n",
            "Iter 4737, loss = 1.4616337\n",
            "Iter 4738, loss = 0.97483397\n",
            "Iter 4739, loss = 1.3116231\n",
            "Iter 4740, loss = 0.86882156\n",
            "Iter 4741, loss = 1.3977337\n",
            "Iter 4742, loss = 1.1900872\n",
            "Iter 4743, loss = 1.0102992\n",
            "Iter 4744, loss = 0.8913424\n",
            "Iter 4745, loss = 0.97457767\n",
            "Iter 4746, loss = 0.8176891\n",
            "Iter 4747, loss = 0.84340024\n",
            "Iter 4748, loss = 1.2113112\n",
            "Iter 4749, loss = 0.91635585\n",
            "Iter 4750, loss = 1.0809054\n",
            "Iter 4751, loss = 0.9128528\n",
            "Iter 4752, loss = 1.1859999\n",
            "Iter 4753, loss = 1.2101988\n",
            "Iter 4754, loss = 1.3280789\n",
            "Iter 4755, loss = 1.1527188\n",
            "Iter 4756, loss = 0.8522309\n",
            "Iter 4757, loss = 0.9359758\n",
            "Iter 4758, loss = 0.9922075\n",
            "Iter 4759, loss = 0.96077526\n",
            "Iter 4760, loss = 1.1504344\n",
            "Iter 4761, loss = 0.88025737\n",
            "Iter 4762, loss = 1.1210412\n",
            "Iter 4763, loss = 0.8453641\n",
            "Iter 4764, loss = 1.1610229\n",
            "Iter 4765, loss = 1.4101639\n",
            "Iter 4766, loss = 0.8489202\n",
            "Iter 4767, loss = 0.9639746\n",
            "Iter 4768, loss = 0.83363575\n",
            "Iter 4769, loss = 0.81453407\n",
            "Iter 4770, loss = 1.1169951\n",
            "Iter 4771, loss = 0.82337195\n",
            "Iter 4772, loss = 1.0556657\n",
            "Iter 4773, loss = 0.8501093\n",
            "Iter 4774, loss = 1.145504\n",
            "Iter 4775, loss = 1.1337094\n",
            "Iter 4776, loss = 0.9720825\n",
            "Iter 4777, loss = 0.6533981\n",
            "Iter 4778, loss = 1.0772266\n",
            "Iter 4779, loss = 0.8289804\n",
            "Iter 4780, loss = 1.2987609\n",
            "Iter 4781, loss = 1.0335609\n",
            "Iter 4782, loss = 1.671654\n",
            "Iter 4783, loss = 1.6794217\n",
            "Iter 4784, loss = 0.9190229\n",
            "Iter 4785, loss = 1.184402\n",
            "Iter 4786, loss = 0.88059926\n",
            "Iter 4787, loss = 1.1823133\n",
            "Iter 4788, loss = 0.9599311\n",
            "Iter 4789, loss = 1.0252285\n",
            "Iter 4790, loss = 0.90051425\n",
            "Iter 4791, loss = 0.8564793\n",
            "Iter 4792, loss = 0.88563305\n",
            "Iter 4793, loss = 1.2178566\n",
            "Iter 4794, loss = 1.5501779\n",
            "Iter 4795, loss = 1.3012309\n",
            "Iter 4796, loss = 0.9862533\n",
            "Iter 4797, loss = 0.97172964\n",
            "Iter 4798, loss = 1.5069187\n",
            "Iter 4799, loss = 1.1747873\n",
            "Iter 4800, loss = 0.9565271\n",
            "Iter 4801, loss = 0.85281307\n",
            "Iter 4802, loss = 1.0300014\n",
            "Iter 4803, loss = 0.89842504\n",
            "Iter 4804, loss = 0.9468694\n",
            "Iter 4805, loss = 1.0055037\n",
            "Iter 4806, loss = 0.9656539\n",
            "Iter 4807, loss = 1.6362346\n",
            "Iter 4808, loss = 1.1043878\n",
            "Iter 4809, loss = 0.94704974\n",
            "Iter 4810, loss = 0.9413129\n",
            "Iter 4811, loss = 1.2499102\n",
            "Iter 4812, loss = 1.4023306\n",
            "Iter 4813, loss = 1.3095713\n",
            "Iter 4814, loss = 1.1425439\n",
            "Iter 4815, loss = 1.1345822\n",
            "Iter 4816, loss = 1.094003\n",
            "Iter 4817, loss = 1.3264832\n",
            "Iter 4818, loss = 1.0327462\n",
            "Iter 4819, loss = 1.2660365\n",
            "Iter 4820, loss = 1.2470101\n",
            "Iter 4821, loss = 1.0562869\n",
            "Iter 4822, loss = 0.86624634\n",
            "Iter 4823, loss = 0.8719718\n",
            "Iter 4824, loss = 0.8556577\n",
            "Iter 4825, loss = 1.0380161\n",
            "Iter 4826, loss = 1.4527805\n",
            "Iter 4827, loss = 1.1087391\n",
            "Iter 4828, loss = 1.1559972\n",
            "Iter 4829, loss = 0.9339273\n",
            "Iter 4830, loss = 1.2316251\n",
            "Iter 4831, loss = 1.0746787\n",
            "Iter 4832, loss = 1.0212003\n",
            "Iter 4833, loss = 0.86081743\n",
            "Iter 4834, loss = 1.0310646\n",
            "Iter 4835, loss = 1.5442846\n",
            "Iter 4836, loss = 0.9583219\n",
            "Iter 4837, loss = 1.1343061\n",
            "Iter 4838, loss = 1.2936033\n",
            "Iter 4839, loss = 1.1442426\n",
            "Iter 4840, loss = 0.7978852\n",
            "Iter 4841, loss = 0.93072337\n",
            "Iter 4842, loss = 1.1242316\n",
            "Iter 4843, loss = 1.0302835\n",
            "Iter 4844, loss = 0.9819209\n",
            "Iter 4845, loss = 1.183686\n",
            "Iter 4846, loss = 1.4340842\n",
            "Iter 4847, loss = 0.9583671\n",
            "Iter 4848, loss = 1.2531266\n",
            "Iter 4849, loss = 1.1882714\n",
            "Iter 4850, loss = 0.9226824\n",
            "Iter 4851, loss = 0.8082645\n",
            "Iter 4852, loss = 0.9733729\n",
            "Iter 4853, loss = 0.90343237\n",
            "Iter 4854, loss = 1.1164925\n",
            "Iter 4855, loss = 1.0909988\n",
            "Iter 4856, loss = 1.0694321\n",
            "Iter 4857, loss = 1.1134344\n",
            "Iter 4858, loss = 0.72551346\n",
            "Iter 4859, loss = 1.0251715\n",
            "Iter 4860, loss = 1.3161242\n",
            "Iter 4861, loss = 1.5773721\n",
            "Iter 4862, loss = 1.5342731\n",
            "Iter 4863, loss = 0.90295494\n",
            "Iter 4864, loss = 1.4486179\n",
            "Iter 4865, loss = 0.8686167\n",
            "Iter 4866, loss = 0.9388367\n",
            "Iter 4867, loss = 1.7384715\n",
            "Iter 4868, loss = 1.3996322\n",
            "Iter 4869, loss = 1.0489125\n",
            "Iter 4870, loss = 0.9995173\n",
            "Iter 4871, loss = 0.99312764\n",
            "Iter 4872, loss = 1.0024682\n",
            "Iter 4873, loss = 1.1211863\n",
            "Iter 4874, loss = 1.0066614\n",
            "Iter 4875, loss = 0.9778767\n",
            "Iter 4876, loss = 1.2402399\n",
            "Iter 4877, loss = 0.8227798\n",
            "Iter 4878, loss = 0.9049708\n",
            "Iter 4879, loss = 1.2571093\n",
            "Iter 4880, loss = 1.0460098\n",
            "Iter 4881, loss = 0.9890771\n",
            "Iter 4882, loss = 1.1623366\n",
            "Iter 4883, loss = 0.7044173\n",
            "Iter 4884, loss = 0.8218795\n",
            "Iter 4885, loss = 1.0009862\n",
            "Iter 4886, loss = 1.4236166\n",
            "Iter 4887, loss = 1.2368737\n",
            "Iter 4888, loss = 0.90131396\n",
            "Iter 4889, loss = 0.807197\n",
            "Iter 4890, loss = 1.0068507\n",
            "Iter 4891, loss = 0.7373311\n",
            "Iter 4892, loss = 1.3008063\n",
            "Iter 4893, loss = 1.0914861\n",
            "Iter 4894, loss = 0.9552709\n",
            "Iter 4895, loss = 1.0282711\n",
            "Iter 4896, loss = 1.0328186\n",
            "Iter 4897, loss = 1.3737587\n",
            "Iter 4898, loss = 1.0201926\n",
            "Iter 4899, loss = 1.2135658\n",
            "Iter 4900, loss = 1.2594876\n",
            "Iter 4901, loss = 0.99478376\n",
            "Iter 4902, loss = 1.1751317\n",
            "Iter 4903, loss = 1.1204023\n",
            "Iter 4904, loss = 1.0511111\n",
            "Iter 4905, loss = 1.3620188\n",
            "Iter 4906, loss = 1.2717752\n",
            "Iter 4907, loss = 0.9708281\n",
            "Iter 4908, loss = 1.0714312\n",
            "Iter 4909, loss = 1.1577826\n",
            "Iter 4910, loss = 1.1109717\n",
            "Iter 4911, loss = 1.2090162\n",
            "Iter 4912, loss = 1.1545483\n",
            "Iter 4913, loss = 1.0103025\n",
            "Iter 4914, loss = 1.1039401\n",
            "Iter 4915, loss = 0.9909011\n",
            "Iter 4916, loss = 1.0244036\n",
            "Iter 4917, loss = 0.9129163\n",
            "Iter 4918, loss = 1.0465983\n",
            "Iter 4919, loss = 1.1562772\n",
            "Iter 4920, loss = 0.9988769\n",
            "Iter 4921, loss = 1.1977049\n",
            "Iter 4922, loss = 1.1207898\n",
            "Iter 4923, loss = 1.3207629\n",
            "Iter 4924, loss = 1.6919898\n",
            "Iter 4925, loss = 1.3030702\n",
            "Iter 4926, loss = 1.0258477\n",
            "Iter 4927, loss = 1.0151768\n",
            "Iter 4928, loss = 1.1754827\n",
            "Iter 4929, loss = 1.1233246\n",
            "Iter 4930, loss = 1.139849\n",
            "Iter 4931, loss = 0.9253207\n",
            "Iter 4932, loss = 1.1650333\n",
            "Iter 4933, loss = 0.9211166\n",
            "Iter 4934, loss = 1.0298983\n",
            "Iter 4935, loss = 1.0786451\n",
            "Iter 4936, loss = 0.98273873\n",
            "Iter 4937, loss = 1.1131449\n",
            "Iter 4938, loss = 1.6104336\n",
            "Iter 4939, loss = 1.2689116\n",
            "Iter 4940, loss = 0.8457153\n",
            "Iter 4941, loss = 1.2523243\n",
            "Iter 4942, loss = 0.75553894\n",
            "Iter 4943, loss = 0.9346629\n",
            "Iter 4944, loss = 1.1163777\n",
            "Iter 4945, loss = 1.1143026\n",
            "Iter 4946, loss = 1.0420086\n",
            "Iter 4947, loss = 1.0879667\n",
            "Iter 4948, loss = 1.0589963\n",
            "Iter 4949, loss = 0.95909464\n",
            "Iter 4950, loss = 1.141978\n",
            "Iter 4951, loss = 0.95686746\n",
            "Iter 4952, loss = 1.1531941\n",
            "Iter 4953, loss = 1.3963152\n",
            "Iter 4954, loss = 1.2124903\n",
            "Iter 4955, loss = 1.3314941\n",
            "Iter 4956, loss = 0.89381206\n",
            "Iter 4957, loss = 1.1843922\n",
            "Iter 4958, loss = 0.93521464\n",
            "Iter 4959, loss = 1.00425\n",
            "Iter 4960, loss = 0.96949375\n",
            "Iter 4961, loss = 0.8458253\n",
            "Iter 4962, loss = 1.2343587\n",
            "Iter 4963, loss = 0.98578054\n",
            "Iter 4964, loss = 0.8877813\n",
            "Iter 4965, loss = 1.0297675\n",
            "Iter 4966, loss = 1.3384441\n",
            "Iter 4967, loss = 0.9384913\n",
            "Iter 4968, loss = 1.0465646\n",
            "Iter 4969, loss = 1.1061631\n",
            "Iter 4970, loss = 1.137454\n",
            "Iter 4971, loss = 1.014147\n",
            "Iter 4972, loss = 1.285703\n",
            "Iter 4973, loss = 1.0111145\n",
            "Iter 4974, loss = 0.98054934\n",
            "Iter 4975, loss = 0.9945992\n",
            "Iter 4976, loss = 1.5459085\n",
            "Iter 4977, loss = 1.1161889\n",
            "Iter 4978, loss = 0.9462049\n",
            "Iter 4979, loss = 1.004942\n",
            "Iter 4980, loss = 1.1077071\n",
            "Iter 4981, loss = 1.215322\n",
            "Iter 4982, loss = 0.82833195\n",
            "Iter 4983, loss = 1.0777862\n",
            "Iter 4984, loss = 1.0030026\n",
            "Iter 4985, loss = 0.80337477\n",
            "Iter 4986, loss = 0.87798357\n",
            "Iter 4987, loss = 0.8830374\n",
            "Iter 4988, loss = 1.3340919\n",
            "Iter 4989, loss = 1.0216355\n",
            "Iter 4990, loss = 0.899118\n",
            "Iter 4991, loss = 1.6903365\n",
            "Iter 4992, loss = 1.3118534\n",
            "Iter 4993, loss = 1.1813307\n",
            "Iter 4994, loss = 1.2664387\n",
            "Iter 4995, loss = 1.0287088\n",
            "Iter 4996, loss = 1.7078512\n",
            "Iter 4997, loss = 1.4805412\n",
            "Iter 4998, loss = 1.3225666\n",
            "Iter 4999, loss = 1.2555386\n",
            "Iter 5000, loss = 0.93721485\n",
            "Iter 5001, loss = 1.1731673\n",
            "Iter 5002, loss = 1.2609057\n",
            "Iter 5003, loss = 0.79414207\n",
            "Iter 5004, loss = 1.3006197\n",
            "Iter 5005, loss = 0.8361637\n",
            "Iter 5006, loss = 0.9099948\n",
            "Iter 5007, loss = 1.1794794\n",
            "Iter 5008, loss = 0.79571104\n",
            "Iter 5009, loss = 1.0614508\n",
            "Iter 5010, loss = 0.9840236\n",
            "Iter 5011, loss = 0.9349562\n",
            "Iter 5012, loss = 0.83867574\n",
            "Iter 5013, loss = 0.8863975\n",
            "Iter 5014, loss = 0.9645746\n",
            "Iter 5015, loss = 0.96889824\n",
            "Iter 5016, loss = 0.8992312\n",
            "Iter 5017, loss = 1.09102\n",
            "Iter 5018, loss = 1.0481038\n",
            "Iter 5019, loss = 1.2888846\n",
            "Iter 5020, loss = 0.9471526\n",
            "Iter 5021, loss = 0.9295673\n",
            "Iter 5022, loss = 1.2627242\n",
            "Iter 5023, loss = 0.9429282\n",
            "Iter 5024, loss = 1.0542667\n",
            "Iter 5025, loss = 1.4823341\n",
            "Iter 5026, loss = 1.415533\n",
            "Iter 5027, loss = 1.3233744\n",
            "Iter 5028, loss = 1.0250739\n",
            "Iter 5029, loss = 0.9925488\n",
            "Iter 5030, loss = 1.290926\n",
            "Iter 5031, loss = 1.2925017\n",
            "Iter 5032, loss = 1.0494912\n",
            "Iter 5033, loss = 1.033998\n",
            "Iter 5034, loss = 1.290149\n",
            "Iter 5035, loss = 1.4475769\n",
            "Iter 5036, loss = 1.2988912\n",
            "Iter 5037, loss = 1.4106101\n",
            "Iter 5038, loss = 1.1254247\n",
            "Iter 5039, loss = 1.0059576\n",
            "Iter 5040, loss = 1.0542252\n",
            "Iter 5041, loss = 1.4786229\n",
            "Iter 5042, loss = 1.0337903\n",
            "Iter 5043, loss = 1.0710685\n",
            "Iter 5044, loss = 1.4048383\n",
            "Iter 5045, loss = 1.0769535\n",
            "Iter 5046, loss = 1.0435537\n",
            "Iter 5047, loss = 1.2704976\n",
            "Iter 5048, loss = 1.27078\n",
            "Iter 5049, loss = 0.7804385\n",
            "Iter 5050, loss = 1.0170577\n",
            "Iter 5051, loss = 1.645689\n",
            "Iter 5052, loss = 1.174257\n",
            "Iter 5053, loss = 1.045618\n",
            "Iter 5054, loss = 1.1119769\n",
            "Iter 5055, loss = 1.0943921\n",
            "Iter 5056, loss = 1.0249116\n",
            "Iter 5057, loss = 1.2254405\n",
            "Iter 5058, loss = 1.2054837\n",
            "Iter 5059, loss = 1.118553\n",
            "Iter 5060, loss = 1.258043\n",
            "Iter 5061, loss = 1.0773377\n",
            "Iter 5062, loss = 1.2909163\n",
            "Iter 5063, loss = 1.0638704\n",
            "Iter 5064, loss = 1.0650988\n",
            "Iter 5065, loss = 1.4633199\n",
            "Iter 5066, loss = 1.0562\n",
            "Iter 5067, loss = 0.8585009\n",
            "Iter 5068, loss = 1.1720984\n",
            "Iter 5069, loss = 1.0243902\n",
            "Iter 5070, loss = 0.90496117\n",
            "Iter 5071, loss = 1.0488082\n",
            "Iter 5072, loss = 0.93684053\n",
            "Iter 5073, loss = 1.2135689\n",
            "Iter 5074, loss = 0.9111895\n",
            "Iter 5075, loss = 1.1007878\n",
            "Iter 5076, loss = 0.9494257\n",
            "Iter 5077, loss = 1.1285703\n",
            "Iter 5078, loss = 1.0812901\n",
            "Iter 5079, loss = 1.3205438\n",
            "Iter 5080, loss = 1.0305712\n",
            "Iter 5081, loss = 1.062599\n",
            "Iter 5082, loss = 0.80041695\n",
            "Iter 5083, loss = 0.77304345\n",
            "Iter 5084, loss = 1.1847464\n",
            "Iter 5085, loss = 0.9829241\n",
            "Iter 5086, loss = 0.9178934\n",
            "Iter 5087, loss = 1.3933611\n",
            "Iter 5088, loss = 1.1713803\n",
            "Iter 5089, loss = 0.92691666\n",
            "Iter 5090, loss = 1.3953109\n",
            "Iter 5091, loss = 0.8609933\n",
            "Iter 5092, loss = 1.7493808\n",
            "Iter 5093, loss = 0.9047668\n",
            "Iter 5094, loss = 0.87193966\n",
            "Iter 5095, loss = 1.47227\n",
            "Iter 5096, loss = 1.0997196\n",
            "Iter 5097, loss = 1.1607609\n",
            "Iter 5098, loss = 1.1590266\n",
            "Iter 5099, loss = 1.0880809\n",
            "Iter 5100, loss = 1.6127579\n",
            "Iter 5101, loss = 1.2846508\n",
            "Iter 5102, loss = 0.9486691\n",
            "Iter 5103, loss = 0.8284663\n",
            "Iter 5104, loss = 1.1267209\n",
            "Iter 5105, loss = 1.1397378\n",
            "Iter 5106, loss = 1.0395856\n",
            "Iter 5107, loss = 1.1326659\n",
            "Iter 5108, loss = 0.93349266\n",
            "Iter 5109, loss = 0.8881194\n",
            "Iter 5110, loss = 1.0122691\n",
            "Iter 5111, loss = 0.98443997\n",
            "Iter 5112, loss = 1.0518432\n",
            "Iter 5113, loss = 0.7810563\n",
            "Iter 5114, loss = 1.0077646\n",
            "Iter 5115, loss = 0.8771713\n",
            "Iter 5116, loss = 1.1742806\n",
            "Iter 5117, loss = 0.99305797\n",
            "Iter 5118, loss = 0.9157562\n",
            "Iter 5119, loss = 0.9977232\n",
            "Iter 5120, loss = 0.9330938\n",
            "Iter 5121, loss = 1.182927\n",
            "Iter 5122, loss = 1.1893122\n",
            "Iter 5123, loss = 1.2321435\n",
            "Iter 5124, loss = 0.9915143\n",
            "Iter 5125, loss = 1.5127829\n",
            "Iter 5126, loss = 1.0862312\n",
            "Iter 5127, loss = 0.9982062\n",
            "Iter 5128, loss = 0.9113605\n",
            "Iter 5129, loss = 0.92216605\n",
            "Iter 5130, loss = 0.75185883\n",
            "Iter 5131, loss = 0.94614375\n",
            "Iter 5132, loss = 0.9920554\n",
            "Iter 5133, loss = 1.053364\n",
            "Iter 5134, loss = 1.9951136\n",
            "Iter 5135, loss = 1.070349\n",
            "Iter 5136, loss = 0.77573705\n",
            "Iter 5137, loss = 0.6948969\n",
            "Iter 5138, loss = 0.9166292\n",
            "Iter 5139, loss = 0.83802307\n",
            "Iter 5140, loss = 1.0110087\n",
            "Iter 5141, loss = 1.2236768\n",
            "Iter 5142, loss = 1.1356786\n",
            "Iter 5143, loss = 1.1355948\n",
            "Iter 5144, loss = 1.1699538\n",
            "Iter 5145, loss = 1.176399\n",
            "Iter 5146, loss = 0.9671403\n",
            "Iter 5147, loss = 0.9657607\n",
            "Iter 5148, loss = 0.8593091\n",
            "Iter 5149, loss = 0.94951105\n",
            "Iter 5150, loss = 1.0586638\n",
            "Iter 5151, loss = 1.3872938\n",
            "Iter 5152, loss = 1.1905864\n",
            "Iter 5153, loss = 1.0696027\n",
            "Iter 5154, loss = 1.0645223\n",
            "Iter 5155, loss = 1.6795424\n",
            "Iter 5156, loss = 1.1498532\n",
            "Iter 5157, loss = 1.223078\n",
            "Iter 5158, loss = 1.2811348\n",
            "Iter 5159, loss = 1.3306949\n",
            "Iter 5160, loss = 1.8169422\n",
            "Iter 5161, loss = 1.0498917\n",
            "Iter 5162, loss = 1.2057424\n",
            "Iter 5163, loss = 1.5354283\n",
            "Iter 5164, loss = 1.0864921\n",
            "Iter 5165, loss = 0.94196326\n",
            "Iter 5166, loss = 0.99332815\n",
            "Iter 5167, loss = 0.9178622\n",
            "Iter 5168, loss = 1.0845902\n",
            "Iter 5169, loss = 0.9921746\n",
            "Iter 5170, loss = 0.9567425\n",
            "Iter 5171, loss = 1.5704613\n",
            "Iter 5172, loss = 0.91677225\n",
            "Iter 5173, loss = 0.7112652\n",
            "Iter 5174, loss = 1.0486977\n",
            "Iter 5175, loss = 1.1726019\n",
            "Iter 5176, loss = 1.0789722\n",
            "Iter 5177, loss = 0.923975\n",
            "Iter 5178, loss = 1.281182\n",
            "Iter 5179, loss = 1.273057\n",
            "Iter 5180, loss = 0.9499546\n",
            "Iter 5181, loss = 1.2382903\n",
            "Iter 5182, loss = 1.1312935\n",
            "Iter 5183, loss = 0.9804455\n",
            "Iter 5184, loss = 0.9937186\n",
            "Iter 5185, loss = 0.7898505\n",
            "Iter 5186, loss = 1.4816164\n",
            "Iter 5187, loss = 0.97564304\n",
            "Iter 5188, loss = 0.89434826\n",
            "Iter 5189, loss = 1.0008981\n",
            "Iter 5190, loss = 1.0566093\n",
            "Iter 5191, loss = 1.0657454\n",
            "Iter 5192, loss = 1.1726775\n",
            "Iter 5193, loss = 0.8769809\n",
            "Iter 5194, loss = 0.78855705\n",
            "Iter 5195, loss = 0.69357294\n",
            "Iter 5196, loss = 1.1586374\n",
            "Iter 5197, loss = 1.0995156\n",
            "Iter 5198, loss = 1.0551114\n",
            "Iter 5199, loss = 1.1429949\n",
            "Iter 5200, loss = 1.2109516\n",
            "Iter 5201, loss = 1.0117335\n",
            "Iter 5202, loss = 0.8574805\n",
            "Iter 5203, loss = 1.3223543\n",
            "Iter 5204, loss = 1.2270312\n",
            "Iter 5205, loss = 1.043272\n",
            "Iter 5206, loss = 1.1215438\n",
            "Iter 5207, loss = 1.0471567\n",
            "Iter 5208, loss = 0.96880156\n",
            "Iter 5209, loss = 1.0277046\n",
            "Iter 5210, loss = 0.7454034\n",
            "Iter 5211, loss = 0.9768127\n",
            "Iter 5212, loss = 0.9376016\n",
            "Iter 5213, loss = 1.6416333\n",
            "Iter 5214, loss = 1.1395154\n",
            "Iter 5215, loss = 1.5284908\n",
            "Iter 5216, loss = 0.70671165\n",
            "Iter 5217, loss = 1.1993\n",
            "Iter 5218, loss = 0.920715\n",
            "Iter 5219, loss = 1.0196164\n",
            "Iter 5220, loss = 0.95404696\n",
            "Iter 5221, loss = 1.3327203\n",
            "Iter 5222, loss = 0.8563516\n",
            "Iter 5223, loss = 1.1076841\n",
            "Iter 5224, loss = 1.042439\n",
            "Iter 5225, loss = 1.1447182\n",
            "Iter 5226, loss = 0.8997307\n",
            "Iter 5227, loss = 1.4588091\n",
            "Iter 5228, loss = 0.96378374\n",
            "Iter 5229, loss = 1.170917\n",
            "Iter 5230, loss = 1.0999588\n",
            "Iter 5231, loss = 1.2126877\n",
            "Iter 5232, loss = 1.4624069\n",
            "Iter 5233, loss = 1.0593354\n",
            "Iter 5234, loss = 1.2809124\n",
            "Iter 5235, loss = 1.1136698\n",
            "Iter 5236, loss = 0.8871082\n",
            "Iter 5237, loss = 0.9810227\n",
            "Iter 5238, loss = 0.9356848\n",
            "Iter 5239, loss = 1.6068652\n",
            "Iter 5240, loss = 0.93679684\n",
            "Iter 5241, loss = 1.0150592\n",
            "Iter 5242, loss = 1.0921141\n",
            "Iter 5243, loss = 0.9761097\n",
            "Iter 5244, loss = 0.7481822\n",
            "Iter 5245, loss = 1.447639\n",
            "Iter 5246, loss = 1.0702114\n",
            "Iter 5247, loss = 1.5097959\n",
            "Iter 5248, loss = 0.7976133\n",
            "Iter 5249, loss = 1.1325853\n",
            "Iter 5250, loss = 1.2850287\n",
            "Iter 5251, loss = 1.0355289\n",
            "Iter 5252, loss = 0.7870774\n",
            "Iter 5253, loss = 1.3187361\n",
            "Iter 5254, loss = 1.3012469\n",
            "Iter 5255, loss = 0.9438766\n",
            "Iter 5256, loss = 1.136209\n",
            "Iter 5257, loss = 0.781266\n",
            "Iter 5258, loss = 1.3778048\n",
            "Iter 5259, loss = 1.1183094\n",
            "Iter 5260, loss = 0.9749806\n",
            "Iter 5261, loss = 0.8296007\n",
            "Iter 5262, loss = 0.97240496\n",
            "Iter 5263, loss = 1.2844375\n",
            "Iter 5264, loss = 1.2740927\n",
            "Iter 5265, loss = 1.3956479\n",
            "Iter 5266, loss = 0.8246455\n",
            "Iter 5267, loss = 1.4868419\n",
            "Iter 5268, loss = 0.98309624\n",
            "Iter 5269, loss = 1.5231203\n",
            "Iter 5270, loss = 1.064229\n",
            "Iter 5271, loss = 0.82167804\n",
            "Iter 5272, loss = 1.1927512\n",
            "Iter 5273, loss = 0.9390439\n",
            "Iter 5274, loss = 0.97181046\n",
            "Iter 5275, loss = 1.1764061\n",
            "Iter 5276, loss = 0.9615414\n",
            "Iter 5277, loss = 1.9539566\n",
            "Iter 5278, loss = 1.1144384\n",
            "Iter 5279, loss = 1.3724523\n",
            "Iter 5280, loss = 1.0569954\n",
            "Iter 5281, loss = 1.1898961\n",
            "Iter 5282, loss = 1.3254514\n",
            "Iter 5283, loss = 1.1859572\n",
            "Iter 5284, loss = 1.1965854\n",
            "Iter 5285, loss = 1.2954812\n",
            "Iter 5286, loss = 1.0836084\n",
            "Iter 5287, loss = 1.1256808\n",
            "Iter 5288, loss = 1.2275496\n",
            "Iter 5289, loss = 1.0410477\n",
            "Iter 5290, loss = 0.8776274\n",
            "Iter 5291, loss = 1.0482149\n",
            "Iter 5292, loss = 1.4066265\n",
            "Iter 5293, loss = 1.1703606\n",
            "Iter 5294, loss = 1.0493805\n",
            "Iter 5295, loss = 0.98016423\n",
            "Iter 5296, loss = 0.94666815\n",
            "Iter 5297, loss = 1.086157\n",
            "Iter 5298, loss = 1.118716\n",
            "Iter 5299, loss = 0.9132637\n",
            "Iter 5300, loss = 1.3203908\n",
            "Iter 5301, loss = 0.83173627\n",
            "Iter 5302, loss = 1.0235536\n",
            "Iter 5303, loss = 0.8508245\n",
            "Iter 5304, loss = 1.3099371\n",
            "Iter 5305, loss = 1.1359751\n",
            "Iter 5306, loss = 0.8518759\n",
            "Iter 5307, loss = 1.0142363\n",
            "Iter 5308, loss = 0.77451646\n",
            "Iter 5309, loss = 1.0411224\n",
            "Iter 5310, loss = 0.9202397\n",
            "Iter 5311, loss = 1.0415595\n",
            "Iter 5312, loss = 1.0479732\n",
            "Iter 5313, loss = 0.6982768\n",
            "Iter 5314, loss = 1.8087931\n",
            "Iter 5315, loss = 1.0112127\n",
            "Iter 5316, loss = 1.0784374\n",
            "Iter 5317, loss = 1.0446014\n",
            "Iter 5318, loss = 1.2362571\n",
            "Iter 5319, loss = 1.5694121\n",
            "Iter 5320, loss = 1.186294\n",
            "Iter 5321, loss = 1.0600824\n",
            "Iter 5322, loss = 1.3479444\n",
            "Iter 5323, loss = 0.9671016\n",
            "Iter 5324, loss = 1.2221191\n",
            "Iter 5325, loss = 1.3761511\n",
            "Iter 5326, loss = 0.661332\n",
            "Iter 5327, loss = 0.7852111\n",
            "Iter 5328, loss = 1.1471338\n",
            "Iter 5329, loss = 1.0688587\n",
            "Iter 5330, loss = 1.5471268\n",
            "Iter 5331, loss = 1.0377569\n",
            "Iter 5332, loss = 0.9197825\n",
            "Iter 5333, loss = 0.9882963\n",
            "Iter 5334, loss = 0.9820876\n",
            "Iter 5335, loss = 0.81081533\n",
            "Iter 5336, loss = 1.1136463\n",
            "Iter 5337, loss = 1.4230667\n",
            "Iter 5338, loss = 1.1420068\n",
            "Iter 5339, loss = 1.1706901\n",
            "Iter 5340, loss = 1.0764562\n",
            "Iter 5341, loss = 1.4278206\n",
            "Iter 5342, loss = 0.82652706\n",
            "Iter 5343, loss = 0.8665421\n",
            "Iter 5344, loss = 1.2464647\n",
            "Iter 5345, loss = 1.047512\n",
            "Iter 5346, loss = 0.9492084\n",
            "Iter 5347, loss = 1.2552153\n",
            "Iter 5348, loss = 1.5122998\n",
            "Iter 5349, loss = 1.4399693\n",
            "Iter 5350, loss = 0.8626166\n",
            "Iter 5351, loss = 1.0078689\n",
            "Iter 5352, loss = 1.1223065\n",
            "Iter 5353, loss = 1.0835702\n",
            "Iter 5354, loss = 0.86694276\n",
            "Iter 5355, loss = 1.4893099\n",
            "Iter 5356, loss = 0.7645092\n",
            "Iter 5357, loss = 1.1650176\n",
            "Iter 5358, loss = 0.99692374\n",
            "Iter 5359, loss = 0.7344455\n",
            "Iter 5360, loss = 1.1560491\n",
            "Iter 5361, loss = 1.1953807\n",
            "Iter 5362, loss = 1.5211838\n",
            "Iter 5363, loss = 1.5092646\n",
            "Iter 5364, loss = 0.7175766\n",
            "Iter 5365, loss = 1.1560738\n",
            "Iter 5366, loss = 1.1360202\n",
            "Iter 5367, loss = 1.1239829\n",
            "Iter 5368, loss = 1.0666208\n",
            "Iter 5369, loss = 0.967325\n",
            "Iter 5370, loss = 1.1165044\n",
            "Iter 5371, loss = 0.90186286\n",
            "Iter 5372, loss = 1.3424926\n",
            "Iter 5373, loss = 1.6371658\n",
            "Iter 5374, loss = 1.1261759\n",
            "Iter 5375, loss = 1.0442898\n",
            "Iter 5376, loss = 1.0947686\n",
            "Iter 5377, loss = 1.1590981\n",
            "Iter 5378, loss = 1.4160784\n",
            "Iter 5379, loss = 1.2171961\n",
            "Iter 5380, loss = 1.3319551\n",
            "Iter 5381, loss = 0.8417438\n",
            "Iter 5382, loss = 0.98545086\n",
            "Iter 5383, loss = 1.031143\n",
            "Iter 5384, loss = 0.9958577\n",
            "Iter 5385, loss = 1.2497568\n",
            "Iter 5386, loss = 1.1301061\n",
            "Iter 5387, loss = 1.2068387\n",
            "Iter 5388, loss = 0.9002032\n",
            "Iter 5389, loss = 1.0850258\n",
            "Iter 5390, loss = 0.9153801\n",
            "Iter 5391, loss = 1.0226581\n",
            "Iter 5392, loss = 0.92052054\n",
            "Iter 5393, loss = 0.9829623\n",
            "Iter 5394, loss = 1.1352232\n",
            "Iter 5395, loss = 0.9237133\n",
            "Iter 5396, loss = 0.87146175\n",
            "Iter 5397, loss = 1.1358991\n",
            "Iter 5398, loss = 0.9558991\n",
            "Iter 5399, loss = 1.2005486\n",
            "Iter 5400, loss = 0.82443213\n",
            "Iter 5401, loss = 0.8691041\n",
            "Iter 5402, loss = 1.124242\n",
            "Iter 5403, loss = 0.8251264\n",
            "Iter 5404, loss = 0.98545235\n",
            "Iter 5405, loss = 0.9527978\n",
            "Iter 5406, loss = 1.1491938\n",
            "Iter 5407, loss = 1.0406758\n",
            "Iter 5408, loss = 0.98549217\n",
            "Iter 5409, loss = 1.061132\n",
            "Iter 5410, loss = 0.97218835\n",
            "Iter 5411, loss = 1.120949\n",
            "Iter 5412, loss = 1.1665761\n",
            "Iter 5413, loss = 0.96073544\n",
            "Iter 5414, loss = 0.9447073\n",
            "Iter 5415, loss = 1.0647185\n",
            "Iter 5416, loss = 0.9848185\n",
            "Iter 5417, loss = 1.0254984\n",
            "Iter 5418, loss = 1.1217155\n",
            "Iter 5419, loss = 1.2181718\n",
            "Iter 5420, loss = 1.297854\n",
            "Iter 5421, loss = 1.1349881\n",
            "Iter 5422, loss = 1.0334028\n",
            "Iter 5423, loss = 1.0133266\n",
            "Iter 5424, loss = 1.4935765\n",
            "Iter 5425, loss = 1.0948328\n",
            "Iter 5426, loss = 1.3248181\n",
            "Iter 5427, loss = 0.99700916\n",
            "Iter 5428, loss = 1.0167603\n",
            "Iter 5429, loss = 0.9328194\n",
            "Iter 5430, loss = 0.86362875\n",
            "Iter 5431, loss = 0.9532179\n",
            "Iter 5432, loss = 1.3793219\n",
            "Iter 5433, loss = 0.93492824\n",
            "Iter 5434, loss = 1.1874993\n",
            "Iter 5435, loss = 1.0483944\n",
            "Iter 5436, loss = 1.1195892\n",
            "Iter 5437, loss = 1.0360674\n",
            "Iter 5438, loss = 1.1234275\n",
            "Iter 5439, loss = 0.7893266\n",
            "Iter 5440, loss = 0.68637365\n",
            "Iter 5441, loss = 1.0190028\n",
            "Iter 5442, loss = 0.973248\n",
            "Iter 5443, loss = 1.1055784\n",
            "Iter 5444, loss = 1.1170359\n",
            "Iter 5445, loss = 1.1470401\n",
            "Iter 5446, loss = 0.981157\n",
            "Iter 5447, loss = 1.0738615\n",
            "Iter 5448, loss = 1.0875422\n",
            "Iter 5449, loss = 1.5690432\n",
            "Iter 5450, loss = 0.81427234\n",
            "Iter 5451, loss = 0.6727473\n",
            "Iter 5452, loss = 1.0815818\n",
            "Iter 5453, loss = 0.98395014\n",
            "Iter 5454, loss = 1.0213009\n",
            "Iter 5455, loss = 1.27036\n",
            "Iter 5456, loss = 1.095997\n",
            "Iter 5457, loss = 0.72591025\n",
            "Iter 5458, loss = 0.7216295\n",
            "Iter 5459, loss = 1.1160066\n",
            "Iter 5460, loss = 1.0720869\n",
            "Iter 5461, loss = 0.97265196\n",
            "Iter 5462, loss = 1.1574106\n",
            "Iter 5463, loss = 1.2539132\n",
            "Iter 5464, loss = 0.99422574\n",
            "Iter 5465, loss = 1.1865399\n",
            "Iter 5466, loss = 1.1852617\n",
            "Iter 5467, loss = 1.5213835\n",
            "Iter 5468, loss = 1.2573057\n",
            "Iter 5469, loss = 0.96452415\n",
            "Iter 5470, loss = 0.8945719\n",
            "Iter 5471, loss = 1.6134138\n",
            "Iter 5472, loss = 1.0234632\n",
            "Iter 5473, loss = 1.3793322\n",
            "Iter 5474, loss = 1.2656544\n",
            "Iter 5475, loss = 1.0880401\n",
            "Iter 5476, loss = 1.0069356\n",
            "Iter 5477, loss = 0.92275834\n",
            "Iter 5478, loss = 0.8436452\n",
            "Iter 5479, loss = 0.8915647\n",
            "Iter 5480, loss = 1.5500288\n",
            "Iter 5481, loss = 0.9902499\n",
            "Iter 5482, loss = 0.9414692\n",
            "Iter 5483, loss = 1.4157497\n",
            "Iter 5484, loss = 1.0517198\n",
            "Iter 5485, loss = 0.9721425\n",
            "Iter 5486, loss = 0.901631\n",
            "Iter 5487, loss = 1.1933964\n",
            "Iter 5488, loss = 1.0102582\n",
            "Iter 5489, loss = 0.91394603\n",
            "Iter 5490, loss = 0.92967564\n",
            "Iter 5491, loss = 0.94398373\n",
            "Iter 5492, loss = 1.1385822\n",
            "Iter 5493, loss = 1.208011\n",
            "Iter 5494, loss = 1.0642798\n",
            "Iter 5495, loss = 0.8573127\n",
            "Iter 5496, loss = 1.0410354\n",
            "Iter 5497, loss = 1.1196889\n",
            "Iter 5498, loss = 1.2282197\n",
            "Iter 5499, loss = 1.3507714\n",
            "Iter 5500, loss = 1.3607439\n",
            "Iter 5501, loss = 1.5118232\n",
            "Iter 5502, loss = 1.1342807\n",
            "Iter 5503, loss = 0.91007924\n",
            "Iter 5504, loss = 1.1393954\n",
            "Iter 5505, loss = 0.86616087\n",
            "Iter 5506, loss = 1.3645229\n",
            "Iter 5507, loss = 1.0425858\n",
            "Iter 5508, loss = 1.3856335\n",
            "Iter 5509, loss = 0.99376273\n",
            "Iter 5510, loss = 1.0686336\n",
            "Iter 5511, loss = 1.2375932\n",
            "Iter 5512, loss = 1.3638673\n",
            "Iter 5513, loss = 1.3732631\n",
            "Iter 5514, loss = 0.9359548\n",
            "Iter 5515, loss = 1.0337127\n",
            "Iter 5516, loss = 1.0236677\n",
            "Iter 5517, loss = 0.9176555\n",
            "Iter 5518, loss = 0.8429581\n",
            "Iter 5519, loss = 1.071812\n",
            "Iter 5520, loss = 1.1166675\n",
            "Iter 5521, loss = 0.9543505\n",
            "Iter 5522, loss = 0.8897135\n",
            "Iter 5523, loss = 1.21205\n",
            "Iter 5524, loss = 1.5965638\n",
            "Iter 5525, loss = 0.992774\n",
            "Iter 5526, loss = 0.98191595\n",
            "Iter 5527, loss = 1.3365033\n",
            "Iter 5528, loss = 1.1774871\n",
            "Iter 5529, loss = 0.9833735\n",
            "Iter 5530, loss = 1.4823681\n",
            "Iter 5531, loss = 0.65157825\n",
            "Iter 5532, loss = 1.0555881\n",
            "Iter 5533, loss = 0.9596179\n",
            "Iter 5534, loss = 1.0053618\n",
            "Iter 5535, loss = 0.9982819\n",
            "Iter 5536, loss = 0.9174426\n",
            "Iter 5537, loss = 1.4725451\n",
            "Iter 5538, loss = 1.0806098\n",
            "Iter 5539, loss = 1.2439477\n",
            "Iter 5540, loss = 1.33764\n",
            "Iter 5541, loss = 1.1634371\n",
            "Iter 5542, loss = 1.3744049\n",
            "Iter 5543, loss = 1.4492158\n",
            "Iter 5544, loss = 1.1546607\n",
            "Iter 5545, loss = 0.8782044\n",
            "Iter 5546, loss = 1.1876683\n",
            "Iter 5547, loss = 0.9920952\n",
            "Iter 5548, loss = 0.9402549\n",
            "Iter 5549, loss = 1.1426948\n",
            "Iter 5550, loss = 0.84659874\n",
            "Iter 5551, loss = 1.4387825\n",
            "Iter 5552, loss = 1.0471169\n",
            "Iter 5553, loss = 1.2003474\n",
            "Iter 5554, loss = 1.0866287\n",
            "Iter 5555, loss = 1.1061127\n",
            "Iter 5556, loss = 0.8160106\n",
            "Iter 5557, loss = 0.9551961\n",
            "Iter 5558, loss = 1.1508167\n",
            "Iter 5559, loss = 0.8516134\n",
            "Iter 5560, loss = 1.203366\n",
            "Iter 5561, loss = 0.93285936\n",
            "Iter 5562, loss = 0.87814057\n",
            "Iter 5563, loss = 1.1406311\n",
            "Iter 5564, loss = 1.4376738\n",
            "Iter 5565, loss = 1.1970057\n",
            "Iter 5566, loss = 1.1598499\n",
            "Iter 5567, loss = 0.8457638\n",
            "Iter 5568, loss = 0.88430107\n",
            "Iter 5569, loss = 0.96153677\n",
            "Iter 5570, loss = 1.133795\n",
            "Iter 5571, loss = 1.028354\n",
            "Iter 5572, loss = 1.1092155\n",
            "Iter 5573, loss = 1.3797946\n",
            "Iter 5574, loss = 1.0494968\n",
            "Iter 5575, loss = 1.1483274\n",
            "Iter 5576, loss = 1.1717677\n",
            "Iter 5577, loss = 1.1901835\n",
            "Iter 5578, loss = 0.9555445\n",
            "Iter 5579, loss = 1.5549471\n",
            "Iter 5580, loss = 1.01851\n",
            "Iter 5581, loss = 1.2209444\n",
            "Iter 5582, loss = 1.385905\n",
            "Iter 5583, loss = 1.2627383\n",
            "Iter 5584, loss = 1.0097799\n",
            "Iter 5585, loss = 1.0483088\n",
            "Iter 5586, loss = 1.1427461\n",
            "Iter 5587, loss = 1.4463372\n",
            "Iter 5588, loss = 0.9527005\n",
            "Iter 5589, loss = 0.8926712\n",
            "Iter 5590, loss = 1.0898072\n",
            "Iter 5591, loss = 1.013104\n",
            "Iter 5592, loss = 0.9702898\n",
            "Iter 5593, loss = 1.0380685\n",
            "Iter 5594, loss = 1.3723457\n",
            "Iter 5595, loss = 1.219888\n",
            "Iter 5596, loss = 1.0411971\n",
            "Iter 5597, loss = 1.0146832\n",
            "Iter 5598, loss = 0.97852933\n",
            "Iter 5599, loss = 1.1807611\n",
            "Iter 5600, loss = 1.0159224\n",
            "Iter 5601, loss = 0.9421873\n",
            "Iter 5602, loss = 1.0239077\n",
            "Iter 5603, loss = 1.3172877\n",
            "Iter 5604, loss = 1.0547397\n",
            "Iter 5605, loss = 1.112467\n",
            "Iter 5606, loss = 1.6792428\n",
            "Iter 5607, loss = 0.9648132\n",
            "Iter 5608, loss = 0.9682009\n",
            "Iter 5609, loss = 0.98494375\n",
            "Iter 5610, loss = 1.319862\n",
            "Iter 5611, loss = 1.0406973\n",
            "Iter 5612, loss = 1.3169837\n",
            "Iter 5613, loss = 0.9285958\n",
            "Iter 5614, loss = 1.2069261\n",
            "Iter 5615, loss = 1.0351346\n",
            "Iter 5616, loss = 0.814463\n",
            "Iter 5617, loss = 1.081026\n",
            "Iter 5618, loss = 1.0226747\n",
            "Iter 5619, loss = 1.2342877\n",
            "Iter 5620, loss = 0.7094884\n",
            "Iter 5621, loss = 1.0761677\n",
            "Iter 5622, loss = 1.6558778\n",
            "Iter 5623, loss = 0.87229156\n",
            "Iter 5624, loss = 0.8070823\n",
            "Iter 5625, loss = 0.7767246\n",
            "Iter 5626, loss = 0.76431054\n",
            "Iter 5627, loss = 0.7785423\n",
            "Iter 5628, loss = 1.1443193\n",
            "Iter 5629, loss = 1.3696299\n",
            "Iter 5630, loss = 1.2375085\n",
            "Iter 5631, loss = 1.8511155\n",
            "Iter 5632, loss = 1.3580594\n",
            "Iter 5633, loss = 1.0288785\n",
            "Iter 5634, loss = 1.1639029\n",
            "Iter 5635, loss = 0.9618781\n",
            "Iter 5636, loss = 1.1565824\n",
            "Iter 5637, loss = 1.0067036\n",
            "Iter 5638, loss = 1.1374841\n",
            "Iter 5639, loss = 0.6558299\n",
            "Iter 5640, loss = 1.1825296\n",
            "Iter 5641, loss = 1.2346959\n",
            "Iter 5642, loss = 0.8806454\n",
            "Iter 5643, loss = 1.0347755\n",
            "Iter 5644, loss = 1.2208037\n",
            "Iter 5645, loss = 0.91319585\n",
            "Iter 5646, loss = 1.4997194\n",
            "Iter 5647, loss = 0.87581617\n",
            "Iter 5648, loss = 1.1085875\n",
            "Iter 5649, loss = 1.2842429\n",
            "Iter 5650, loss = 0.66862416\n",
            "Iter 5651, loss = 1.4112544\n",
            "Iter 5652, loss = 1.1559455\n",
            "Iter 5653, loss = 1.0411339\n",
            "Iter 5654, loss = 0.8929436\n",
            "Iter 5655, loss = 1.3635256\n",
            "Iter 5656, loss = 0.9102204\n",
            "Iter 5657, loss = 0.981723\n",
            "Iter 5658, loss = 1.2809746\n",
            "Iter 5659, loss = 0.8410177\n",
            "Iter 5660, loss = 1.4429789\n",
            "Iter 5661, loss = 1.250121\n",
            "Iter 5662, loss = 1.0044769\n",
            "Iter 5663, loss = 0.92073184\n",
            "Iter 5664, loss = 1.0803137\n",
            "Iter 5665, loss = 1.009913\n",
            "Iter 5666, loss = 1.090174\n",
            "Iter 5667, loss = 1.2642205\n",
            "Iter 5668, loss = 1.0662092\n",
            "Iter 5669, loss = 1.3350725\n",
            "Iter 5670, loss = 0.8730078\n",
            "Iter 5671, loss = 1.0468841\n",
            "Iter 5672, loss = 0.855745\n",
            "Iter 5673, loss = 0.97489136\n",
            "Iter 5674, loss = 1.1014237\n",
            "Iter 5675, loss = 1.1867315\n",
            "Iter 5676, loss = 0.874583\n",
            "Iter 5677, loss = 1.1554435\n",
            "Iter 5678, loss = 0.8896804\n",
            "Iter 5679, loss = 0.914531\n",
            "Iter 5680, loss = 0.85381204\n",
            "Iter 5681, loss = 1.16958\n",
            "Iter 5682, loss = 1.215891\n",
            "Iter 5683, loss = 0.9363302\n",
            "Iter 5684, loss = 0.9911226\n",
            "Iter 5685, loss = 1.1541829\n",
            "Iter 5686, loss = 0.9385877\n",
            "Iter 5687, loss = 0.87378764\n",
            "Iter 5688, loss = 1.1000671\n",
            "Iter 5689, loss = 1.012083\n",
            "Iter 5690, loss = 1.4385296\n",
            "Iter 5691, loss = 1.157138\n",
            "Iter 5692, loss = 0.8613817\n",
            "Iter 5693, loss = 0.93566227\n",
            "Iter 5694, loss = 1.2370563\n",
            "Iter 5695, loss = 0.9677634\n",
            "Iter 5696, loss = 1.1373839\n",
            "Iter 5697, loss = 1.21772\n",
            "Iter 5698, loss = 0.9443016\n",
            "Iter 5699, loss = 0.9473398\n",
            "Iter 5700, loss = 1.0111061\n",
            "Iter 5701, loss = 1.6614835\n",
            "Iter 5702, loss = 0.81411326\n",
            "Iter 5703, loss = 1.0145957\n",
            "Iter 5704, loss = 1.2998521\n",
            "Iter 5705, loss = 1.3080716\n",
            "Iter 5706, loss = 0.88223445\n",
            "Iter 5707, loss = 0.800233\n",
            "Iter 5708, loss = 0.9951805\n",
            "Iter 5709, loss = 0.9406884\n",
            "Iter 5710, loss = 0.70844555\n",
            "Iter 5711, loss = 1.4101832\n",
            "Iter 5712, loss = 0.9569129\n",
            "Iter 5713, loss = 1.0893129\n",
            "Iter 5714, loss = 0.8425836\n",
            "Iter 5715, loss = 1.0004005\n",
            "Iter 5716, loss = 1.2170217\n",
            "Iter 5717, loss = 0.9346816\n",
            "Iter 5718, loss = 0.82604754\n",
            "Iter 5719, loss = 1.0293415\n",
            "Iter 5720, loss = 0.7387507\n",
            "Iter 5721, loss = 0.8961152\n",
            "Iter 5722, loss = 1.1866167\n",
            "Iter 5723, loss = 1.066743\n",
            "Iter 5724, loss = 1.2258942\n",
            "Iter 5725, loss = 0.8845842\n",
            "Iter 5726, loss = 1.4521394\n",
            "Iter 5727, loss = 1.220448\n",
            "Iter 5728, loss = 0.7632221\n",
            "Iter 5729, loss = 1.3998827\n",
            "Iter 5730, loss = 0.99183756\n",
            "Iter 5731, loss = 1.0579965\n",
            "Iter 5732, loss = 0.99365956\n",
            "Iter 5733, loss = 1.2839551\n",
            "Iter 5734, loss = 1.2766056\n",
            "Iter 5735, loss = 0.7898154\n",
            "Iter 5736, loss = 1.2038523\n",
            "Iter 5737, loss = 0.9186648\n",
            "Iter 5738, loss = 1.1603138\n",
            "Iter 5739, loss = 1.3349367\n",
            "Iter 5740, loss = 1.5713031\n",
            "Iter 5741, loss = 1.282113\n",
            "Iter 5742, loss = 0.94247174\n",
            "Iter 5743, loss = 0.92671025\n",
            "Iter 5744, loss = 0.86609435\n",
            "Iter 5745, loss = 0.91084564\n",
            "Iter 5746, loss = 0.9288584\n",
            "Iter 5747, loss = 1.0157249\n",
            "Iter 5748, loss = 1.3272879\n",
            "Iter 5749, loss = 0.8789622\n",
            "Iter 5750, loss = 1.0270938\n",
            "Iter 5751, loss = 0.9509003\n",
            "Iter 5752, loss = 1.6213984\n",
            "Iter 5753, loss = 1.6647844\n",
            "Iter 5754, loss = 1.2922788\n",
            "Iter 5755, loss = 0.8491025\n",
            "Iter 5756, loss = 0.9618139\n",
            "Iter 5757, loss = 1.2074691\n",
            "Iter 5758, loss = 1.0718725\n",
            "Iter 5759, loss = 1.1777527\n",
            "Iter 5760, loss = 1.0651654\n",
            "Iter 5761, loss = 1.1287372\n",
            "Iter 5762, loss = 1.2126372\n",
            "Iter 5763, loss = 1.2543285\n",
            "Iter 5764, loss = 1.2394639\n",
            "Iter 5765, loss = 1.3437955\n",
            "Iter 5766, loss = 1.0089003\n",
            "Iter 5767, loss = 1.0660477\n",
            "Iter 5768, loss = 0.97404164\n",
            "Iter 5769, loss = 1.0519419\n",
            "Iter 5770, loss = 0.991251\n",
            "Iter 5771, loss = 1.0750237\n",
            "Iter 5772, loss = 1.0667453\n",
            "Iter 5773, loss = 0.84513766\n",
            "Iter 5774, loss = 1.3599358\n",
            "Iter 5775, loss = 1.1997538\n",
            "Iter 5776, loss = 1.4259336\n",
            "Iter 5777, loss = 1.3443661\n",
            "Iter 5778, loss = 0.8641333\n",
            "Iter 5779, loss = 1.3896515\n",
            "Iter 5780, loss = 1.1735934\n",
            "Iter 5781, loss = 1.0273652\n",
            "Iter 5782, loss = 1.2383227\n",
            "Iter 5783, loss = 1.1308169\n",
            "Iter 5784, loss = 0.980309\n",
            "Iter 5785, loss = 0.91761065\n",
            "Iter 5786, loss = 0.7818271\n",
            "Iter 5787, loss = 1.3051848\n",
            "Iter 5788, loss = 1.4938812\n",
            "Iter 5789, loss = 0.9438634\n",
            "Iter 5790, loss = 0.9515483\n",
            "Iter 5791, loss = 1.1614795\n",
            "Iter 5792, loss = 1.3107657\n",
            "Iter 5793, loss = 0.9574276\n",
            "Iter 5794, loss = 1.1625527\n",
            "Iter 5795, loss = 0.9835215\n",
            "Iter 5796, loss = 1.4555528\n",
            "Iter 5797, loss = 0.93093276\n",
            "Iter 5798, loss = 1.1706047\n",
            "Iter 5799, loss = 0.914751\n",
            "Iter 5800, loss = 1.1615254\n",
            "Iter 5801, loss = 1.07009\n",
            "Iter 5802, loss = 1.0963275\n",
            "Iter 5803, loss = 0.9930566\n",
            "Iter 5804, loss = 0.83035076\n",
            "Iter 5805, loss = 1.0449269\n",
            "Iter 5806, loss = 1.2159978\n",
            "Iter 5807, loss = 1.3960384\n",
            "Iter 5808, loss = 0.9971348\n",
            "Iter 5809, loss = 1.116523\n",
            "Iter 5810, loss = 1.086481\n",
            "Iter 5811, loss = 1.0178515\n",
            "Iter 5812, loss = 0.99977386\n",
            "Iter 5813, loss = 1.1059041\n",
            "Iter 5814, loss = 0.9850143\n",
            "Iter 5815, loss = 1.0770637\n",
            "Iter 5816, loss = 0.90606606\n",
            "Iter 5817, loss = 0.7268146\n",
            "Iter 5818, loss = 0.94913864\n",
            "Iter 5819, loss = 0.6931232\n",
            "Iter 5820, loss = 0.9087002\n",
            "Iter 5821, loss = 0.9055905\n",
            "Iter 5822, loss = 1.0193915\n",
            "Iter 5823, loss = 0.8552446\n",
            "Iter 5824, loss = 1.0518293\n",
            "Iter 5825, loss = 0.9932782\n",
            "Iter 5826, loss = 1.1123687\n",
            "Iter 5827, loss = 0.9167995\n",
            "Iter 5828, loss = 1.069855\n",
            "Iter 5829, loss = 0.98855644\n",
            "Iter 5830, loss = 0.81906044\n",
            "Iter 5831, loss = 0.82164943\n",
            "Iter 5832, loss = 0.93699706\n",
            "Iter 5833, loss = 1.2125932\n",
            "Iter 5834, loss = 0.83477575\n",
            "Iter 5835, loss = 0.9662142\n",
            "Iter 5836, loss = 0.7211616\n",
            "Iter 5837, loss = 1.0618255\n",
            "Iter 5838, loss = 1.6634064\n",
            "Iter 5839, loss = 0.9669897\n",
            "Iter 5840, loss = 1.3261961\n",
            "Iter 5841, loss = 0.8576799\n",
            "Iter 5842, loss = 1.5123053\n",
            "Iter 5843, loss = 0.8950763\n",
            "Iter 5844, loss = 1.3257234\n",
            "Iter 5845, loss = 1.1101619\n",
            "Iter 5846, loss = 1.0237272\n",
            "Iter 5847, loss = 1.426142\n",
            "Iter 5848, loss = 1.083525\n",
            "Iter 5849, loss = 1.1038079\n",
            "Iter 5850, loss = 0.90829206\n",
            "Iter 5851, loss = 1.1968025\n",
            "Iter 5852, loss = 0.83123577\n",
            "Iter 5853, loss = 0.86416614\n",
            "Iter 5854, loss = 1.3705261\n",
            "Iter 5855, loss = 1.0068568\n",
            "Iter 5856, loss = 0.8771192\n",
            "Iter 5857, loss = 1.1545907\n",
            "Iter 5858, loss = 1.1547288\n",
            "Iter 5859, loss = 0.83102584\n",
            "Iter 5860, loss = 1.0007576\n",
            "Iter 5861, loss = 0.80073655\n",
            "Iter 5862, loss = 1.4760556\n",
            "Iter 5863, loss = 1.2857673\n",
            "Iter 5864, loss = 1.3586946\n",
            "Iter 5865, loss = 1.074152\n",
            "Iter 5866, loss = 1.1286123\n",
            "Iter 5867, loss = 0.86156213\n",
            "Iter 5868, loss = 1.3267958\n",
            "Iter 5869, loss = 1.3863823\n",
            "Iter 5870, loss = 0.8633733\n",
            "Iter 5871, loss = 1.1953156\n",
            "Iter 5872, loss = 1.5655258\n",
            "Iter 5873, loss = 1.1729535\n",
            "Iter 5874, loss = 0.9743878\n",
            "Iter 5875, loss = 1.3045306\n",
            "Iter 5876, loss = 1.0195988\n",
            "Iter 5877, loss = 0.9824693\n",
            "Iter 5878, loss = 0.8122748\n",
            "Iter 5879, loss = 1.3360314\n",
            "Iter 5880, loss = 0.9477898\n",
            "Iter 5881, loss = 1.217907\n",
            "Iter 5882, loss = 1.2984374\n",
            "Iter 5883, loss = 0.9616071\n",
            "Iter 5884, loss = 1.0851238\n",
            "Iter 5885, loss = 1.3912508\n",
            "Iter 5886, loss = 1.2142962\n",
            "Iter 5887, loss = 1.1760957\n",
            "Iter 5888, loss = 1.2939295\n",
            "Iter 5889, loss = 1.2786814\n",
            "Iter 5890, loss = 1.1717166\n",
            "Iter 5891, loss = 1.0076054\n",
            "Iter 5892, loss = 0.8979684\n",
            "Iter 5893, loss = 1.1386476\n",
            "Iter 5894, loss = 1.1088569\n",
            "Iter 5895, loss = 1.0155578\n",
            "Iter 5896, loss = 1.04149\n",
            "Iter 5897, loss = 1.090829\n",
            "Iter 5898, loss = 1.0773857\n",
            "Iter 5899, loss = 0.99416566\n",
            "Iter 5900, loss = 0.9717197\n",
            "Iter 5901, loss = 1.1986122\n",
            "Iter 5902, loss = 1.0750196\n",
            "Iter 5903, loss = 0.8959791\n",
            "Iter 5904, loss = 0.8219137\n",
            "Iter 5905, loss = 1.2216527\n",
            "Iter 5906, loss = 0.93490314\n",
            "Iter 5907, loss = 0.97758394\n",
            "Iter 5908, loss = 1.026298\n",
            "Iter 5909, loss = 0.6937394\n",
            "Iter 5910, loss = 1.3444139\n",
            "Iter 5911, loss = 1.0632558\n",
            "Iter 5912, loss = 1.0666225\n",
            "Iter 5913, loss = 1.1608989\n",
            "Iter 5914, loss = 1.427171\n",
            "Iter 5915, loss = 1.5144014\n",
            "Iter 5916, loss = 1.0720296\n",
            "Iter 5917, loss = 0.8573036\n",
            "Iter 5918, loss = 1.1428633\n",
            "Iter 5919, loss = 1.1600592\n",
            "Iter 5920, loss = 1.2844764\n",
            "Iter 5921, loss = 1.2542033\n",
            "Iter 5922, loss = 0.7871946\n",
            "Iter 5923, loss = 1.0461972\n",
            "Iter 5924, loss = 1.6160173\n",
            "Iter 5925, loss = 1.0969539\n",
            "Iter 5926, loss = 1.0899326\n",
            "Iter 5927, loss = 1.1922119\n",
            "Iter 5928, loss = 0.9512236\n",
            "Iter 5929, loss = 1.101373\n",
            "Iter 5930, loss = 1.1895764\n",
            "Iter 5931, loss = 0.906605\n",
            "Iter 5932, loss = 1.3592403\n",
            "Iter 5933, loss = 0.9855442\n",
            "Iter 5934, loss = 1.1632967\n",
            "Iter 5935, loss = 1.0480063\n",
            "Iter 5936, loss = 1.0727043\n",
            "Iter 5937, loss = 1.0665967\n",
            "Iter 5938, loss = 1.0411365\n",
            "Iter 5939, loss = 1.4749216\n",
            "Iter 5940, loss = 0.92224014\n",
            "Iter 5941, loss = 0.9608928\n",
            "Iter 5942, loss = 0.8877675\n",
            "Iter 5943, loss = 1.3812432\n",
            "Iter 5944, loss = 1.0453464\n",
            "Iter 5945, loss = 1.1554033\n",
            "Iter 5946, loss = 0.9379478\n",
            "Iter 5947, loss = 1.3733952\n",
            "Iter 5948, loss = 1.3365059\n",
            "Iter 5949, loss = 1.5030428\n",
            "Iter 5950, loss = 1.1704612\n",
            "Iter 5951, loss = 0.93409413\n",
            "Iter 5952, loss = 0.914853\n",
            "Iter 5953, loss = 1.138186\n",
            "Iter 5954, loss = 0.953127\n",
            "Iter 5955, loss = 0.9237231\n",
            "Iter 5956, loss = 1.1934053\n",
            "Iter 5957, loss = 1.1962743\n",
            "Iter 5958, loss = 1.2013297\n",
            "Iter 5959, loss = 1.1364315\n",
            "Iter 5960, loss = 1.1850867\n",
            "Iter 5961, loss = 1.0816486\n",
            "Iter 5962, loss = 1.3761517\n",
            "Iter 5963, loss = 1.2922311\n",
            "Iter 5964, loss = 1.0554094\n",
            "Iter 5965, loss = 0.9858131\n",
            "Iter 5966, loss = 0.97779125\n",
            "Iter 5967, loss = 1.0142732\n",
            "Iter 5968, loss = 1.3882226\n",
            "Iter 5969, loss = 1.0430899\n",
            "Iter 5970, loss = 0.9737757\n",
            "Iter 5971, loss = 1.0318856\n",
            "Iter 5972, loss = 1.1278689\n",
            "Iter 5973, loss = 1.1316464\n",
            "Iter 5974, loss = 0.9527173\n",
            "Iter 5975, loss = 1.2945143\n",
            "Iter 5976, loss = 1.4155475\n",
            "Iter 5977, loss = 0.8746752\n",
            "Iter 5978, loss = 0.98307586\n",
            "Iter 5979, loss = 1.4108433\n",
            "Iter 5980, loss = 0.8715434\n",
            "Iter 5981, loss = 1.2000847\n",
            "Iter 5982, loss = 1.0320405\n",
            "Iter 5983, loss = 1.2787894\n",
            "Iter 5984, loss = 0.9668645\n",
            "Iter 5985, loss = 0.92971635\n",
            "Iter 5986, loss = 1.3216863\n",
            "Iter 5987, loss = 0.9320279\n",
            "Iter 5988, loss = 1.054492\n",
            "Iter 5989, loss = 1.0709515\n",
            "Iter 5990, loss = 0.9477065\n",
            "Iter 5991, loss = 0.9282931\n",
            "Iter 5992, loss = 1.1506906\n",
            "Iter 5993, loss = 1.2050323\n",
            "Iter 5994, loss = 1.1732881\n",
            "Iter 5995, loss = 1.0574172\n",
            "Iter 5996, loss = 1.0203981\n",
            "Iter 5997, loss = 1.0790482\n",
            "Iter 5998, loss = 1.607406\n",
            "Iter 5999, loss = 1.1465752\n",
            "Iter 6000, loss = 0.8667357\n",
            "Iter 6001, loss = 0.8662124\n",
            "Iter 6002, loss = 0.7749123\n",
            "Iter 6003, loss = 1.3263524\n",
            "Iter 6004, loss = 1.2417078\n",
            "Iter 6005, loss = 1.2267236\n",
            "Iter 6006, loss = 1.0536281\n",
            "Iter 6007, loss = 1.1360905\n",
            "Iter 6008, loss = 0.9703765\n",
            "Iter 6009, loss = 0.89575756\n",
            "Iter 6010, loss = 0.6118438\n",
            "Iter 6011, loss = 1.1505754\n",
            "Iter 6012, loss = 1.3551466\n",
            "Iter 6013, loss = 1.1246729\n",
            "Iter 6014, loss = 1.7128478\n",
            "Iter 6015, loss = 0.89044356\n",
            "Iter 6016, loss = 1.5093181\n",
            "Iter 6017, loss = 1.2481178\n",
            "Iter 6018, loss = 1.3516637\n",
            "Iter 6019, loss = 1.1902008\n",
            "Iter 6020, loss = 1.172892\n",
            "Iter 6021, loss = 1.335795\n",
            "Iter 6022, loss = 0.617653\n",
            "Iter 6023, loss = 1.481081\n",
            "Iter 6024, loss = 0.8609157\n",
            "Iter 6025, loss = 0.9530771\n",
            "Iter 6026, loss = 0.95348394\n",
            "Iter 6027, loss = 0.91081417\n",
            "Iter 6028, loss = 1.2406335\n",
            "Iter 6029, loss = 0.7159813\n",
            "Iter 6030, loss = 0.8528421\n",
            "Iter 6031, loss = 1.3082141\n",
            "Iter 6032, loss = 1.3344436\n",
            "Iter 6033, loss = 1.4240758\n",
            "Iter 6034, loss = 1.174413\n",
            "Iter 6035, loss = 1.2399297\n",
            "Iter 6036, loss = 1.0793858\n",
            "Iter 6037, loss = 0.7483041\n",
            "Iter 6038, loss = 1.1131755\n",
            "Iter 6039, loss = 1.3926141\n",
            "Iter 6040, loss = 1.4376259\n",
            "Iter 6041, loss = 1.1651286\n",
            "Iter 6042, loss = 1.6231451\n",
            "Iter 6043, loss = 1.0723931\n",
            "Iter 6044, loss = 1.0515895\n",
            "Iter 6045, loss = 1.2239106\n",
            "Iter 6046, loss = 1.1386893\n",
            "Iter 6047, loss = 1.1819973\n",
            "Iter 6048, loss = 1.385814\n",
            "Iter 6049, loss = 1.0448883\n",
            "Iter 6050, loss = 0.8950455\n",
            "Iter 6051, loss = 0.90304214\n",
            "Iter 6052, loss = 1.0169504\n",
            "Iter 6053, loss = 2.0047636\n",
            "Iter 6054, loss = 0.97991073\n",
            "Iter 6055, loss = 0.9657449\n",
            "Iter 6056, loss = 1.2905484\n",
            "Iter 6057, loss = 1.1931006\n",
            "Iter 6058, loss = 1.2063478\n",
            "Iter 6059, loss = 1.4151102\n",
            "Iter 6060, loss = 1.3092148\n",
            "Iter 6061, loss = 1.0502295\n",
            "Iter 6062, loss = 0.9785944\n",
            "Iter 6063, loss = 0.82481253\n",
            "Iter 6064, loss = 0.8924743\n",
            "Iter 6065, loss = 0.89441305\n",
            "Iter 6066, loss = 1.2188705\n",
            "Iter 6067, loss = 0.84565824\n",
            "Iter 6068, loss = 0.9275135\n",
            "Iter 6069, loss = 0.859056\n",
            "Iter 6070, loss = 1.0752473\n",
            "Iter 6071, loss = 1.2483456\n",
            "Iter 6072, loss = 0.8682676\n",
            "Iter 6073, loss = 1.2438521\n",
            "Iter 6074, loss = 1.3615549\n",
            "Iter 6075, loss = 0.84375215\n",
            "Iter 6076, loss = 0.99131954\n",
            "Iter 6077, loss = 0.88076717\n",
            "Iter 6078, loss = 1.0442591\n",
            "Iter 6079, loss = 0.9551039\n",
            "Iter 6080, loss = 1.0508657\n",
            "Iter 6081, loss = 0.82755655\n",
            "Iter 6082, loss = 0.8593636\n",
            "Iter 6083, loss = 1.2833364\n",
            "Iter 6084, loss = 1.2916211\n",
            "Iter 6085, loss = 1.2345994\n",
            "Iter 6086, loss = 1.1319741\n",
            "Iter 6087, loss = 1.2134992\n",
            "Iter 6088, loss = 1.4958227\n",
            "Iter 6089, loss = 0.9838091\n",
            "Iter 6090, loss = 1.226451\n",
            "Iter 6091, loss = 1.3695185\n",
            "Iter 6092, loss = 1.0905747\n",
            "Iter 6093, loss = 0.91944754\n",
            "Iter 6094, loss = 1.2136586\n",
            "Iter 6095, loss = 1.7079551\n",
            "Iter 6096, loss = 1.0349183\n",
            "Iter 6097, loss = 1.1957512\n",
            "Iter 6098, loss = 1.1538545\n",
            "Iter 6099, loss = 1.030498\n",
            "Iter 6100, loss = 0.7662517\n",
            "Iter 6101, loss = 1.0301964\n",
            "Iter 6102, loss = 1.1070642\n",
            "Iter 6103, loss = 0.8861015\n",
            "Iter 6104, loss = 1.0901088\n",
            "Iter 6105, loss = 1.2011422\n",
            "Iter 6106, loss = 1.120446\n",
            "Iter 6107, loss = 1.075638\n",
            "Iter 6108, loss = 1.1025069\n",
            "Iter 6109, loss = 0.6248486\n",
            "Iter 6110, loss = 0.9753129\n",
            "Iter 6111, loss = 1.4374039\n",
            "Iter 6112, loss = 0.81285155\n",
            "Iter 6113, loss = 1.0793877\n",
            "Iter 6114, loss = 1.1780448\n",
            "Iter 6115, loss = 1.252691\n",
            "Iter 6116, loss = 1.0953889\n",
            "Iter 6117, loss = 0.95742506\n",
            "Iter 6118, loss = 1.2834973\n",
            "Iter 6119, loss = 1.1864907\n",
            "Iter 6120, loss = 1.1311657\n",
            "Iter 6121, loss = 0.8437205\n",
            "Iter 6122, loss = 0.93660253\n",
            "Iter 6123, loss = 1.3981498\n",
            "Iter 6124, loss = 1.0094727\n",
            "Iter 6125, loss = 0.8816197\n",
            "Iter 6126, loss = 1.1083548\n",
            "Iter 6127, loss = 0.76705575\n",
            "Iter 6128, loss = 0.9206612\n",
            "Iter 6129, loss = 1.1911974\n",
            "Iter 6130, loss = 1.0517031\n",
            "Iter 6131, loss = 0.93130934\n",
            "Iter 6132, loss = 0.8821464\n",
            "Iter 6133, loss = 1.2670269\n",
            "Iter 6134, loss = 1.5515997\n",
            "Iter 6135, loss = 1.1586065\n",
            "Iter 6136, loss = 0.8353055\n",
            "Iter 6137, loss = 0.90375817\n",
            "Iter 6138, loss = 1.2555711\n",
            "Iter 6139, loss = 1.2736492\n",
            "Iter 6140, loss = 1.0209308\n",
            "Iter 6141, loss = 1.4448543\n",
            "Iter 6142, loss = 0.8905455\n",
            "Iter 6143, loss = 0.7695708\n",
            "Iter 6144, loss = 1.4469391\n",
            "Iter 6145, loss = 1.1817787\n",
            "Iter 6146, loss = 1.0200257\n",
            "Iter 6147, loss = 1.0459085\n",
            "Iter 6148, loss = 1.1804736\n",
            "Iter 6149, loss = 1.159467\n",
            "Iter 6150, loss = 0.93281126\n",
            "Iter 6151, loss = 1.2289708\n",
            "Iter 6152, loss = 1.0093405\n",
            "Iter 6153, loss = 1.3457247\n",
            "Iter 6154, loss = 1.0990508\n",
            "Iter 6155, loss = 1.3882045\n",
            "Iter 6156, loss = 1.1526506\n",
            "Iter 6157, loss = 1.2904446\n",
            "Iter 6158, loss = 0.9081351\n",
            "Iter 6159, loss = 1.2160094\n",
            "Iter 6160, loss = 1.1293125\n",
            "Iter 6161, loss = 0.9242456\n",
            "Iter 6162, loss = 0.9378394\n",
            "Iter 6163, loss = 1.6213382\n",
            "Iter 6164, loss = 1.1003447\n",
            "Iter 6165, loss = 1.1550884\n",
            "Iter 6166, loss = 1.294806\n",
            "Iter 6167, loss = 0.76316255\n",
            "Iter 6168, loss = 0.93230736\n",
            "Iter 6169, loss = 0.9336042\n",
            "Iter 6170, loss = 1.0599465\n",
            "Iter 6171, loss = 1.3555562\n",
            "Iter 6172, loss = 0.8852626\n",
            "Iter 6173, loss = 1.2711515\n",
            "Iter 6174, loss = 1.02524\n",
            "Iter 6175, loss = 0.89063686\n",
            "Iter 6176, loss = 1.0420691\n",
            "Iter 6177, loss = 0.9952852\n",
            "Iter 6178, loss = 1.1134479\n",
            "Iter 6179, loss = 0.7356556\n",
            "Iter 6180, loss = 1.1308587\n",
            "Iter 6181, loss = 1.0368223\n",
            "Iter 6182, loss = 1.9497166\n",
            "Iter 6183, loss = 0.99732655\n",
            "Iter 6184, loss = 0.84444964\n",
            "Iter 6185, loss = 1.1294684\n",
            "Iter 6186, loss = 1.3633318\n",
            "Iter 6187, loss = 1.1723835\n",
            "Iter 6188, loss = 1.0790061\n",
            "Iter 6189, loss = 0.9845697\n",
            "Iter 6190, loss = 1.0499585\n",
            "Iter 6191, loss = 0.9618269\n",
            "Iter 6192, loss = 0.90943223\n",
            "Iter 6193, loss = 1.3632674\n",
            "Iter 6194, loss = 0.747337\n",
            "Iter 6195, loss = 0.99267286\n",
            "Iter 6196, loss = 0.70526576\n",
            "Iter 6197, loss = 0.831607\n",
            "Iter 6198, loss = 1.2352463\n",
            "Iter 6199, loss = 1.0794278\n",
            "Iter 6200, loss = 1.2734458\n",
            "Iter 6201, loss = 0.98169065\n",
            "Iter 6202, loss = 0.87095535\n",
            "Iter 6203, loss = 1.3631638\n",
            "Iter 6204, loss = 1.3248967\n",
            "Iter 6205, loss = 1.3327023\n",
            "Iter 6206, loss = 1.1204047\n",
            "Iter 6207, loss = 1.0315008\n",
            "Iter 6208, loss = 0.97336626\n",
            "Iter 6209, loss = 0.8498316\n",
            "Iter 6210, loss = 0.9128374\n",
            "Iter 6211, loss = 1.2505739\n",
            "Iter 6212, loss = 1.005923\n",
            "Iter 6213, loss = 0.9176557\n",
            "Iter 6214, loss = 0.979581\n",
            "Iter 6215, loss = 0.9561875\n",
            "Iter 6216, loss = 1.313724\n",
            "Iter 6217, loss = 0.97293043\n",
            "Iter 6218, loss = 1.3746126\n",
            "Iter 6219, loss = 0.9511416\n",
            "Iter 6220, loss = 0.8859602\n",
            "Iter 6221, loss = 1.3477256\n",
            "Iter 6222, loss = 1.1590405\n",
            "Iter 6223, loss = 1.1531705\n",
            "Iter 6224, loss = 1.0980227\n",
            "Iter 6225, loss = 1.0599815\n",
            "Iter 6226, loss = 1.1643022\n",
            "Iter 6227, loss = 1.1855032\n",
            "Iter 6228, loss = 0.7802826\n",
            "Iter 6229, loss = 1.2054693\n",
            "Iter 6230, loss = 0.87318975\n",
            "Iter 6231, loss = 0.8978526\n",
            "Iter 6232, loss = 0.9967822\n",
            "Iter 6233, loss = 1.3820189\n",
            "Iter 6234, loss = 1.48467\n",
            "Iter 6235, loss = 0.86833775\n",
            "Iter 6236, loss = 1.1843331\n",
            "Iter 6237, loss = 1.4665271\n",
            "Iter 6238, loss = 1.3825166\n",
            "Iter 6239, loss = 0.9214117\n",
            "Iter 6240, loss = 0.97756016\n",
            "Iter 6241, loss = 0.73019516\n",
            "Iter 6242, loss = 1.1512865\n",
            "Iter 6243, loss = 1.1005001\n",
            "Iter 6244, loss = 0.9916101\n",
            "Iter 6245, loss = 0.7935512\n",
            "Iter 6246, loss = 1.2879856\n",
            "Iter 6247, loss = 1.0044972\n",
            "Iter 6248, loss = 0.9561986\n",
            "Iter 6249, loss = 0.93755716\n",
            "Iter 6250, loss = 0.9755616\n",
            "Iter 6251, loss = 0.90275514\n",
            "Iter 6252, loss = 0.87618375\n",
            "Iter 6253, loss = 0.98475355\n",
            "Iter 6254, loss = 1.1230669\n",
            "Iter 6255, loss = 0.9317177\n",
            "Iter 6256, loss = 1.3934503\n",
            "Iter 6257, loss = 0.99324167\n",
            "Iter 6258, loss = 0.79892653\n",
            "Iter 6259, loss = 0.94551027\n",
            "Iter 6260, loss = 1.1588945\n",
            "Iter 6261, loss = 1.0305173\n",
            "Iter 6262, loss = 0.88060606\n",
            "Iter 6263, loss = 1.0112922\n",
            "Iter 6264, loss = 1.1587821\n",
            "Iter 6265, loss = 1.0440888\n",
            "Iter 6266, loss = 1.2177095\n",
            "Iter 6267, loss = 1.1473197\n",
            "Iter 6268, loss = 0.9362538\n",
            "Iter 6269, loss = 0.76027775\n",
            "Iter 6270, loss = 0.8794396\n",
            "Iter 6271, loss = 1.0875291\n",
            "Iter 6272, loss = 1.4497762\n",
            "Iter 6273, loss = 1.1019685\n",
            "Iter 6274, loss = 1.1599125\n",
            "Iter 6275, loss = 0.8623451\n",
            "Iter 6276, loss = 1.0716488\n",
            "Iter 6277, loss = 1.5019776\n",
            "Iter 6278, loss = 0.92285293\n",
            "Iter 6279, loss = 0.81394786\n",
            "Iter 6280, loss = 1.2327479\n",
            "Iter 6281, loss = 1.0136741\n",
            "Iter 6282, loss = 0.9393041\n",
            "Iter 6283, loss = 0.9488025\n",
            "Iter 6284, loss = 1.2458366\n",
            "Iter 6285, loss = 0.88657326\n",
            "Iter 6286, loss = 1.1817858\n",
            "Iter 6287, loss = 0.8291137\n",
            "Iter 6288, loss = 0.91160274\n",
            "Iter 6289, loss = 1.0226688\n",
            "Iter 6290, loss = 1.1354463\n",
            "Iter 6291, loss = 0.96243346\n",
            "Iter 6292, loss = 0.9259255\n",
            "Iter 6293, loss = 1.2249727\n",
            "Iter 6294, loss = 0.87675244\n",
            "Iter 6295, loss = 0.8863776\n",
            "Iter 6296, loss = 1.042922\n",
            "Iter 6297, loss = 0.9203647\n",
            "Iter 6298, loss = 1.2447248\n",
            "Iter 6299, loss = 2.0363371\n",
            "Iter 6300, loss = 0.96099126\n",
            "Iter 6301, loss = 1.4449401\n",
            "Iter 6302, loss = 1.1168287\n",
            "Iter 6303, loss = 0.95037115\n",
            "Iter 6304, loss = 1.1604817\n",
            "Iter 6305, loss = 0.9221475\n",
            "Iter 6306, loss = 0.80666804\n",
            "Iter 6307, loss = 1.1893265\n",
            "Iter 6308, loss = 0.92294997\n",
            "Iter 6309, loss = 1.1672378\n",
            "Iter 6310, loss = 0.70387405\n",
            "Iter 6311, loss = 0.9700607\n",
            "Iter 6312, loss = 1.4133673\n",
            "Iter 6313, loss = 1.1565487\n",
            "Iter 6314, loss = 0.85774827\n",
            "Iter 6315, loss = 1.035799\n",
            "Iter 6316, loss = 1.3251516\n",
            "Iter 6317, loss = 0.83461475\n",
            "Iter 6318, loss = 0.94798213\n",
            "Iter 6319, loss = 1.0744817\n",
            "Iter 6320, loss = 1.4133102\n",
            "Iter 6321, loss = 1.3732738\n",
            "Iter 6322, loss = 1.2414652\n",
            "Iter 6323, loss = 0.9272183\n",
            "Iter 6324, loss = 1.1536834\n",
            "Iter 6325, loss = 1.0023546\n",
            "Iter 6326, loss = 1.0707893\n",
            "Iter 6327, loss = 0.8425038\n",
            "Iter 6328, loss = 1.0466118\n",
            "Iter 6329, loss = 0.92236066\n",
            "Iter 6330, loss = 0.8214661\n",
            "Iter 6331, loss = 0.9995234\n",
            "Iter 6332, loss = 1.1602148\n",
            "Iter 6333, loss = 0.82130027\n",
            "Iter 6334, loss = 1.2453258\n",
            "Iter 6335, loss = 1.2930093\n",
            "Iter 6336, loss = 1.3490727\n",
            "Iter 6337, loss = 1.022653\n",
            "Iter 6338, loss = 1.2992976\n",
            "Iter 6339, loss = 1.3666772\n",
            "Iter 6340, loss = 0.977975\n",
            "Iter 6341, loss = 0.8348447\n",
            "Iter 6342, loss = 1.015727\n",
            "Iter 6343, loss = 1.4225899\n",
            "Iter 6344, loss = 0.9656285\n",
            "Iter 6345, loss = 0.9919573\n",
            "Iter 6346, loss = 1.0550954\n",
            "Iter 6347, loss = 1.6132034\n",
            "Iter 6348, loss = 1.2326345\n",
            "Iter 6349, loss = 0.9524601\n",
            "Iter 6350, loss = 0.97383744\n",
            "Iter 6351, loss = 1.2469159\n",
            "Iter 6352, loss = 1.3683167\n",
            "Iter 6353, loss = 1.297744\n",
            "Iter 6354, loss = 1.0398941\n",
            "Iter 6355, loss = 1.090188\n",
            "Iter 6356, loss = 0.7885071\n",
            "Iter 6357, loss = 0.92286\n",
            "Iter 6358, loss = 0.8680645\n",
            "Iter 6359, loss = 0.9452233\n",
            "Iter 6360, loss = 1.9131998\n",
            "Iter 6361, loss = 0.98008597\n",
            "Iter 6362, loss = 1.3012681\n",
            "Iter 6363, loss = 1.1155246\n",
            "Iter 6364, loss = 1.1088656\n",
            "Iter 6365, loss = 0.8157886\n",
            "Iter 6366, loss = 1.2288424\n",
            "Iter 6367, loss = 0.9496605\n",
            "Iter 6368, loss = 0.8148036\n",
            "Iter 6369, loss = 0.8618275\n",
            "Iter 6370, loss = 1.0185328\n",
            "Iter 6371, loss = 0.91607416\n",
            "Iter 6372, loss = 0.84759176\n",
            "Iter 6373, loss = 0.8831408\n",
            "Iter 6374, loss = 1.1227913\n",
            "Iter 6375, loss = 0.94122607\n",
            "Iter 6376, loss = 0.9961442\n",
            "Iter 6377, loss = 1.1195465\n",
            "Iter 6378, loss = 0.8125234\n",
            "Iter 6379, loss = 0.9734545\n",
            "Iter 6380, loss = 1.7378455\n",
            "Iter 6381, loss = 1.024043\n",
            "Iter 6382, loss = 1.2461072\n",
            "Iter 6383, loss = 1.0464014\n",
            "Iter 6384, loss = 1.2672851\n",
            "Iter 6385, loss = 1.170981\n",
            "Iter 6386, loss = 0.9970213\n",
            "Iter 6387, loss = 0.90604806\n",
            "Iter 6388, loss = 1.454004\n",
            "Iter 6389, loss = 1.1317563\n",
            "Iter 6390, loss = 1.010838\n",
            "Iter 6391, loss = 1.1108509\n",
            "Iter 6392, loss = 1.2777396\n",
            "Iter 6393, loss = 1.0035496\n",
            "Iter 6394, loss = 0.8976728\n",
            "Iter 6395, loss = 0.8560765\n",
            "Iter 6396, loss = 0.9614258\n",
            "Iter 6397, loss = 0.66095614\n",
            "Iter 6398, loss = 0.91731834\n",
            "Iter 6399, loss = 1.6887841\n",
            "Iter 6400, loss = 0.96561664\n",
            "Iter 6401, loss = 0.9264901\n",
            "Iter 6402, loss = 1.5056221\n",
            "Iter 6403, loss = 1.2201376\n",
            "Iter 6404, loss = 1.2715877\n",
            "Iter 6405, loss = 1.3380303\n",
            "Iter 6406, loss = 0.9029115\n",
            "Iter 6407, loss = 1.1863228\n",
            "Iter 6408, loss = 0.9320855\n",
            "Iter 6409, loss = 0.9793338\n",
            "Iter 6410, loss = 0.8219437\n",
            "Iter 6411, loss = 0.8110192\n",
            "Iter 6412, loss = 1.0622753\n",
            "Iter 6413, loss = 1.0207034\n",
            "Iter 6414, loss = 1.1523882\n",
            "Iter 6415, loss = 0.9981237\n",
            "Iter 6416, loss = 1.0166041\n",
            "Iter 6417, loss = 1.32703\n",
            "Iter 6418, loss = 1.0613374\n",
            "Iter 6419, loss = 0.9498893\n",
            "Iter 6420, loss = 0.9966577\n",
            "Iter 6421, loss = 1.0313184\n",
            "Iter 6422, loss = 0.7938686\n",
            "Iter 6423, loss = 0.87441623\n",
            "Iter 6424, loss = 1.0924915\n",
            "Iter 6425, loss = 1.0924304\n",
            "Iter 6426, loss = 0.9674497\n",
            "Iter 6427, loss = 0.9485497\n",
            "Iter 6428, loss = 0.92818654\n",
            "Iter 6429, loss = 1.2968986\n",
            "Iter 6430, loss = 0.7460251\n",
            "Iter 6431, loss = 1.3809007\n",
            "Iter 6432, loss = 0.81088746\n",
            "Iter 6433, loss = 2.1021771\n",
            "Iter 6434, loss = 1.0009403\n",
            "Iter 6435, loss = 1.3915215\n",
            "Iter 6436, loss = 0.9415146\n",
            "Iter 6437, loss = 1.266273\n",
            "Iter 6438, loss = 0.8965817\n",
            "Iter 6439, loss = 0.86073756\n",
            "Iter 6440, loss = 0.9382597\n",
            "Iter 6441, loss = 1.3813335\n",
            "Iter 6442, loss = 1.5235908\n",
            "Iter 6443, loss = 1.1795694\n",
            "Iter 6444, loss = 1.2147443\n",
            "Iter 6445, loss = 1.3375928\n",
            "Iter 6446, loss = 1.2084694\n",
            "Iter 6447, loss = 1.0469384\n",
            "Iter 6448, loss = 1.138636\n",
            "Iter 6449, loss = 0.84902465\n",
            "Iter 6450, loss = 1.001771\n",
            "Iter 6451, loss = 1.4253594\n",
            "Iter 6452, loss = 0.7962482\n",
            "Iter 6453, loss = 0.94401336\n",
            "Iter 6454, loss = 1.1286752\n",
            "Iter 6455, loss = 0.81961083\n",
            "Iter 6456, loss = 0.8820579\n",
            "Iter 6457, loss = 1.150552\n",
            "Iter 6458, loss = 1.033586\n",
            "Iter 6459, loss = 1.1512908\n",
            "Iter 6460, loss = 1.0827379\n",
            "Iter 6461, loss = 1.0200149\n",
            "Iter 6462, loss = 1.1922383\n",
            "Iter 6463, loss = 1.180837\n",
            "Iter 6464, loss = 1.6035273\n",
            "Iter 6465, loss = 0.9991587\n",
            "Iter 6466, loss = 0.8083372\n",
            "Iter 6467, loss = 1.2195585\n",
            "Iter 6468, loss = 1.0456977\n",
            "Iter 6469, loss = 1.3481439\n",
            "Iter 6470, loss = 0.921835\n",
            "Iter 6471, loss = 0.97818947\n",
            "Iter 6472, loss = 1.4215689\n",
            "Iter 6473, loss = 1.3783091\n",
            "Iter 6474, loss = 1.4974115\n",
            "Iter 6475, loss = 0.94495916\n",
            "Iter 6476, loss = 1.098446\n",
            "Iter 6477, loss = 1.0901301\n",
            "Iter 6478, loss = 1.0036745\n",
            "Iter 6479, loss = 1.4018624\n",
            "Iter 6480, loss = 0.86673427\n",
            "Iter 6481, loss = 1.1081347\n",
            "Iter 6482, loss = 1.141965\n",
            "Iter 6483, loss = 1.0157044\n",
            "Iter 6484, loss = 0.9787166\n",
            "Iter 6485, loss = 0.95478266\n",
            "Iter 6486, loss = 1.2645242\n",
            "Iter 6487, loss = 1.1754458\n",
            "Iter 6488, loss = 1.0683672\n",
            "Iter 6489, loss = 1.588484\n",
            "Iter 6490, loss = 1.1468194\n",
            "Iter 6491, loss = 1.3257357\n",
            "Iter 6492, loss = 1.1579335\n",
            "Iter 6493, loss = 1.2055447\n",
            "Iter 6494, loss = 1.0353076\n",
            "Iter 6495, loss = 0.91628385\n",
            "Iter 6496, loss = 1.347121\n",
            "Iter 6497, loss = 1.1265447\n",
            "Iter 6498, loss = 1.4850805\n",
            "Iter 6499, loss = 1.0583665\n",
            "\n",
            "Optimization Finished\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr9JtWQ4eycU",
        "colab_type": "text"
      },
      "source": [
        "# Training for word2vec embedding (For Telugu words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-RuSwObYS8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_beng = tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size_beng, embedding_size], -1.0, 1.0))\n",
        "\n",
        "nce_weights_beng = tf.Variable(\n",
        "  tf.truncated_normal([vocabulary_size_beng, embedding_size],\n",
        "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
        "nce_biases_beng = tf.Variable(tf.zeros([vocabulary_size_beng]))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1lqCRIUpQKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_beng = tf.nn.embedding_lookup(embeddings_beng, train_inputs)\n",
        "\n",
        "# Compute the NCE loss, using a sample of the negative labels each time.\n",
        "loss = tf.reduce_mean(\n",
        "  tf.nn.nce_loss(weights=nce_weights_beng,\n",
        "                 biases=nce_biases_beng,\n",
        "                 labels=train_labels,\n",
        "                 inputs=embed_beng,\n",
        "                 num_sampled=10, \n",
        "                 num_classes=vocabulary_size_beng)) #num_sampled = no. of negative samples\n",
        "\n",
        "# We use the SGD optimizer.\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2eWS3oFvs9S",
        "colab_type": "code",
        "outputId": "217ced0e-ecc4-48ec-9f2f-7f8862c553f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    convergence_threshold = 0.5\n",
        "    training_iters = 500*(int((len(embd_inputs_beng))/batch_size))\n",
        "    step=0\n",
        "    n=5\n",
        "    last_n_losses = np.zeros((n),np.float32)\n",
        "    while step<training_iters:\n",
        "        \n",
        "        batch_inputs,batch_labels = generate_batch(embd_inputs_beng,embd_labels_beng,batch_size)\n",
        "        \n",
        "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels.reshape((-1,1))}\n",
        "        _, np_embedding_beng, cur_loss = sess.run([optimizer, embeddings_beng, loss], feed_dict=feed_dict)\n",
        "        \n",
        "        print (\"Iter \"+str(step)+\", loss = \"+str(cur_loss))\n",
        "        last_n_losses[step%n]=cur_loss\n",
        "        if step>=n:\n",
        "            if np.mean(last_n_losses)<=convergence_threshold:\n",
        "                break\n",
        "        step+=1\n",
        "                \n",
        "print (\"\\nOptimization Finished\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, loss = 21.407967\n",
            "Iter 1, loss = 25.387054\n",
            "Iter 2, loss = 22.813337\n",
            "Iter 3, loss = 26.976215\n",
            "Iter 4, loss = 24.917522\n",
            "Iter 5, loss = 25.956388\n",
            "Iter 6, loss = 21.892849\n",
            "Iter 7, loss = 34.35622\n",
            "Iter 8, loss = 19.29571\n",
            "Iter 9, loss = 29.249367\n",
            "Iter 10, loss = 16.431442\n",
            "Iter 11, loss = 20.208668\n",
            "Iter 12, loss = 26.388191\n",
            "Iter 13, loss = 21.162205\n",
            "Iter 14, loss = 19.6852\n",
            "Iter 15, loss = 24.049356\n",
            "Iter 16, loss = 27.024849\n",
            "Iter 17, loss = 18.207684\n",
            "Iter 18, loss = 17.261364\n",
            "Iter 19, loss = 31.72688\n",
            "Iter 20, loss = 13.531954\n",
            "Iter 21, loss = 20.590548\n",
            "Iter 22, loss = 24.633821\n",
            "Iter 23, loss = 16.765854\n",
            "Iter 24, loss = 18.98625\n",
            "Iter 25, loss = 24.05856\n",
            "Iter 26, loss = 29.401443\n",
            "Iter 27, loss = 22.305653\n",
            "Iter 28, loss = 14.865897\n",
            "Iter 29, loss = 18.774445\n",
            "Iter 30, loss = 23.850729\n",
            "Iter 31, loss = 28.589441\n",
            "Iter 32, loss = 16.370083\n",
            "Iter 33, loss = 10.964363\n",
            "Iter 34, loss = 11.008723\n",
            "Iter 35, loss = 22.54179\n",
            "Iter 36, loss = 24.438038\n",
            "Iter 37, loss = 17.963654\n",
            "Iter 38, loss = 18.307785\n",
            "Iter 39, loss = 17.18943\n",
            "Iter 40, loss = 10.881119\n",
            "Iter 41, loss = 17.198347\n",
            "Iter 42, loss = 11.705845\n",
            "Iter 43, loss = 21.248825\n",
            "Iter 44, loss = 25.053772\n",
            "Iter 45, loss = 15.1805725\n",
            "Iter 46, loss = 17.512192\n",
            "Iter 47, loss = 20.33731\n",
            "Iter 48, loss = 14.339167\n",
            "Iter 49, loss = 13.820349\n",
            "Iter 50, loss = 25.835915\n",
            "Iter 51, loss = 17.776697\n",
            "Iter 52, loss = 11.272495\n",
            "Iter 53, loss = 12.958601\n",
            "Iter 54, loss = 17.167755\n",
            "Iter 55, loss = 20.067265\n",
            "Iter 56, loss = 14.317237\n",
            "Iter 57, loss = 15.2883\n",
            "Iter 58, loss = 12.699346\n",
            "Iter 59, loss = 16.296053\n",
            "Iter 60, loss = 5.9562235\n",
            "Iter 61, loss = 12.112214\n",
            "Iter 62, loss = 29.3268\n",
            "Iter 63, loss = 18.714697\n",
            "Iter 64, loss = 16.37249\n",
            "Iter 65, loss = 18.42776\n",
            "Iter 66, loss = 9.5139475\n",
            "Iter 67, loss = 8.772392\n",
            "Iter 68, loss = 13.780529\n",
            "Iter 69, loss = 15.717232\n",
            "Iter 70, loss = 19.942791\n",
            "Iter 71, loss = 10.558838\n",
            "Iter 72, loss = 11.831155\n",
            "Iter 73, loss = 20.467514\n",
            "Iter 74, loss = 13.744181\n",
            "Iter 75, loss = 16.974346\n",
            "Iter 76, loss = 13.1373005\n",
            "Iter 77, loss = 13.853775\n",
            "Iter 78, loss = 7.293389\n",
            "Iter 79, loss = 23.03734\n",
            "Iter 80, loss = 12.261426\n",
            "Iter 81, loss = 12.233469\n",
            "Iter 82, loss = 10.064041\n",
            "Iter 83, loss = 6.816924\n",
            "Iter 84, loss = 12.507081\n",
            "Iter 85, loss = 6.190115\n",
            "Iter 86, loss = 11.98003\n",
            "Iter 87, loss = 13.208277\n",
            "Iter 88, loss = 15.928709\n",
            "Iter 89, loss = 9.892583\n",
            "Iter 90, loss = 10.1850605\n",
            "Iter 91, loss = 8.818165\n",
            "Iter 92, loss = 11.671621\n",
            "Iter 93, loss = 12.268668\n",
            "Iter 94, loss = 6.9434385\n",
            "Iter 95, loss = 5.7986736\n",
            "Iter 96, loss = 10.602221\n",
            "Iter 97, loss = 6.8023243\n",
            "Iter 98, loss = 6.69184\n",
            "Iter 99, loss = 11.637041\n",
            "Iter 100, loss = 7.5147305\n",
            "Iter 101, loss = 6.0340705\n",
            "Iter 102, loss = 13.260582\n",
            "Iter 103, loss = 3.3730152\n",
            "Iter 104, loss = 17.146538\n",
            "Iter 105, loss = 5.5072165\n",
            "Iter 106, loss = 2.866806\n",
            "Iter 107, loss = 7.688192\n",
            "Iter 108, loss = 12.795553\n",
            "Iter 109, loss = 8.594597\n",
            "Iter 110, loss = 13.016834\n",
            "Iter 111, loss = 4.8978176\n",
            "Iter 112, loss = 3.7455106\n",
            "Iter 113, loss = 16.676159\n",
            "Iter 114, loss = 9.062954\n",
            "Iter 115, loss = 8.989557\n",
            "Iter 116, loss = 9.5407295\n",
            "Iter 117, loss = 6.7326202\n",
            "Iter 118, loss = 5.1919603\n",
            "Iter 119, loss = 13.933119\n",
            "Iter 120, loss = 9.601948\n",
            "Iter 121, loss = 4.668419\n",
            "Iter 122, loss = 1.9295149\n",
            "Iter 123, loss = 2.034894\n",
            "Iter 124, loss = 7.131757\n",
            "Iter 125, loss = 4.377925\n",
            "Iter 126, loss = 6.2321796\n",
            "Iter 127, loss = 10.007771\n",
            "Iter 128, loss = 11.5436945\n",
            "Iter 129, loss = 8.099697\n",
            "Iter 130, loss = 9.659601\n",
            "Iter 131, loss = 18.482872\n",
            "Iter 132, loss = 4.2439194\n",
            "Iter 133, loss = 12.281182\n",
            "Iter 134, loss = 4.811248\n",
            "Iter 135, loss = 10.447627\n",
            "Iter 136, loss = 7.46475\n",
            "Iter 137, loss = 7.8952274\n",
            "Iter 138, loss = 4.599833\n",
            "Iter 139, loss = 5.4103136\n",
            "Iter 140, loss = 6.01136\n",
            "Iter 141, loss = 12.359577\n",
            "Iter 142, loss = 14.766954\n",
            "Iter 143, loss = 10.011711\n",
            "Iter 144, loss = 10.990627\n",
            "Iter 145, loss = 7.424634\n",
            "Iter 146, loss = 1.7151783\n",
            "Iter 147, loss = 3.7489476\n",
            "Iter 148, loss = 6.0504384\n",
            "Iter 149, loss = 15.755761\n",
            "Iter 150, loss = 10.709312\n",
            "Iter 151, loss = 13.14285\n",
            "Iter 152, loss = 3.094449\n",
            "Iter 153, loss = 8.931011\n",
            "Iter 154, loss = 3.8352923\n",
            "Iter 155, loss = 3.1033258\n",
            "Iter 156, loss = 3.9824471\n",
            "Iter 157, loss = 8.065946\n",
            "Iter 158, loss = 8.814079\n",
            "Iter 159, loss = 2.3921044\n",
            "Iter 160, loss = 3.0740433\n",
            "Iter 161, loss = 8.763858\n",
            "Iter 162, loss = 2.1579175\n",
            "Iter 163, loss = 2.7680387\n",
            "Iter 164, loss = 4.9348135\n",
            "Iter 165, loss = 9.957918\n",
            "Iter 166, loss = 6.821912\n",
            "Iter 167, loss = 2.6367536\n",
            "Iter 168, loss = 1.3689148\n",
            "Iter 169, loss = 4.264401\n",
            "Iter 170, loss = 2.6631784\n",
            "Iter 171, loss = 6.2949934\n",
            "Iter 172, loss = 8.890482\n",
            "Iter 173, loss = 6.519849\n",
            "Iter 174, loss = 2.4074645\n",
            "Iter 175, loss = 14.3049\n",
            "Iter 176, loss = 2.9453819\n",
            "Iter 177, loss = 6.085084\n",
            "Iter 178, loss = 3.767395\n",
            "Iter 179, loss = 11.255507\n",
            "Iter 180, loss = 2.7050314\n",
            "Iter 181, loss = 8.048473\n",
            "Iter 182, loss = 2.9493165\n",
            "Iter 183, loss = 7.740082\n",
            "Iter 184, loss = 4.732943\n",
            "Iter 185, loss = 2.7988467\n",
            "Iter 186, loss = 9.47541\n",
            "Iter 187, loss = 3.3642523\n",
            "Iter 188, loss = 6.321431\n",
            "Iter 189, loss = 1.828\n",
            "Iter 190, loss = 2.3247118\n",
            "Iter 191, loss = 9.201949\n",
            "Iter 192, loss = 8.363106\n",
            "Iter 193, loss = 3.6656027\n",
            "Iter 194, loss = 3.3581717\n",
            "Iter 195, loss = 3.4712877\n",
            "Iter 196, loss = 9.812187\n",
            "Iter 197, loss = 6.8080463\n",
            "Iter 198, loss = 7.8580446\n",
            "Iter 199, loss = 2.4734046\n",
            "Iter 200, loss = 2.2489326\n",
            "Iter 201, loss = 2.627469\n",
            "Iter 202, loss = 2.987162\n",
            "Iter 203, loss = 1.589545\n",
            "Iter 204, loss = 2.569462\n",
            "Iter 205, loss = 2.3568304\n",
            "Iter 206, loss = 4.8495474\n",
            "Iter 207, loss = 2.9024837\n",
            "Iter 208, loss = 5.1213923\n",
            "Iter 209, loss = 1.4385338\n",
            "Iter 210, loss = 1.2547362\n",
            "Iter 211, loss = 8.668449\n",
            "Iter 212, loss = 9.566981\n",
            "Iter 213, loss = 6.5879545\n",
            "Iter 214, loss = 4.0931625\n",
            "Iter 215, loss = 5.438792\n",
            "Iter 216, loss = 3.1964731\n",
            "Iter 217, loss = 2.4716885\n",
            "Iter 218, loss = 2.486376\n",
            "Iter 219, loss = 3.2657146\n",
            "Iter 220, loss = 3.8647742\n",
            "Iter 221, loss = 8.184169\n",
            "Iter 222, loss = 6.717073\n",
            "Iter 223, loss = 2.7124605\n",
            "Iter 224, loss = 6.556494\n",
            "Iter 225, loss = 6.1885757\n",
            "Iter 226, loss = 3.2875195\n",
            "Iter 227, loss = 4.623337\n",
            "Iter 228, loss = 4.378414\n",
            "Iter 229, loss = 1.7488163\n",
            "Iter 230, loss = 2.001516\n",
            "Iter 231, loss = 2.1981583\n",
            "Iter 232, loss = 1.3591871\n",
            "Iter 233, loss = 4.1315665\n",
            "Iter 234, loss = 5.673992\n",
            "Iter 235, loss = 1.6374753\n",
            "Iter 236, loss = 1.4702435\n",
            "Iter 237, loss = 1.8265837\n",
            "Iter 238, loss = 3.9424713\n",
            "Iter 239, loss = 19.547508\n",
            "Iter 240, loss = 2.3614461\n",
            "Iter 241, loss = 3.3135138\n",
            "Iter 242, loss = 3.1905766\n",
            "Iter 243, loss = 1.5826026\n",
            "Iter 244, loss = 6.919425\n",
            "Iter 245, loss = 1.8970592\n",
            "Iter 246, loss = 5.9974575\n",
            "Iter 247, loss = 1.9040501\n",
            "Iter 248, loss = 1.8358535\n",
            "Iter 249, loss = 6.8274336\n",
            "Iter 250, loss = 4.134926\n",
            "Iter 251, loss = 5.0572925\n",
            "Iter 252, loss = 5.9199924\n",
            "Iter 253, loss = 1.7863811\n",
            "Iter 254, loss = 3.3226075\n",
            "Iter 255, loss = 3.3111653\n",
            "Iter 256, loss = 2.2446785\n",
            "Iter 257, loss = 1.5766765\n",
            "Iter 258, loss = 1.6432414\n",
            "Iter 259, loss = 2.0653088\n",
            "Iter 260, loss = 0.986184\n",
            "Iter 261, loss = 5.9240007\n",
            "Iter 262, loss = 1.5133009\n",
            "Iter 263, loss = 1.7895377\n",
            "Iter 264, loss = 1.4630142\n",
            "Iter 265, loss = 1.4179184\n",
            "Iter 266, loss = 2.0640974\n",
            "Iter 267, loss = 1.4656019\n",
            "Iter 268, loss = 6.0020475\n",
            "Iter 269, loss = 1.603787\n",
            "Iter 270, loss = 3.0126586\n",
            "Iter 271, loss = 1.2576505\n",
            "Iter 272, loss = 3.2661114\n",
            "Iter 273, loss = 1.8082535\n",
            "Iter 274, loss = 4.1290054\n",
            "Iter 275, loss = 1.7345495\n",
            "Iter 276, loss = 1.2407528\n",
            "Iter 277, loss = 1.3477279\n",
            "Iter 278, loss = 3.1319962\n",
            "Iter 279, loss = 7.9985304\n",
            "Iter 280, loss = 3.4030895\n",
            "Iter 281, loss = 6.2919006\n",
            "Iter 282, loss = 4.133164\n",
            "Iter 283, loss = 6.6017485\n",
            "Iter 284, loss = 2.7392278\n",
            "Iter 285, loss = 1.6101563\n",
            "Iter 286, loss = 1.5126762\n",
            "Iter 287, loss = 1.2220128\n",
            "Iter 288, loss = 1.3224324\n",
            "Iter 289, loss = 2.0483084\n",
            "Iter 290, loss = 1.2745491\n",
            "Iter 291, loss = 2.4961858\n",
            "Iter 292, loss = 3.506309\n",
            "Iter 293, loss = 7.5103807\n",
            "Iter 294, loss = 2.4390335\n",
            "Iter 295, loss = 5.993532\n",
            "Iter 296, loss = 2.0050964\n",
            "Iter 297, loss = 2.469944\n",
            "Iter 298, loss = 1.4622673\n",
            "Iter 299, loss = 1.1062107\n",
            "Iter 300, loss = 4.807533\n",
            "Iter 301, loss = 6.366804\n",
            "Iter 302, loss = 1.5047734\n",
            "Iter 303, loss = 1.0438683\n",
            "Iter 304, loss = 1.4002459\n",
            "Iter 305, loss = 1.4749064\n",
            "Iter 306, loss = 1.1468863\n",
            "Iter 307, loss = 2.75273\n",
            "Iter 308, loss = 1.2867703\n",
            "Iter 309, loss = 1.2607038\n",
            "Iter 310, loss = 1.6091745\n",
            "Iter 311, loss = 1.5564225\n",
            "Iter 312, loss = 0.795399\n",
            "Iter 313, loss = 2.9455795\n",
            "Iter 314, loss = 2.0999174\n",
            "Iter 315, loss = 1.4104028\n",
            "Iter 316, loss = 7.108737\n",
            "Iter 317, loss = 2.995202\n",
            "Iter 318, loss = 1.6019188\n",
            "Iter 319, loss = 2.411724\n",
            "Iter 320, loss = 5.3448715\n",
            "Iter 321, loss = 2.3696847\n",
            "Iter 322, loss = 1.1741246\n",
            "Iter 323, loss = 1.1836665\n",
            "Iter 324, loss = 2.4548244\n",
            "Iter 325, loss = 3.9910808\n",
            "Iter 326, loss = 1.5675011\n",
            "Iter 327, loss = 1.6733849\n",
            "Iter 328, loss = 1.4503851\n",
            "Iter 329, loss = 1.1171799\n",
            "Iter 330, loss = 1.039507\n",
            "Iter 331, loss = 4.137497\n",
            "Iter 332, loss = 0.99980134\n",
            "Iter 333, loss = 1.204204\n",
            "Iter 334, loss = 6.8632536\n",
            "Iter 335, loss = 4.0660644\n",
            "Iter 336, loss = 1.4446363\n",
            "Iter 337, loss = 1.0843565\n",
            "Iter 338, loss = 1.573477\n",
            "Iter 339, loss = 1.8056722\n",
            "Iter 340, loss = 0.9484637\n",
            "Iter 341, loss = 9.767225\n",
            "Iter 342, loss = 2.0813007\n",
            "Iter 343, loss = 1.7970902\n",
            "Iter 344, loss = 1.3733759\n",
            "Iter 345, loss = 1.8868823\n",
            "Iter 346, loss = 1.1547102\n",
            "Iter 347, loss = 1.2942648\n",
            "Iter 348, loss = 1.1046448\n",
            "Iter 349, loss = 1.8842089\n",
            "Iter 350, loss = 8.3222065\n",
            "Iter 351, loss = 3.9967766\n",
            "Iter 352, loss = 6.741638\n",
            "Iter 353, loss = 6.3268023\n",
            "Iter 354, loss = 1.3123581\n",
            "Iter 355, loss = 1.1965604\n",
            "Iter 356, loss = 2.1299813\n",
            "Iter 357, loss = 1.1004105\n",
            "Iter 358, loss = 1.4188682\n",
            "Iter 359, loss = 6.143219\n",
            "Iter 360, loss = 1.0970432\n",
            "Iter 361, loss = 1.3602929\n",
            "Iter 362, loss = 1.5673317\n",
            "Iter 363, loss = 1.665785\n",
            "Iter 364, loss = 4.045617\n",
            "Iter 365, loss = 1.3854289\n",
            "Iter 366, loss = 2.2464046\n",
            "Iter 367, loss = 1.0803941\n",
            "Iter 368, loss = 1.8818247\n",
            "Iter 369, loss = 1.3761125\n",
            "Iter 370, loss = 1.0457886\n",
            "Iter 371, loss = 1.2835627\n",
            "Iter 372, loss = 6.848968\n",
            "Iter 373, loss = 0.9860758\n",
            "Iter 374, loss = 2.4498215\n",
            "Iter 375, loss = 6.0059376\n",
            "Iter 376, loss = 2.9435682\n",
            "Iter 377, loss = 1.8042719\n",
            "Iter 378, loss = 2.792647\n",
            "Iter 379, loss = 2.0366762\n",
            "Iter 380, loss = 8.075634\n",
            "Iter 381, loss = 6.613639\n",
            "Iter 382, loss = 1.2892188\n",
            "Iter 383, loss = 1.263654\n",
            "Iter 384, loss = 1.1806082\n",
            "Iter 385, loss = 2.6361227\n",
            "Iter 386, loss = 2.2168882\n",
            "Iter 387, loss = 1.5436444\n",
            "Iter 388, loss = 1.259917\n",
            "Iter 389, loss = 1.0863101\n",
            "Iter 390, loss = 5.0681605\n",
            "Iter 391, loss = 1.183763\n",
            "Iter 392, loss = 0.9593626\n",
            "Iter 393, loss = 1.1935238\n",
            "Iter 394, loss = 1.1474271\n",
            "Iter 395, loss = 0.9684797\n",
            "Iter 396, loss = 1.2222898\n",
            "Iter 397, loss = 1.2208766\n",
            "Iter 398, loss = 1.2055595\n",
            "Iter 399, loss = 1.7548039\n",
            "Iter 400, loss = 1.4751732\n",
            "Iter 401, loss = 2.6476982\n",
            "Iter 402, loss = 0.9044217\n",
            "Iter 403, loss = 1.2455146\n",
            "Iter 404, loss = 7.6230087\n",
            "Iter 405, loss = 6.038142\n",
            "Iter 406, loss = 3.2418394\n",
            "Iter 407, loss = 7.2165422\n",
            "Iter 408, loss = 2.5410128\n",
            "Iter 409, loss = 1.1632861\n",
            "Iter 410, loss = 0.991977\n",
            "Iter 411, loss = 0.9943142\n",
            "Iter 412, loss = 0.92383415\n",
            "Iter 413, loss = 1.3378471\n",
            "Iter 414, loss = 0.79989773\n",
            "Iter 415, loss = 1.1198195\n",
            "Iter 416, loss = 1.4112029\n",
            "Iter 417, loss = 1.2275352\n",
            "Iter 418, loss = 0.9632459\n",
            "Iter 419, loss = 1.1915478\n",
            "Iter 420, loss = 1.1999311\n",
            "Iter 421, loss = 1.6633809\n",
            "Iter 422, loss = 1.7907643\n",
            "Iter 423, loss = 1.3924778\n",
            "Iter 424, loss = 0.9744357\n",
            "Iter 425, loss = 1.282682\n",
            "Iter 426, loss = 1.7273588\n",
            "Iter 427, loss = 1.0692226\n",
            "Iter 428, loss = 1.0856936\n",
            "Iter 429, loss = 0.9981634\n",
            "Iter 430, loss = 1.020489\n",
            "Iter 431, loss = 1.312927\n",
            "Iter 432, loss = 1.0005207\n",
            "Iter 433, loss = 1.0916204\n",
            "Iter 434, loss = 1.6365184\n",
            "Iter 435, loss = 1.1184506\n",
            "Iter 436, loss = 1.9102855\n",
            "Iter 437, loss = 0.7917078\n",
            "Iter 438, loss = 1.5324999\n",
            "Iter 439, loss = 0.99270684\n",
            "Iter 440, loss = 3.517346\n",
            "Iter 441, loss = 1.0021625\n",
            "Iter 442, loss = 1.1155446\n",
            "Iter 443, loss = 1.1157814\n",
            "Iter 444, loss = 1.8438535\n",
            "Iter 445, loss = 1.0602219\n",
            "Iter 446, loss = 1.4283638\n",
            "Iter 447, loss = 1.135681\n",
            "Iter 448, loss = 1.1007384\n",
            "Iter 449, loss = 1.0231467\n",
            "Iter 450, loss = 1.5978231\n",
            "Iter 451, loss = 0.9304844\n",
            "Iter 452, loss = 0.88637745\n",
            "Iter 453, loss = 0.8963449\n",
            "Iter 454, loss = 1.1412214\n",
            "Iter 455, loss = 1.3163419\n",
            "Iter 456, loss = 3.2919974\n",
            "Iter 457, loss = 1.3562198\n",
            "Iter 458, loss = 2.5836668\n",
            "Iter 459, loss = 1.0573896\n",
            "Iter 460, loss = 1.1818697\n",
            "Iter 461, loss = 0.80249166\n",
            "Iter 462, loss = 1.3929143\n",
            "Iter 463, loss = 1.1423739\n",
            "Iter 464, loss = 1.1689457\n",
            "Iter 465, loss = 1.1859398\n",
            "Iter 466, loss = 1.7048724\n",
            "Iter 467, loss = 0.97356206\n",
            "Iter 468, loss = 2.113452\n",
            "Iter 469, loss = 1.1008251\n",
            "Iter 470, loss = 1.0794511\n",
            "Iter 471, loss = 1.151251\n",
            "Iter 472, loss = 0.8620412\n",
            "Iter 473, loss = 0.854241\n",
            "Iter 474, loss = 1.1703436\n",
            "Iter 475, loss = 1.0193996\n",
            "Iter 476, loss = 0.9932209\n",
            "Iter 477, loss = 2.7424257\n",
            "Iter 478, loss = 1.1587955\n",
            "Iter 479, loss = 6.117772\n",
            "Iter 480, loss = 1.6266733\n",
            "Iter 481, loss = 3.1827612\n",
            "Iter 482, loss = 1.0204979\n",
            "Iter 483, loss = 1.6002342\n",
            "Iter 484, loss = 1.0087683\n",
            "Iter 485, loss = 1.499177\n",
            "Iter 486, loss = 1.4458215\n",
            "Iter 487, loss = 1.0233598\n",
            "Iter 488, loss = 1.3757275\n",
            "Iter 489, loss = 0.8546654\n",
            "Iter 490, loss = 0.8717275\n",
            "Iter 491, loss = 1.1763817\n",
            "Iter 492, loss = 1.0486926\n",
            "Iter 493, loss = 1.1149938\n",
            "Iter 494, loss = 1.2137744\n",
            "Iter 495, loss = 1.0447693\n",
            "Iter 496, loss = 2.4453895\n",
            "Iter 497, loss = 1.0125365\n",
            "Iter 498, loss = 1.3087981\n",
            "Iter 499, loss = 0.79623866\n",
            "Iter 500, loss = 0.9448607\n",
            "Iter 501, loss = 0.7622977\n",
            "Iter 502, loss = 1.0428956\n",
            "Iter 503, loss = 1.3546968\n",
            "Iter 504, loss = 1.3889401\n",
            "Iter 505, loss = 1.045409\n",
            "Iter 506, loss = 0.890381\n",
            "Iter 507, loss = 1.1587858\n",
            "Iter 508, loss = 1.0379032\n",
            "Iter 509, loss = 0.9348401\n",
            "Iter 510, loss = 0.9470227\n",
            "Iter 511, loss = 1.2751133\n",
            "Iter 512, loss = 0.82572883\n",
            "Iter 513, loss = 1.7698705\n",
            "Iter 514, loss = 1.2723292\n",
            "Iter 515, loss = 0.66841954\n",
            "Iter 516, loss = 0.79482216\n",
            "Iter 517, loss = 0.76996875\n",
            "Iter 518, loss = 1.7014465\n",
            "Iter 519, loss = 1.2500454\n",
            "Iter 520, loss = 1.3041325\n",
            "Iter 521, loss = 1.0652909\n",
            "Iter 522, loss = 0.86948943\n",
            "Iter 523, loss = 1.2405703\n",
            "Iter 524, loss = 1.5237976\n",
            "Iter 525, loss = 1.0559936\n",
            "Iter 526, loss = 0.8969834\n",
            "Iter 527, loss = 1.0599837\n",
            "Iter 528, loss = 6.2005653\n",
            "Iter 529, loss = 1.2298522\n",
            "Iter 530, loss = 0.9580967\n",
            "Iter 531, loss = 1.2253083\n",
            "Iter 532, loss = 0.93254066\n",
            "Iter 533, loss = 1.0074996\n",
            "Iter 534, loss = 0.95358187\n",
            "Iter 535, loss = 1.2244786\n",
            "Iter 536, loss = 1.5154221\n",
            "Iter 537, loss = 1.0569979\n",
            "Iter 538, loss = 1.8650713\n",
            "Iter 539, loss = 0.9150949\n",
            "Iter 540, loss = 1.4111311\n",
            "Iter 541, loss = 1.2016308\n",
            "Iter 542, loss = 0.9404582\n",
            "Iter 543, loss = 0.7000444\n",
            "Iter 544, loss = 1.3282166\n",
            "Iter 545, loss = 0.8736048\n",
            "Iter 546, loss = 0.9965709\n",
            "Iter 547, loss = 0.8295045\n",
            "Iter 548, loss = 1.0975885\n",
            "Iter 549, loss = 2.8431227\n",
            "Iter 550, loss = 0.9553309\n",
            "Iter 551, loss = 0.98894364\n",
            "Iter 552, loss = 2.8894992\n",
            "Iter 553, loss = 1.1774449\n",
            "Iter 554, loss = 1.304861\n",
            "Iter 555, loss = 1.0331484\n",
            "Iter 556, loss = 0.98278534\n",
            "Iter 557, loss = 0.9240752\n",
            "Iter 558, loss = 1.1408519\n",
            "Iter 559, loss = 0.8753621\n",
            "Iter 560, loss = 0.54011583\n",
            "Iter 561, loss = 1.405779\n",
            "Iter 562, loss = 1.1113949\n",
            "Iter 563, loss = 1.0907952\n",
            "Iter 564, loss = 0.8923936\n",
            "Iter 565, loss = 3.0390973\n",
            "Iter 566, loss = 1.0119226\n",
            "Iter 567, loss = 0.96781754\n",
            "Iter 568, loss = 1.420396\n",
            "Iter 569, loss = 0.8995588\n",
            "Iter 570, loss = 0.9932173\n",
            "Iter 571, loss = 0.9290905\n",
            "Iter 572, loss = 1.0059289\n",
            "Iter 573, loss = 0.9051214\n",
            "Iter 574, loss = 0.9137963\n",
            "Iter 575, loss = 1.0761539\n",
            "Iter 576, loss = 1.0145543\n",
            "Iter 577, loss = 1.007112\n",
            "Iter 578, loss = 1.4525015\n",
            "Iter 579, loss = 1.1536227\n",
            "Iter 580, loss = 1.027176\n",
            "Iter 581, loss = 0.826038\n",
            "Iter 582, loss = 0.86916804\n",
            "Iter 583, loss = 1.2150078\n",
            "Iter 584, loss = 1.09884\n",
            "Iter 585, loss = 1.191679\n",
            "Iter 586, loss = 0.9891124\n",
            "Iter 587, loss = 1.1920652\n",
            "Iter 588, loss = 0.93031895\n",
            "Iter 589, loss = 1.0093023\n",
            "Iter 590, loss = 1.4161696\n",
            "Iter 591, loss = 1.1514962\n",
            "Iter 592, loss = 0.73370236\n",
            "Iter 593, loss = 1.4531758\n",
            "Iter 594, loss = 1.1127931\n",
            "Iter 595, loss = 0.9335279\n",
            "Iter 596, loss = 0.81470394\n",
            "Iter 597, loss = 0.80997676\n",
            "Iter 598, loss = 0.61274564\n",
            "Iter 599, loss = 1.1941241\n",
            "Iter 600, loss = 0.88263416\n",
            "Iter 601, loss = 1.0896478\n",
            "Iter 602, loss = 1.126477\n",
            "Iter 603, loss = 3.019567\n",
            "Iter 604, loss = 0.97429585\n",
            "Iter 605, loss = 6.4306216\n",
            "Iter 606, loss = 0.7574621\n",
            "Iter 607, loss = 1.7168298\n",
            "Iter 608, loss = 1.1827811\n",
            "Iter 609, loss = 1.0597445\n",
            "Iter 610, loss = 1.106585\n",
            "Iter 611, loss = 1.473938\n",
            "Iter 612, loss = 0.8324659\n",
            "Iter 613, loss = 0.98838466\n",
            "Iter 614, loss = 1.0416319\n",
            "Iter 615, loss = 1.0241867\n",
            "Iter 616, loss = 0.8233409\n",
            "Iter 617, loss = 1.0197628\n",
            "Iter 618, loss = 1.0269223\n",
            "Iter 619, loss = 0.90989125\n",
            "Iter 620, loss = 0.86237955\n",
            "Iter 621, loss = 0.91218245\n",
            "Iter 622, loss = 1.1465297\n",
            "Iter 623, loss = 0.7520909\n",
            "Iter 624, loss = 1.0399313\n",
            "Iter 625, loss = 0.8162922\n",
            "Iter 626, loss = 1.0743089\n",
            "Iter 627, loss = 1.0528347\n",
            "Iter 628, loss = 0.91942734\n",
            "Iter 629, loss = 0.64796144\n",
            "Iter 630, loss = 0.835143\n",
            "Iter 631, loss = 0.72685766\n",
            "Iter 632, loss = 0.9924201\n",
            "Iter 633, loss = 0.93072236\n",
            "Iter 634, loss = 1.3841748\n",
            "Iter 635, loss = 0.97465354\n",
            "Iter 636, loss = 1.0730951\n",
            "Iter 637, loss = 1.1285903\n",
            "Iter 638, loss = 0.86997503\n",
            "Iter 639, loss = 1.5713348\n",
            "Iter 640, loss = 0.92161155\n",
            "Iter 641, loss = 0.9916295\n",
            "Iter 642, loss = 1.1727569\n",
            "Iter 643, loss = 1.2275069\n",
            "Iter 644, loss = 0.96241707\n",
            "Iter 645, loss = 1.2072036\n",
            "Iter 646, loss = 0.7554643\n",
            "Iter 647, loss = 1.0390218\n",
            "Iter 648, loss = 0.9906974\n",
            "Iter 649, loss = 0.6811527\n",
            "Iter 650, loss = 1.0366052\n",
            "Iter 651, loss = 0.8013632\n",
            "Iter 652, loss = 1.1611223\n",
            "Iter 653, loss = 0.8972137\n",
            "Iter 654, loss = 0.91542065\n",
            "Iter 655, loss = 0.96905684\n",
            "Iter 656, loss = 1.1994666\n",
            "Iter 657, loss = 1.1354672\n",
            "Iter 658, loss = 1.0208673\n",
            "Iter 659, loss = 0.84724414\n",
            "Iter 660, loss = 0.803601\n",
            "Iter 661, loss = 5.824129\n",
            "Iter 662, loss = 0.80319965\n",
            "Iter 663, loss = 0.6438327\n",
            "Iter 664, loss = 1.3254219\n",
            "Iter 665, loss = 0.9093145\n",
            "Iter 666, loss = 0.93242806\n",
            "Iter 667, loss = 1.2304723\n",
            "Iter 668, loss = 1.1095054\n",
            "Iter 669, loss = 1.027103\n",
            "Iter 670, loss = 0.90742743\n",
            "Iter 671, loss = 1.4590833\n",
            "Iter 672, loss = 0.9861762\n",
            "Iter 673, loss = 0.8390085\n",
            "Iter 674, loss = 0.8894298\n",
            "Iter 675, loss = 0.79321295\n",
            "Iter 676, loss = 1.030263\n",
            "Iter 677, loss = 0.87950826\n",
            "Iter 678, loss = 0.7401603\n",
            "Iter 679, loss = 1.1586794\n",
            "Iter 680, loss = 1.1080105\n",
            "Iter 681, loss = 1.1460062\n",
            "Iter 682, loss = 5.7861137\n",
            "Iter 683, loss = 1.0801103\n",
            "Iter 684, loss = 0.6528661\n",
            "Iter 685, loss = 1.2068758\n",
            "Iter 686, loss = 1.0791963\n",
            "Iter 687, loss = 0.9825368\n",
            "Iter 688, loss = 0.7883837\n",
            "Iter 689, loss = 0.9187571\n",
            "Iter 690, loss = 0.6831749\n",
            "Iter 691, loss = 1.1122664\n",
            "Iter 692, loss = 0.7951699\n",
            "Iter 693, loss = 0.81346\n",
            "Iter 694, loss = 1.2562263\n",
            "Iter 695, loss = 0.91618735\n",
            "Iter 696, loss = 1.4233217\n",
            "Iter 697, loss = 1.1749288\n",
            "Iter 698, loss = 1.0330038\n",
            "Iter 699, loss = 0.8852825\n",
            "Iter 700, loss = 0.71444786\n",
            "Iter 701, loss = 1.1172029\n",
            "Iter 702, loss = 1.43235\n",
            "Iter 703, loss = 1.0149535\n",
            "Iter 704, loss = 0.91926646\n",
            "Iter 705, loss = 1.1735308\n",
            "Iter 706, loss = 1.0996318\n",
            "Iter 707, loss = 1.3845981\n",
            "Iter 708, loss = 0.9739548\n",
            "Iter 709, loss = 0.81320214\n",
            "Iter 710, loss = 0.79621005\n",
            "Iter 711, loss = 1.0182431\n",
            "Iter 712, loss = 1.0216894\n",
            "Iter 713, loss = 1.119599\n",
            "Iter 714, loss = 0.9585492\n",
            "Iter 715, loss = 0.79784393\n",
            "Iter 716, loss = 0.6770641\n",
            "Iter 717, loss = 0.6992515\n",
            "Iter 718, loss = 1.3343387\n",
            "Iter 719, loss = 0.90653825\n",
            "Iter 720, loss = 0.85405856\n",
            "Iter 721, loss = 0.8681907\n",
            "Iter 722, loss = 0.9331453\n",
            "Iter 723, loss = 1.1704899\n",
            "Iter 724, loss = 0.97513235\n",
            "Iter 725, loss = 0.6363002\n",
            "Iter 726, loss = 0.93639463\n",
            "Iter 727, loss = 1.1147045\n",
            "Iter 728, loss = 0.5140816\n",
            "Iter 729, loss = 1.2739877\n",
            "Iter 730, loss = 1.1335227\n",
            "Iter 731, loss = 0.8928596\n",
            "Iter 732, loss = 1.2498552\n",
            "Iter 733, loss = 0.8458227\n",
            "Iter 734, loss = 1.0809493\n",
            "Iter 735, loss = 0.9908338\n",
            "Iter 736, loss = 0.82046247\n",
            "Iter 737, loss = 1.0124309\n",
            "Iter 738, loss = 1.1308783\n",
            "Iter 739, loss = 0.6040821\n",
            "Iter 740, loss = 1.1880597\n",
            "Iter 741, loss = 0.9362557\n",
            "Iter 742, loss = 1.0118424\n",
            "Iter 743, loss = 0.7285222\n",
            "Iter 744, loss = 1.0647118\n",
            "Iter 745, loss = 1.1351831\n",
            "Iter 746, loss = 1.187482\n",
            "Iter 747, loss = 1.1951826\n",
            "Iter 748, loss = 1.0806954\n",
            "Iter 749, loss = 0.8696536\n",
            "Iter 750, loss = 1.0009699\n",
            "Iter 751, loss = 0.8246211\n",
            "Iter 752, loss = 0.79892087\n",
            "Iter 753, loss = 1.0769238\n",
            "Iter 754, loss = 1.1074407\n",
            "Iter 755, loss = 0.88825244\n",
            "Iter 756, loss = 0.8150344\n",
            "Iter 757, loss = 0.79608184\n",
            "Iter 758, loss = 1.0587971\n",
            "Iter 759, loss = 1.1077869\n",
            "Iter 760, loss = 0.73214865\n",
            "Iter 761, loss = 1.0965977\n",
            "Iter 762, loss = 1.0071164\n",
            "Iter 763, loss = 1.1824045\n",
            "Iter 764, loss = 1.15607\n",
            "Iter 765, loss = 0.8047875\n",
            "Iter 766, loss = 1.0850304\n",
            "Iter 767, loss = 0.93108153\n",
            "Iter 768, loss = 0.92649424\n",
            "Iter 769, loss = 0.99998057\n",
            "Iter 770, loss = 0.827762\n",
            "Iter 771, loss = 0.85216224\n",
            "Iter 772, loss = 0.78741646\n",
            "Iter 773, loss = 0.8453491\n",
            "Iter 774, loss = 1.2490922\n",
            "Iter 775, loss = 1.0625232\n",
            "Iter 776, loss = 1.0375408\n",
            "Iter 777, loss = 5.833104\n",
            "Iter 778, loss = 0.75844526\n",
            "Iter 779, loss = 1.0203114\n",
            "Iter 780, loss = 0.87338996\n",
            "Iter 781, loss = 1.2001756\n",
            "Iter 782, loss = 0.9310926\n",
            "Iter 783, loss = 0.8618016\n",
            "Iter 784, loss = 0.9017155\n",
            "Iter 785, loss = 0.8491993\n",
            "Iter 786, loss = 0.8862101\n",
            "Iter 787, loss = 1.0087756\n",
            "Iter 788, loss = 0.97335327\n",
            "Iter 789, loss = 0.8129811\n",
            "Iter 790, loss = 0.73972356\n",
            "Iter 791, loss = 1.4778805\n",
            "Iter 792, loss = 0.9193341\n",
            "Iter 793, loss = 0.87361825\n",
            "Iter 794, loss = 0.6672816\n",
            "Iter 795, loss = 1.1410375\n",
            "Iter 796, loss = 0.79935735\n",
            "Iter 797, loss = 0.8639953\n",
            "Iter 798, loss = 3.6313572\n",
            "Iter 799, loss = 1.154257\n",
            "Iter 800, loss = 1.1949439\n",
            "Iter 801, loss = 0.81787646\n",
            "Iter 802, loss = 1.1531225\n",
            "Iter 803, loss = 0.86206377\n",
            "Iter 804, loss = 1.2527328\n",
            "Iter 805, loss = 0.96825755\n",
            "Iter 806, loss = 0.97014123\n",
            "Iter 807, loss = 0.94830716\n",
            "Iter 808, loss = 0.69577396\n",
            "Iter 809, loss = 0.9720948\n",
            "Iter 810, loss = 0.94638103\n",
            "Iter 811, loss = 0.90329456\n",
            "Iter 812, loss = 0.8307085\n",
            "Iter 813, loss = 1.2090125\n",
            "Iter 814, loss = 1.1075277\n",
            "Iter 815, loss = 0.9972465\n",
            "Iter 816, loss = 0.94655466\n",
            "Iter 817, loss = 0.6625403\n",
            "Iter 818, loss = 0.67873025\n",
            "Iter 819, loss = 0.9232077\n",
            "Iter 820, loss = 1.1611543\n",
            "Iter 821, loss = 0.7671087\n",
            "Iter 822, loss = 1.0445428\n",
            "Iter 823, loss = 0.7588365\n",
            "Iter 824, loss = 0.9007177\n",
            "Iter 825, loss = 1.2408053\n",
            "Iter 826, loss = 1.1051958\n",
            "Iter 827, loss = 0.9419907\n",
            "Iter 828, loss = 0.89032936\n",
            "Iter 829, loss = 0.87690103\n",
            "Iter 830, loss = 0.7494578\n",
            "Iter 831, loss = 0.8036044\n",
            "Iter 832, loss = 1.0545385\n",
            "Iter 833, loss = 0.73036027\n",
            "Iter 834, loss = 0.78429496\n",
            "Iter 835, loss = 1.3307287\n",
            "Iter 836, loss = 1.1222413\n",
            "Iter 837, loss = 0.82945013\n",
            "Iter 838, loss = 0.7898083\n",
            "Iter 839, loss = 0.8645275\n",
            "Iter 840, loss = 0.85802054\n",
            "Iter 841, loss = 1.5033066\n",
            "Iter 842, loss = 1.1905241\n",
            "Iter 843, loss = 0.9561757\n",
            "Iter 844, loss = 5.9640446\n",
            "Iter 845, loss = 1.0300729\n",
            "Iter 846, loss = 0.9948864\n",
            "Iter 847, loss = 0.8572539\n",
            "Iter 848, loss = 1.2228804\n",
            "Iter 849, loss = 0.97549766\n",
            "Iter 850, loss = 1.018606\n",
            "Iter 851, loss = 0.9190381\n",
            "Iter 852, loss = 0.90111727\n",
            "Iter 853, loss = 0.94977885\n",
            "Iter 854, loss = 0.7930269\n",
            "Iter 855, loss = 0.9403217\n",
            "Iter 856, loss = 0.8325633\n",
            "Iter 857, loss = 0.88151395\n",
            "Iter 858, loss = 1.1876534\n",
            "Iter 859, loss = 1.3580952\n",
            "Iter 860, loss = 1.1377323\n",
            "Iter 861, loss = 1.0603282\n",
            "Iter 862, loss = 1.250654\n",
            "Iter 863, loss = 0.98849803\n",
            "Iter 864, loss = 0.98153996\n",
            "Iter 865, loss = 0.98239386\n",
            "Iter 866, loss = 0.98800033\n",
            "Iter 867, loss = 0.9733793\n",
            "Iter 868, loss = 0.92953324\n",
            "Iter 869, loss = 0.8937973\n",
            "Iter 870, loss = 0.6091044\n",
            "Iter 871, loss = 0.9449253\n",
            "Iter 872, loss = 1.177541\n",
            "Iter 873, loss = 0.8272338\n",
            "Iter 874, loss = 1.0354648\n",
            "Iter 875, loss = 0.8916885\n",
            "Iter 876, loss = 0.914222\n",
            "Iter 877, loss = 0.6814954\n",
            "Iter 878, loss = 0.98873705\n",
            "Iter 879, loss = 1.0081171\n",
            "Iter 880, loss = 1.0625546\n",
            "Iter 881, loss = 0.99396914\n",
            "Iter 882, loss = 0.954315\n",
            "Iter 883, loss = 0.8146833\n",
            "Iter 884, loss = 0.90803075\n",
            "Iter 885, loss = 0.7863709\n",
            "Iter 886, loss = 1.0329547\n",
            "Iter 887, loss = 0.9512919\n",
            "Iter 888, loss = 0.73475754\n",
            "Iter 889, loss = 1.0890931\n",
            "Iter 890, loss = 0.98898447\n",
            "Iter 891, loss = 0.45083028\n",
            "Iter 892, loss = 1.2015207\n",
            "Iter 893, loss = 1.1352046\n",
            "Iter 894, loss = 0.7141618\n",
            "Iter 895, loss = 0.9585395\n",
            "Iter 896, loss = 1.2320745\n",
            "Iter 897, loss = 0.92957675\n",
            "Iter 898, loss = 0.94240963\n",
            "Iter 899, loss = 5.939956\n",
            "Iter 900, loss = 1.0400352\n",
            "Iter 901, loss = 1.0762768\n",
            "Iter 902, loss = 1.3255789\n",
            "Iter 903, loss = 0.8576474\n",
            "Iter 904, loss = 1.1141073\n",
            "Iter 905, loss = 0.99819314\n",
            "Iter 906, loss = 0.97040254\n",
            "Iter 907, loss = 1.0115472\n",
            "Iter 908, loss = 0.8702457\n",
            "Iter 909, loss = 0.89855397\n",
            "Iter 910, loss = 1.0677196\n",
            "Iter 911, loss = 0.93212783\n",
            "Iter 912, loss = 0.9228581\n",
            "Iter 913, loss = 0.8382775\n",
            "Iter 914, loss = 0.9052609\n",
            "Iter 915, loss = 0.78577286\n",
            "Iter 916, loss = 1.054605\n",
            "Iter 917, loss = 0.80028427\n",
            "Iter 918, loss = 0.97993433\n",
            "Iter 919, loss = 1.034255\n",
            "Iter 920, loss = 0.9247685\n",
            "Iter 921, loss = 1.1869516\n",
            "Iter 922, loss = 1.080931\n",
            "Iter 923, loss = 0.8297398\n",
            "Iter 924, loss = 0.7159374\n",
            "Iter 925, loss = 0.540793\n",
            "Iter 926, loss = 1.4380814\n",
            "Iter 927, loss = 0.9998224\n",
            "Iter 928, loss = 0.6868875\n",
            "Iter 929, loss = 1.0346576\n",
            "Iter 930, loss = 1.0354909\n",
            "Iter 931, loss = 0.74174845\n",
            "Iter 932, loss = 1.1169338\n",
            "Iter 933, loss = 1.0079024\n",
            "Iter 934, loss = 0.799978\n",
            "Iter 935, loss = 1.1879433\n",
            "Iter 936, loss = 0.77489996\n",
            "Iter 937, loss = 0.9443875\n",
            "Iter 938, loss = 1.0028642\n",
            "Iter 939, loss = 0.87580264\n",
            "Iter 940, loss = 0.7332793\n",
            "Iter 941, loss = 0.86366737\n",
            "Iter 942, loss = 0.98925257\n",
            "Iter 943, loss = 0.7725663\n",
            "Iter 944, loss = 0.8962818\n",
            "Iter 945, loss = 0.6992881\n",
            "Iter 946, loss = 0.9762243\n",
            "Iter 947, loss = 0.8514408\n",
            "Iter 948, loss = 1.2395331\n",
            "Iter 949, loss = 1.0206612\n",
            "Iter 950, loss = 0.8510573\n",
            "Iter 951, loss = 0.7710185\n",
            "Iter 952, loss = 0.92694926\n",
            "Iter 953, loss = 0.7838736\n",
            "Iter 954, loss = 0.64028853\n",
            "Iter 955, loss = 1.2302307\n",
            "Iter 956, loss = 0.7039008\n",
            "Iter 957, loss = 0.9731375\n",
            "Iter 958, loss = 1.0619335\n",
            "Iter 959, loss = 0.97367924\n",
            "Iter 960, loss = 0.7172606\n",
            "Iter 961, loss = 0.8210038\n",
            "Iter 962, loss = 1.1065931\n",
            "Iter 963, loss = 0.60433364\n",
            "Iter 964, loss = 0.90260565\n",
            "Iter 965, loss = 0.90248936\n",
            "Iter 966, loss = 1.0199115\n",
            "Iter 967, loss = 0.82328343\n",
            "Iter 968, loss = 0.822221\n",
            "Iter 969, loss = 0.838069\n",
            "Iter 970, loss = 1.3562522\n",
            "Iter 971, loss = 0.74613416\n",
            "Iter 972, loss = 0.924463\n",
            "Iter 973, loss = 0.71316874\n",
            "Iter 974, loss = 1.007081\n",
            "Iter 975, loss = 1.022958\n",
            "Iter 976, loss = 0.8291765\n",
            "Iter 977, loss = 0.8211603\n",
            "Iter 978, loss = 0.9990089\n",
            "Iter 979, loss = 0.85136294\n",
            "Iter 980, loss = 0.9208555\n",
            "Iter 981, loss = 0.88795006\n",
            "Iter 982, loss = 1.3874094\n",
            "Iter 983, loss = 0.80857086\n",
            "Iter 984, loss = 0.93565905\n",
            "Iter 985, loss = 0.9073436\n",
            "Iter 986, loss = 0.89464307\n",
            "Iter 987, loss = 0.9204451\n",
            "Iter 988, loss = 0.70777845\n",
            "Iter 989, loss = 1.0974529\n",
            "Iter 990, loss = 1.0581062\n",
            "Iter 991, loss = 1.1005816\n",
            "Iter 992, loss = 0.8304699\n",
            "Iter 993, loss = 0.95000714\n",
            "Iter 994, loss = 0.8484595\n",
            "Iter 995, loss = 1.0160329\n",
            "Iter 996, loss = 0.6986706\n",
            "Iter 997, loss = 1.2497795\n",
            "Iter 998, loss = 1.1772496\n",
            "Iter 999, loss = 0.8164408\n",
            "Iter 1000, loss = 0.9473516\n",
            "Iter 1001, loss = 0.61131245\n",
            "Iter 1002, loss = 0.7952451\n",
            "Iter 1003, loss = 0.8816161\n",
            "Iter 1004, loss = 1.0086603\n",
            "Iter 1005, loss = 0.816729\n",
            "Iter 1006, loss = 0.777679\n",
            "Iter 1007, loss = 0.8645283\n",
            "Iter 1008, loss = 1.0993087\n",
            "Iter 1009, loss = 0.6305686\n",
            "Iter 1010, loss = 0.8941269\n",
            "Iter 1011, loss = 1.2036347\n",
            "Iter 1012, loss = 0.9702027\n",
            "Iter 1013, loss = 1.1649009\n",
            "Iter 1014, loss = 0.89514613\n",
            "Iter 1015, loss = 1.6389897\n",
            "Iter 1016, loss = 0.74428046\n",
            "Iter 1017, loss = 1.0636628\n",
            "Iter 1018, loss = 0.754745\n",
            "Iter 1019, loss = 0.9005761\n",
            "Iter 1020, loss = 0.9336287\n",
            "Iter 1021, loss = 0.9219805\n",
            "Iter 1022, loss = 0.906435\n",
            "Iter 1023, loss = 0.99475193\n",
            "Iter 1024, loss = 0.6993806\n",
            "Iter 1025, loss = 0.7567074\n",
            "Iter 1026, loss = 0.9004694\n",
            "Iter 1027, loss = 0.8008803\n",
            "Iter 1028, loss = 1.0875529\n",
            "Iter 1029, loss = 0.9499938\n",
            "Iter 1030, loss = 0.94741976\n",
            "Iter 1031, loss = 1.0744107\n",
            "Iter 1032, loss = 1.0622481\n",
            "Iter 1033, loss = 0.52279246\n",
            "Iter 1034, loss = 1.1364272\n",
            "Iter 1035, loss = 0.75089896\n",
            "Iter 1036, loss = 0.7553858\n",
            "Iter 1037, loss = 0.75863194\n",
            "Iter 1038, loss = 1.0743628\n",
            "Iter 1039, loss = 0.98681104\n",
            "Iter 1040, loss = 0.9467598\n",
            "Iter 1041, loss = 0.8990222\n",
            "Iter 1042, loss = 0.73523164\n",
            "Iter 1043, loss = 0.7917194\n",
            "Iter 1044, loss = 1.1613429\n",
            "Iter 1045, loss = 1.0104299\n",
            "Iter 1046, loss = 1.0980327\n",
            "Iter 1047, loss = 0.87595534\n",
            "Iter 1048, loss = 0.9229091\n",
            "Iter 1049, loss = 0.6292809\n",
            "Iter 1050, loss = 0.976691\n",
            "Iter 1051, loss = 1.0075393\n",
            "Iter 1052, loss = 0.8860462\n",
            "Iter 1053, loss = 0.80695605\n",
            "Iter 1054, loss = 1.0920968\n",
            "Iter 1055, loss = 0.8599708\n",
            "Iter 1056, loss = 0.91850847\n",
            "Iter 1057, loss = 0.84173805\n",
            "Iter 1058, loss = 0.84221995\n",
            "Iter 1059, loss = 0.8404324\n",
            "Iter 1060, loss = 0.93722165\n",
            "Iter 1061, loss = 1.0245197\n",
            "Iter 1062, loss = 0.9288223\n",
            "Iter 1063, loss = 0.8134904\n",
            "Iter 1064, loss = 1.125948\n",
            "Iter 1065, loss = 0.95380056\n",
            "Iter 1066, loss = 0.9085734\n",
            "Iter 1067, loss = 0.97358227\n",
            "Iter 1068, loss = 0.97392225\n",
            "Iter 1069, loss = 0.8708032\n",
            "Iter 1070, loss = 0.9054898\n",
            "Iter 1071, loss = 0.91413957\n",
            "Iter 1072, loss = 0.93466985\n",
            "Iter 1073, loss = 0.59659505\n",
            "Iter 1074, loss = 1.3127961\n",
            "Iter 1075, loss = 0.8195915\n",
            "Iter 1076, loss = 1.0688477\n",
            "Iter 1077, loss = 1.1733572\n",
            "Iter 1078, loss = 1.0186484\n",
            "Iter 1079, loss = 0.9386356\n",
            "Iter 1080, loss = 0.76207393\n",
            "Iter 1081, loss = 0.9894906\n",
            "Iter 1082, loss = 1.0288844\n",
            "Iter 1083, loss = 0.7796159\n",
            "Iter 1084, loss = 0.9245723\n",
            "Iter 1085, loss = 0.9030508\n",
            "Iter 1086, loss = 0.8342209\n",
            "Iter 1087, loss = 1.0578394\n",
            "Iter 1088, loss = 0.85464144\n",
            "Iter 1089, loss = 0.83274263\n",
            "Iter 1090, loss = 1.1210635\n",
            "Iter 1091, loss = 0.70119345\n",
            "Iter 1092, loss = 0.7984878\n",
            "Iter 1093, loss = 1.420125\n",
            "Iter 1094, loss = 0.9245404\n",
            "Iter 1095, loss = 0.7684486\n",
            "Iter 1096, loss = 0.73769283\n",
            "Iter 1097, loss = 0.97566354\n",
            "Iter 1098, loss = 1.031983\n",
            "Iter 1099, loss = 0.7712706\n",
            "Iter 1100, loss = 1.0553722\n",
            "Iter 1101, loss = 0.9353773\n",
            "Iter 1102, loss = 0.9473755\n",
            "Iter 1103, loss = 0.81047016\n",
            "Iter 1104, loss = 0.766946\n",
            "Iter 1105, loss = 0.8691287\n",
            "Iter 1106, loss = 1.0279114\n",
            "Iter 1107, loss = 0.7173085\n",
            "Iter 1108, loss = 0.9919776\n",
            "Iter 1109, loss = 0.970943\n",
            "Iter 1110, loss = 0.988865\n",
            "Iter 1111, loss = 0.88800323\n",
            "Iter 1112, loss = 0.89059114\n",
            "Iter 1113, loss = 1.039324\n",
            "Iter 1114, loss = 0.68205416\n",
            "Iter 1115, loss = 0.6799964\n",
            "Iter 1116, loss = 0.8393842\n",
            "Iter 1117, loss = 0.97539324\n",
            "Iter 1118, loss = 0.78122157\n",
            "Iter 1119, loss = 0.7995831\n",
            "Iter 1120, loss = 0.9454523\n",
            "Iter 1121, loss = 0.91714\n",
            "Iter 1122, loss = 0.87026346\n",
            "Iter 1123, loss = 1.0022647\n",
            "Iter 1124, loss = 0.90914524\n",
            "Iter 1125, loss = 1.261909\n",
            "Iter 1126, loss = 0.84782\n",
            "Iter 1127, loss = 0.81393987\n",
            "Iter 1128, loss = 0.719555\n",
            "Iter 1129, loss = 0.9374264\n",
            "Iter 1130, loss = 0.8948612\n",
            "Iter 1131, loss = 1.0164654\n",
            "Iter 1132, loss = 0.78933185\n",
            "Iter 1133, loss = 0.7390411\n",
            "Iter 1134, loss = 1.0750053\n",
            "Iter 1135, loss = 0.938153\n",
            "Iter 1136, loss = 1.1250545\n",
            "Iter 1137, loss = 0.7884107\n",
            "Iter 1138, loss = 1.1926775\n",
            "Iter 1139, loss = 0.8146416\n",
            "Iter 1140, loss = 0.63266075\n",
            "Iter 1141, loss = 0.64830244\n",
            "Iter 1142, loss = 1.0598413\n",
            "Iter 1143, loss = 0.9472183\n",
            "Iter 1144, loss = 0.9498159\n",
            "Iter 1145, loss = 1.0405164\n",
            "Iter 1146, loss = 0.55477196\n",
            "Iter 1147, loss = 1.3551975\n",
            "Iter 1148, loss = 0.8748175\n",
            "Iter 1149, loss = 1.1810138\n",
            "Iter 1150, loss = 0.7836905\n",
            "Iter 1151, loss = 0.71434665\n",
            "Iter 1152, loss = 0.904055\n",
            "Iter 1153, loss = 0.7258305\n",
            "Iter 1154, loss = 1.2828461\n",
            "Iter 1155, loss = 0.63244855\n",
            "Iter 1156, loss = 0.8941714\n",
            "Iter 1157, loss = 0.89897096\n",
            "Iter 1158, loss = 0.9328371\n",
            "Iter 1159, loss = 0.95142335\n",
            "Iter 1160, loss = 0.8070664\n",
            "Iter 1161, loss = 0.87654805\n",
            "Iter 1162, loss = 1.2272646\n",
            "Iter 1163, loss = 0.7761912\n",
            "Iter 1164, loss = 0.6687538\n",
            "Iter 1165, loss = 0.9579021\n",
            "Iter 1166, loss = 1.1008133\n",
            "Iter 1167, loss = 0.92982936\n",
            "Iter 1168, loss = 0.96019256\n",
            "Iter 1169, loss = 0.81716317\n",
            "Iter 1170, loss = 0.8887042\n",
            "Iter 1171, loss = 0.80937433\n",
            "Iter 1172, loss = 1.0828228\n",
            "Iter 1173, loss = 0.9100679\n",
            "Iter 1174, loss = 0.6025628\n",
            "Iter 1175, loss = 0.57496065\n",
            "Iter 1176, loss = 1.4013523\n",
            "Iter 1177, loss = 0.8024144\n",
            "Iter 1178, loss = 0.8163702\n",
            "Iter 1179, loss = 1.2046313\n",
            "Iter 1180, loss = 0.863569\n",
            "Iter 1181, loss = 0.814991\n",
            "Iter 1182, loss = 0.68827\n",
            "Iter 1183, loss = 1.0964125\n",
            "Iter 1184, loss = 0.79629827\n",
            "Iter 1185, loss = 0.5051117\n",
            "Iter 1186, loss = 0.74780077\n",
            "Iter 1187, loss = 1.0508976\n",
            "Iter 1188, loss = 0.92740464\n",
            "Iter 1189, loss = 0.5924001\n",
            "Iter 1190, loss = 1.1816996\n",
            "Iter 1191, loss = 0.76058483\n",
            "Iter 1192, loss = 0.90063566\n",
            "Iter 1193, loss = 0.8283367\n",
            "Iter 1194, loss = 0.9835333\n",
            "Iter 1195, loss = 0.859915\n",
            "Iter 1196, loss = 0.8348428\n",
            "Iter 1197, loss = 0.9485655\n",
            "Iter 1198, loss = 0.63783103\n",
            "Iter 1199, loss = 1.2421294\n",
            "Iter 1200, loss = 0.8950032\n",
            "Iter 1201, loss = 0.8275465\n",
            "Iter 1202, loss = 0.89649385\n",
            "Iter 1203, loss = 0.94830084\n",
            "Iter 1204, loss = 0.87067294\n",
            "Iter 1205, loss = 0.8054397\n",
            "Iter 1206, loss = 0.6002722\n",
            "Iter 1207, loss = 0.87485194\n",
            "Iter 1208, loss = 0.7545776\n",
            "Iter 1209, loss = 1.355654\n",
            "Iter 1210, loss = 0.9639705\n",
            "Iter 1211, loss = 0.9233116\n",
            "Iter 1212, loss = 0.8679416\n",
            "Iter 1213, loss = 0.7786497\n",
            "Iter 1214, loss = 0.9388817\n",
            "Iter 1215, loss = 0.73705906\n",
            "Iter 1216, loss = 0.75834715\n",
            "Iter 1217, loss = 0.9317416\n",
            "Iter 1218, loss = 0.53263867\n",
            "Iter 1219, loss = 0.82953036\n",
            "Iter 1220, loss = 0.82644707\n",
            "Iter 1221, loss = 0.75656474\n",
            "Iter 1222, loss = 0.84808934\n",
            "Iter 1223, loss = 0.77764153\n",
            "Iter 1224, loss = 0.8602314\n",
            "Iter 1225, loss = 0.76645315\n",
            "Iter 1226, loss = 0.92936814\n",
            "Iter 1227, loss = 0.8144038\n",
            "Iter 1228, loss = 0.9875585\n",
            "Iter 1229, loss = 1.1337694\n",
            "Iter 1230, loss = 0.7040966\n",
            "Iter 1231, loss = 0.8474952\n",
            "Iter 1232, loss = 0.89775634\n",
            "Iter 1233, loss = 0.80228394\n",
            "Iter 1234, loss = 0.98346543\n",
            "Iter 1235, loss = 0.61771995\n",
            "Iter 1236, loss = 0.7786936\n",
            "Iter 1237, loss = 1.000121\n",
            "Iter 1238, loss = 1.1167532\n",
            "Iter 1239, loss = 0.90076256\n",
            "Iter 1240, loss = 0.8803826\n",
            "Iter 1241, loss = 1.0414531\n",
            "Iter 1242, loss = 0.82717454\n",
            "Iter 1243, loss = 0.7961916\n",
            "Iter 1244, loss = 0.78646636\n",
            "Iter 1245, loss = 0.7023952\n",
            "Iter 1246, loss = 0.8659885\n",
            "Iter 1247, loss = 0.89095366\n",
            "Iter 1248, loss = 0.92751133\n",
            "Iter 1249, loss = 0.5664866\n",
            "Iter 1250, loss = 0.8465376\n",
            "Iter 1251, loss = 0.96746266\n",
            "Iter 1252, loss = 0.80490613\n",
            "Iter 1253, loss = 0.73845994\n",
            "Iter 1254, loss = 0.8415719\n",
            "Iter 1255, loss = 0.73174095\n",
            "Iter 1256, loss = 1.0336075\n",
            "Iter 1257, loss = 1.0497763\n",
            "Iter 1258, loss = 0.9836004\n",
            "Iter 1259, loss = 0.74792427\n",
            "Iter 1260, loss = 0.9099201\n",
            "Iter 1261, loss = 0.75225455\n",
            "Iter 1262, loss = 0.8812001\n",
            "Iter 1263, loss = 0.82080734\n",
            "Iter 1264, loss = 1.1561728\n",
            "Iter 1265, loss = 0.59576255\n",
            "Iter 1266, loss = 0.7421062\n",
            "Iter 1267, loss = 0.91388714\n",
            "Iter 1268, loss = 0.754848\n",
            "Iter 1269, loss = 0.7113019\n",
            "Iter 1270, loss = 0.9134122\n",
            "Iter 1271, loss = 0.881593\n",
            "Iter 1272, loss = 1.1421534\n",
            "Iter 1273, loss = 0.93028206\n",
            "Iter 1274, loss = 0.5867727\n",
            "Iter 1275, loss = 1.3410301\n",
            "Iter 1276, loss = 0.85033923\n",
            "Iter 1277, loss = 0.90456903\n",
            "Iter 1278, loss = 1.3284762\n",
            "Iter 1279, loss = 0.90404475\n",
            "Iter 1280, loss = 0.9171004\n",
            "Iter 1281, loss = 0.83992696\n",
            "Iter 1282, loss = 0.90261114\n",
            "Iter 1283, loss = 0.96135676\n",
            "Iter 1284, loss = 0.9119396\n",
            "Iter 1285, loss = 0.5577892\n",
            "Iter 1286, loss = 0.59767586\n",
            "Iter 1287, loss = 1.0147156\n",
            "Iter 1288, loss = 0.81386185\n",
            "Iter 1289, loss = 1.1694162\n",
            "Iter 1290, loss = 0.850531\n",
            "Iter 1291, loss = 1.0083271\n",
            "Iter 1292, loss = 1.0718096\n",
            "Iter 1293, loss = 0.8582456\n",
            "Iter 1294, loss = 0.95425165\n",
            "Iter 1295, loss = 1.1064551\n",
            "Iter 1296, loss = 0.726652\n",
            "Iter 1297, loss = 0.80664164\n",
            "Iter 1298, loss = 0.64513016\n",
            "Iter 1299, loss = 0.8715692\n",
            "Iter 1300, loss = 1.2102267\n",
            "Iter 1301, loss = 0.7521047\n",
            "Iter 1302, loss = 1.1820428\n",
            "Iter 1303, loss = 0.77500975\n",
            "Iter 1304, loss = 1.2992632\n",
            "Iter 1305, loss = 0.7486698\n",
            "Iter 1306, loss = 0.961334\n",
            "Iter 1307, loss = 0.80217767\n",
            "Iter 1308, loss = 0.81017125\n",
            "Iter 1309, loss = 0.8479501\n",
            "Iter 1310, loss = 0.76937973\n",
            "Iter 1311, loss = 0.8028108\n",
            "Iter 1312, loss = 0.41557732\n",
            "Iter 1313, loss = 0.67061186\n",
            "Iter 1314, loss = 1.2214742\n",
            "Iter 1315, loss = 1.2571255\n",
            "Iter 1316, loss = 0.8040242\n",
            "Iter 1317, loss = 1.0706991\n",
            "Iter 1318, loss = 0.88027644\n",
            "Iter 1319, loss = 1.1264237\n",
            "Iter 1320, loss = 0.92668724\n",
            "Iter 1321, loss = 0.7874426\n",
            "Iter 1322, loss = 0.5124198\n",
            "Iter 1323, loss = 0.7312079\n",
            "Iter 1324, loss = 0.562244\n",
            "Iter 1325, loss = 0.80684745\n",
            "Iter 1326, loss = 0.63750684\n",
            "Iter 1327, loss = 0.76071894\n",
            "Iter 1328, loss = 1.0473647\n",
            "Iter 1329, loss = 0.82265675\n",
            "Iter 1330, loss = 1.0134115\n",
            "Iter 1331, loss = 0.6161318\n",
            "Iter 1332, loss = 0.98451245\n",
            "Iter 1333, loss = 0.73020005\n",
            "Iter 1334, loss = 0.87667894\n",
            "Iter 1335, loss = 1.1264819\n",
            "Iter 1336, loss = 0.7312411\n",
            "Iter 1337, loss = 0.59313923\n",
            "Iter 1338, loss = 1.3200645\n",
            "Iter 1339, loss = 1.213276\n",
            "Iter 1340, loss = 0.7750002\n",
            "Iter 1341, loss = 0.81393456\n",
            "Iter 1342, loss = 0.9482517\n",
            "Iter 1343, loss = 0.750561\n",
            "Iter 1344, loss = 1.1865244\n",
            "Iter 1345, loss = 0.6286121\n",
            "Iter 1346, loss = 0.94587743\n",
            "Iter 1347, loss = 0.56026417\n",
            "Iter 1348, loss = 0.57434475\n",
            "Iter 1349, loss = 0.60143393\n",
            "Iter 1350, loss = 1.0636098\n",
            "Iter 1351, loss = 0.93294555\n",
            "Iter 1352, loss = 1.1930077\n",
            "Iter 1353, loss = 0.7527586\n",
            "Iter 1354, loss = 0.69889\n",
            "Iter 1355, loss = 1.0388258\n",
            "Iter 1356, loss = 0.9449279\n",
            "Iter 1357, loss = 0.7594624\n",
            "Iter 1358, loss = 0.8564545\n",
            "Iter 1359, loss = 1.1536627\n",
            "Iter 1360, loss = 0.7071567\n",
            "Iter 1361, loss = 0.6513488\n",
            "Iter 1362, loss = 0.95095426\n",
            "Iter 1363, loss = 0.8828685\n",
            "Iter 1364, loss = 1.0704486\n",
            "Iter 1365, loss = 1.0278397\n",
            "Iter 1366, loss = 0.8155733\n",
            "Iter 1367, loss = 0.8096551\n",
            "Iter 1368, loss = 0.8429393\n",
            "Iter 1369, loss = 1.0981252\n",
            "Iter 1370, loss = 0.9957117\n",
            "Iter 1371, loss = 0.61163497\n",
            "Iter 1372, loss = 1.0968571\n",
            "Iter 1373, loss = 0.7120816\n",
            "Iter 1374, loss = 1.0421003\n",
            "Iter 1375, loss = 0.7364404\n",
            "Iter 1376, loss = 1.0493727\n",
            "Iter 1377, loss = 0.7848191\n",
            "Iter 1378, loss = 0.9298208\n",
            "Iter 1379, loss = 0.80038095\n",
            "Iter 1380, loss = 0.7397826\n",
            "Iter 1381, loss = 0.7074392\n",
            "Iter 1382, loss = 1.0683005\n",
            "Iter 1383, loss = 0.8789506\n",
            "Iter 1384, loss = 0.9454582\n",
            "Iter 1385, loss = 1.0129838\n",
            "Iter 1386, loss = 0.51489\n",
            "Iter 1387, loss = 1.0379924\n",
            "Iter 1388, loss = 0.95861053\n",
            "Iter 1389, loss = 0.83677566\n",
            "Iter 1390, loss = 0.8604461\n",
            "Iter 1391, loss = 1.0079781\n",
            "Iter 1392, loss = 1.3128594\n",
            "Iter 1393, loss = 1.2275047\n",
            "Iter 1394, loss = 0.80275923\n",
            "Iter 1395, loss = 0.60184133\n",
            "Iter 1396, loss = 1.1311803\n",
            "Iter 1397, loss = 0.8049484\n",
            "Iter 1398, loss = 0.6286403\n",
            "Iter 1399, loss = 0.8510778\n",
            "Iter 1400, loss = 0.9822591\n",
            "Iter 1401, loss = 1.2098436\n",
            "Iter 1402, loss = 0.89630437\n",
            "Iter 1403, loss = 1.0112627\n",
            "Iter 1404, loss = 0.943567\n",
            "Iter 1405, loss = 0.8957138\n",
            "Iter 1406, loss = 0.80207103\n",
            "Iter 1407, loss = 0.7508819\n",
            "Iter 1408, loss = 0.7834867\n",
            "Iter 1409, loss = 0.68286186\n",
            "Iter 1410, loss = 0.66902673\n",
            "Iter 1411, loss = 1.1322984\n",
            "Iter 1412, loss = 0.9023752\n",
            "Iter 1413, loss = 0.89634633\n",
            "Iter 1414, loss = 0.8017012\n",
            "Iter 1415, loss = 0.68943286\n",
            "Iter 1416, loss = 1.0318282\n",
            "Iter 1417, loss = 0.9585215\n",
            "Iter 1418, loss = 0.86094975\n",
            "Iter 1419, loss = 0.76401067\n",
            "Iter 1420, loss = 1.1255598\n",
            "Iter 1421, loss = 0.9024968\n",
            "Iter 1422, loss = 0.9542629\n",
            "Iter 1423, loss = 0.71543634\n",
            "Iter 1424, loss = 0.7395284\n",
            "Iter 1425, loss = 0.80181056\n",
            "Iter 1426, loss = 0.9154521\n",
            "Iter 1427, loss = 0.784937\n",
            "Iter 1428, loss = 0.71758723\n",
            "Iter 1429, loss = 0.84421843\n",
            "Iter 1430, loss = 0.7207349\n",
            "Iter 1431, loss = 0.97447485\n",
            "Iter 1432, loss = 0.56294507\n",
            "Iter 1433, loss = 1.1139605\n",
            "Iter 1434, loss = 0.87696207\n",
            "Iter 1435, loss = 0.76882875\n",
            "Iter 1436, loss = 0.89488256\n",
            "Iter 1437, loss = 0.93499935\n",
            "Iter 1438, loss = 0.78091615\n",
            "Iter 1439, loss = 1.0555395\n",
            "Iter 1440, loss = 0.7595161\n",
            "Iter 1441, loss = 1.0820436\n",
            "Iter 1442, loss = 1.0725642\n",
            "Iter 1443, loss = 1.1154536\n",
            "Iter 1444, loss = 0.8972385\n",
            "Iter 1445, loss = 0.74792784\n",
            "Iter 1446, loss = 0.62872124\n",
            "Iter 1447, loss = 1.3179057\n",
            "Iter 1448, loss = 0.7810049\n",
            "Iter 1449, loss = 1.1608772\n",
            "Iter 1450, loss = 0.7015302\n",
            "Iter 1451, loss = 0.78579426\n",
            "Iter 1452, loss = 0.5715205\n",
            "Iter 1453, loss = 1.1966476\n",
            "Iter 1454, loss = 1.1138415\n",
            "Iter 1455, loss = 1.0005699\n",
            "Iter 1456, loss = 1.0597465\n",
            "Iter 1457, loss = 0.9083499\n",
            "Iter 1458, loss = 0.71741897\n",
            "Iter 1459, loss = 0.7307569\n",
            "Iter 1460, loss = 1.3832471\n",
            "Iter 1461, loss = 0.78466535\n",
            "Iter 1462, loss = 0.9478659\n",
            "Iter 1463, loss = 0.7860434\n",
            "Iter 1464, loss = 0.9861948\n",
            "Iter 1465, loss = 0.6577927\n",
            "Iter 1466, loss = 0.9447185\n",
            "Iter 1467, loss = 0.90557355\n",
            "Iter 1468, loss = 0.6868184\n",
            "Iter 1469, loss = 1.0519804\n",
            "Iter 1470, loss = 0.89292645\n",
            "Iter 1471, loss = 0.86906767\n",
            "Iter 1472, loss = 1.1366887\n",
            "Iter 1473, loss = 0.7839527\n",
            "Iter 1474, loss = 1.0413036\n",
            "Iter 1475, loss = 0.7594017\n",
            "Iter 1476, loss = 0.8870849\n",
            "Iter 1477, loss = 0.8307462\n",
            "Iter 1478, loss = 0.6345995\n",
            "Iter 1479, loss = 0.84841883\n",
            "Iter 1480, loss = 0.7189143\n",
            "Iter 1481, loss = 0.61599165\n",
            "Iter 1482, loss = 0.6718931\n",
            "Iter 1483, loss = 1.0601066\n",
            "Iter 1484, loss = 0.8192674\n",
            "Iter 1485, loss = 0.8259984\n",
            "Iter 1486, loss = 0.7251504\n",
            "Iter 1487, loss = 0.86339355\n",
            "Iter 1488, loss = 0.8985564\n",
            "Iter 1489, loss = 0.7137027\n",
            "Iter 1490, loss = 0.75835896\n",
            "Iter 1491, loss = 0.7394699\n",
            "Iter 1492, loss = 0.8477379\n",
            "Iter 1493, loss = 0.88259494\n",
            "Iter 1494, loss = 1.0616033\n",
            "Iter 1495, loss = 0.6840695\n",
            "Iter 1496, loss = 0.93685097\n",
            "Iter 1497, loss = 0.63546634\n",
            "Iter 1498, loss = 0.882556\n",
            "Iter 1499, loss = 1.0327175\n",
            "Iter 1500, loss = 0.9562358\n",
            "Iter 1501, loss = 0.85050845\n",
            "Iter 1502, loss = 1.1187196\n",
            "Iter 1503, loss = 0.6804561\n",
            "Iter 1504, loss = 0.7505451\n",
            "Iter 1505, loss = 1.1383669\n",
            "Iter 1506, loss = 0.95998085\n",
            "Iter 1507, loss = 0.779265\n",
            "Iter 1508, loss = 0.69645214\n",
            "Iter 1509, loss = 1.0628109\n",
            "Iter 1510, loss = 0.7846073\n",
            "Iter 1511, loss = 1.1620286\n",
            "Iter 1512, loss = 1.0622501\n",
            "Iter 1513, loss = 0.944739\n",
            "Iter 1514, loss = 1.0619719\n",
            "Iter 1515, loss = 1.1276824\n",
            "Iter 1516, loss = 0.8087735\n",
            "Iter 1517, loss = 0.8195714\n",
            "Iter 1518, loss = 0.89353496\n",
            "Iter 1519, loss = 1.077534\n",
            "Iter 1520, loss = 0.96981055\n",
            "Iter 1521, loss = 0.80676514\n",
            "Iter 1522, loss = 0.7193228\n",
            "Iter 1523, loss = 0.58145696\n",
            "Iter 1524, loss = 1.4224389\n",
            "Iter 1525, loss = 0.77600193\n",
            "Iter 1526, loss = 0.87167466\n",
            "Iter 1527, loss = 0.9298614\n",
            "Iter 1528, loss = 1.0736299\n",
            "Iter 1529, loss = 0.822977\n",
            "Iter 1530, loss = 0.9559406\n",
            "Iter 1531, loss = 1.2330381\n",
            "Iter 1532, loss = 0.9514122\n",
            "Iter 1533, loss = 0.8217083\n",
            "Iter 1534, loss = 1.040699\n",
            "Iter 1535, loss = 0.7974483\n",
            "Iter 1536, loss = 0.6926021\n",
            "Iter 1537, loss = 0.9398264\n",
            "Iter 1538, loss = 0.9747364\n",
            "Iter 1539, loss = 0.85122424\n",
            "Iter 1540, loss = 0.8150637\n",
            "Iter 1541, loss = 0.99515456\n",
            "Iter 1542, loss = 0.8963023\n",
            "Iter 1543, loss = 0.76845706\n",
            "Iter 1544, loss = 0.876496\n",
            "Iter 1545, loss = 0.7515228\n",
            "Iter 1546, loss = 1.0159228\n",
            "Iter 1547, loss = 0.9296644\n",
            "Iter 1548, loss = 0.53253406\n",
            "Iter 1549, loss = 0.49109685\n",
            "Iter 1550, loss = 1.0330303\n",
            "Iter 1551, loss = 0.79049087\n",
            "Iter 1552, loss = 0.9713577\n",
            "Iter 1553, loss = 0.9183396\n",
            "Iter 1554, loss = 0.848616\n",
            "Iter 1555, loss = 0.814311\n",
            "Iter 1556, loss = 0.89856297\n",
            "Iter 1557, loss = 0.7933191\n",
            "Iter 1558, loss = 0.85272616\n",
            "Iter 1559, loss = 0.86225235\n",
            "Iter 1560, loss = 0.67195994\n",
            "Iter 1561, loss = 0.9379151\n",
            "Iter 1562, loss = 0.8518372\n",
            "Iter 1563, loss = 1.1546011\n",
            "Iter 1564, loss = 0.6475282\n",
            "Iter 1565, loss = 0.8898544\n",
            "Iter 1566, loss = 0.6413747\n",
            "Iter 1567, loss = 0.7202014\n",
            "Iter 1568, loss = 0.8674861\n",
            "Iter 1569, loss = 1.0751151\n",
            "Iter 1570, loss = 0.8818486\n",
            "Iter 1571, loss = 0.87622094\n",
            "Iter 1572, loss = 1.1244547\n",
            "Iter 1573, loss = 0.63484687\n",
            "Iter 1574, loss = 1.1178207\n",
            "Iter 1575, loss = 0.7654725\n",
            "Iter 1576, loss = 0.6898925\n",
            "Iter 1577, loss = 0.9899826\n",
            "Iter 1578, loss = 0.6740449\n",
            "Iter 1579, loss = 0.69057524\n",
            "Iter 1580, loss = 0.7536362\n",
            "Iter 1581, loss = 0.575868\n",
            "Iter 1582, loss = 1.156569\n",
            "Iter 1583, loss = 0.787449\n",
            "Iter 1584, loss = 0.7864195\n",
            "Iter 1585, loss = 0.94435954\n",
            "Iter 1586, loss = 0.77738273\n",
            "Iter 1587, loss = 0.907735\n",
            "Iter 1588, loss = 1.0741999\n",
            "Iter 1589, loss = 0.8263664\n",
            "Iter 1590, loss = 0.7633349\n",
            "Iter 1591, loss = 0.7397927\n",
            "Iter 1592, loss = 0.6273119\n",
            "Iter 1593, loss = 1.0791373\n",
            "Iter 1594, loss = 0.7331067\n",
            "Iter 1595, loss = 0.69536144\n",
            "Iter 1596, loss = 0.7878037\n",
            "Iter 1597, loss = 0.850143\n",
            "Iter 1598, loss = 0.96471566\n",
            "Iter 1599, loss = 0.7661507\n",
            "Iter 1600, loss = 0.88806665\n",
            "Iter 1601, loss = 1.0632153\n",
            "Iter 1602, loss = 0.8036121\n",
            "Iter 1603, loss = 0.75500107\n",
            "Iter 1604, loss = 0.84459037\n",
            "Iter 1605, loss = 0.750648\n",
            "Iter 1606, loss = 0.69454527\n",
            "Iter 1607, loss = 0.84993285\n",
            "Iter 1608, loss = 0.7434579\n",
            "Iter 1609, loss = 1.0901309\n",
            "Iter 1610, loss = 0.9235608\n",
            "Iter 1611, loss = 0.8653325\n",
            "Iter 1612, loss = 0.6577773\n",
            "Iter 1613, loss = 0.93861026\n",
            "Iter 1614, loss = 0.58471155\n",
            "Iter 1615, loss = 0.9952173\n",
            "Iter 1616, loss = 0.7147877\n",
            "Iter 1617, loss = 0.8043411\n",
            "Iter 1618, loss = 0.95029795\n",
            "Iter 1619, loss = 0.77256286\n",
            "Iter 1620, loss = 0.7821039\n",
            "Iter 1621, loss = 0.7708806\n",
            "Iter 1622, loss = 0.6229993\n",
            "Iter 1623, loss = 0.98285604\n",
            "Iter 1624, loss = 0.97678095\n",
            "Iter 1625, loss = 0.81410176\n",
            "Iter 1626, loss = 0.9602337\n",
            "Iter 1627, loss = 0.77931166\n",
            "Iter 1628, loss = 0.9560012\n",
            "Iter 1629, loss = 1.007088\n",
            "Iter 1630, loss = 0.8419989\n",
            "Iter 1631, loss = 0.8869978\n",
            "Iter 1632, loss = 0.6648861\n",
            "Iter 1633, loss = 1.0404892\n",
            "Iter 1634, loss = 0.7246897\n",
            "Iter 1635, loss = 1.1266882\n",
            "Iter 1636, loss = 0.9273757\n",
            "Iter 1637, loss = 0.8901276\n",
            "Iter 1638, loss = 0.7332616\n",
            "Iter 1639, loss = 0.8459902\n",
            "Iter 1640, loss = 0.8187984\n",
            "Iter 1641, loss = 0.8907359\n",
            "Iter 1642, loss = 0.931427\n",
            "Iter 1643, loss = 0.7396496\n",
            "Iter 1644, loss = 1.5196902\n",
            "Iter 1645, loss = 0.58170015\n",
            "Iter 1646, loss = 0.43473047\n",
            "Iter 1647, loss = 1.2716987\n",
            "Iter 1648, loss = 0.628995\n",
            "Iter 1649, loss = 0.7770305\n",
            "Iter 1650, loss = 0.72696906\n",
            "Iter 1651, loss = 1.093223\n",
            "Iter 1652, loss = 1.2372559\n",
            "Iter 1653, loss = 0.8964\n",
            "Iter 1654, loss = 0.6757035\n",
            "Iter 1655, loss = 1.0336692\n",
            "Iter 1656, loss = 0.76858234\n",
            "Iter 1657, loss = 0.7079491\n",
            "Iter 1658, loss = 0.7791333\n",
            "Iter 1659, loss = 0.61427677\n",
            "Iter 1660, loss = 0.92431927\n",
            "Iter 1661, loss = 0.9054373\n",
            "Iter 1662, loss = 0.77519095\n",
            "Iter 1663, loss = 1.0392687\n",
            "Iter 1664, loss = 0.78280425\n",
            "Iter 1665, loss = 0.67839336\n",
            "Iter 1666, loss = 0.85866535\n",
            "Iter 1667, loss = 0.8536036\n",
            "Iter 1668, loss = 0.7638775\n",
            "Iter 1669, loss = 0.88803816\n",
            "Iter 1670, loss = 0.83268625\n",
            "Iter 1671, loss = 1.309251\n",
            "Iter 1672, loss = 0.9108943\n",
            "Iter 1673, loss = 0.6670698\n",
            "Iter 1674, loss = 0.9390496\n",
            "Iter 1675, loss = 0.67480254\n",
            "Iter 1676, loss = 0.92599833\n",
            "Iter 1677, loss = 0.73847306\n",
            "Iter 1678, loss = 0.8780969\n",
            "Iter 1679, loss = 0.67910796\n",
            "Iter 1680, loss = 1.2215708\n",
            "Iter 1681, loss = 0.8749136\n",
            "Iter 1682, loss = 0.7924643\n",
            "Iter 1683, loss = 0.8457717\n",
            "Iter 1684, loss = 0.91502\n",
            "Iter 1685, loss = 0.8752891\n",
            "Iter 1686, loss = 0.9225557\n",
            "Iter 1687, loss = 0.7381613\n",
            "Iter 1688, loss = 1.0505581\n",
            "Iter 1689, loss = 0.8042139\n",
            "Iter 1690, loss = 0.6374812\n",
            "Iter 1691, loss = 0.94035435\n",
            "Iter 1692, loss = 0.6880697\n",
            "Iter 1693, loss = 0.7223708\n",
            "Iter 1694, loss = 0.72770965\n",
            "Iter 1695, loss = 0.8652414\n",
            "Iter 1696, loss = 0.87493765\n",
            "Iter 1697, loss = 0.8545608\n",
            "Iter 1698, loss = 0.7552583\n",
            "Iter 1699, loss = 0.91229904\n",
            "Iter 1700, loss = 0.9360677\n",
            "Iter 1701, loss = 1.0777146\n",
            "Iter 1702, loss = 0.8008575\n",
            "Iter 1703, loss = 0.85293543\n",
            "Iter 1704, loss = 0.78725857\n",
            "Iter 1705, loss = 1.2381171\n",
            "Iter 1706, loss = 1.181843\n",
            "Iter 1707, loss = 0.83440554\n",
            "Iter 1708, loss = 0.880586\n",
            "Iter 1709, loss = 1.1078587\n",
            "Iter 1710, loss = 0.63521063\n",
            "Iter 1711, loss = 0.94673145\n",
            "Iter 1712, loss = 0.80399144\n",
            "Iter 1713, loss = 0.70343155\n",
            "Iter 1714, loss = 0.7497742\n",
            "Iter 1715, loss = 1.0042893\n",
            "Iter 1716, loss = 0.8045035\n",
            "Iter 1717, loss = 0.83345115\n",
            "Iter 1718, loss = 0.91790503\n",
            "Iter 1719, loss = 0.74452704\n",
            "Iter 1720, loss = 0.761721\n",
            "Iter 1721, loss = 1.2354583\n",
            "Iter 1722, loss = 0.79875875\n",
            "Iter 1723, loss = 0.8013314\n",
            "Iter 1724, loss = 0.74372\n",
            "Iter 1725, loss = 0.6618931\n",
            "Iter 1726, loss = 1.0997319\n",
            "Iter 1727, loss = 0.8644364\n",
            "Iter 1728, loss = 0.7335025\n",
            "Iter 1729, loss = 0.6866102\n",
            "Iter 1730, loss = 1.1199788\n",
            "Iter 1731, loss = 0.88460016\n",
            "Iter 1732, loss = 0.7552647\n",
            "Iter 1733, loss = 0.7028586\n",
            "Iter 1734, loss = 0.9728856\n",
            "Iter 1735, loss = 0.8188566\n",
            "Iter 1736, loss = 0.71759677\n",
            "Iter 1737, loss = 0.92317915\n",
            "Iter 1738, loss = 0.69181967\n",
            "Iter 1739, loss = 0.9129746\n",
            "Iter 1740, loss = 0.75846326\n",
            "Iter 1741, loss = 0.6529399\n",
            "Iter 1742, loss = 0.9954895\n",
            "Iter 1743, loss = 1.0650578\n",
            "Iter 1744, loss = 0.70368236\n",
            "Iter 1745, loss = 0.80847883\n",
            "Iter 1746, loss = 0.99535066\n",
            "Iter 1747, loss = 0.6685384\n",
            "Iter 1748, loss = 1.0384014\n",
            "Iter 1749, loss = 0.72099245\n",
            "Iter 1750, loss = 0.9855808\n",
            "Iter 1751, loss = 0.8822272\n",
            "Iter 1752, loss = 0.89692795\n",
            "Iter 1753, loss = 0.9478204\n",
            "Iter 1754, loss = 1.0526412\n",
            "Iter 1755, loss = 1.1318908\n",
            "Iter 1756, loss = 0.84256077\n",
            "Iter 1757, loss = 0.7245494\n",
            "Iter 1758, loss = 0.912344\n",
            "Iter 1759, loss = 0.95930934\n",
            "Iter 1760, loss = 0.7250311\n",
            "Iter 1761, loss = 0.7277095\n",
            "Iter 1762, loss = 0.7614217\n",
            "Iter 1763, loss = 0.6788709\n",
            "Iter 1764, loss = 0.93679017\n",
            "Iter 1765, loss = 0.8962864\n",
            "Iter 1766, loss = 0.88518775\n",
            "Iter 1767, loss = 1.0867894\n",
            "Iter 1768, loss = 0.9773809\n",
            "Iter 1769, loss = 0.90520966\n",
            "Iter 1770, loss = 0.65297204\n",
            "Iter 1771, loss = 0.76614\n",
            "Iter 1772, loss = 0.7172134\n",
            "Iter 1773, loss = 0.9293684\n",
            "Iter 1774, loss = 0.82258755\n",
            "Iter 1775, loss = 0.75140184\n",
            "Iter 1776, loss = 0.8275329\n",
            "Iter 1777, loss = 0.8399304\n",
            "Iter 1778, loss = 0.6278733\n",
            "Iter 1779, loss = 0.9412158\n",
            "Iter 1780, loss = 1.0260935\n",
            "Iter 1781, loss = 0.703721\n",
            "Iter 1782, loss = 0.6159947\n",
            "Iter 1783, loss = 0.71604335\n",
            "Iter 1784, loss = 0.6815829\n",
            "Iter 1785, loss = 1.172065\n",
            "Iter 1786, loss = 0.59744716\n",
            "Iter 1787, loss = 0.99688745\n",
            "Iter 1788, loss = 0.63559836\n",
            "Iter 1789, loss = 0.9027214\n",
            "Iter 1790, loss = 0.84821725\n",
            "Iter 1791, loss = 1.1095626\n",
            "Iter 1792, loss = 0.84451663\n",
            "Iter 1793, loss = 0.9844765\n",
            "Iter 1794, loss = 0.85279787\n",
            "Iter 1795, loss = 1.0561703\n",
            "Iter 1796, loss = 0.8902224\n",
            "Iter 1797, loss = 1.0656424\n",
            "Iter 1798, loss = 0.8536115\n",
            "Iter 1799, loss = 0.7985875\n",
            "Iter 1800, loss = 1.1591358\n",
            "Iter 1801, loss = 0.85020936\n",
            "Iter 1802, loss = 0.5285107\n",
            "Iter 1803, loss = 0.9661325\n",
            "Iter 1804, loss = 1.0012153\n",
            "Iter 1805, loss = 1.0279863\n",
            "Iter 1806, loss = 0.9201662\n",
            "Iter 1807, loss = 0.7794863\n",
            "Iter 1808, loss = 0.87516123\n",
            "Iter 1809, loss = 1.0507728\n",
            "Iter 1810, loss = 0.86418337\n",
            "Iter 1811, loss = 1.0156918\n",
            "Iter 1812, loss = 0.95540786\n",
            "Iter 1813, loss = 0.59675956\n",
            "Iter 1814, loss = 0.71820927\n",
            "Iter 1815, loss = 0.8727387\n",
            "Iter 1816, loss = 1.086494\n",
            "Iter 1817, loss = 0.7899611\n",
            "Iter 1818, loss = 0.7108048\n",
            "Iter 1819, loss = 1.1364715\n",
            "Iter 1820, loss = 0.78817266\n",
            "Iter 1821, loss = 1.0569007\n",
            "Iter 1822, loss = 0.78542185\n",
            "Iter 1823, loss = 0.7671994\n",
            "Iter 1824, loss = 0.97514886\n",
            "Iter 1825, loss = 0.7362466\n",
            "Iter 1826, loss = 1.1208131\n",
            "Iter 1827, loss = 0.7165556\n",
            "Iter 1828, loss = 0.585274\n",
            "Iter 1829, loss = 0.7941083\n",
            "Iter 1830, loss = 0.9554542\n",
            "Iter 1831, loss = 0.8759619\n",
            "Iter 1832, loss = 0.92869437\n",
            "Iter 1833, loss = 0.51142746\n",
            "Iter 1834, loss = 1.0332522\n",
            "Iter 1835, loss = 0.8188311\n",
            "Iter 1836, loss = 0.7610547\n",
            "Iter 1837, loss = 0.68185675\n",
            "Iter 1838, loss = 0.7822665\n",
            "Iter 1839, loss = 1.2891381\n",
            "Iter 1840, loss = 0.9528854\n",
            "Iter 1841, loss = 0.8561817\n",
            "Iter 1842, loss = 0.8632046\n",
            "Iter 1843, loss = 1.1024376\n",
            "Iter 1844, loss = 0.7033387\n",
            "Iter 1845, loss = 0.69444275\n",
            "Iter 1846, loss = 0.9517123\n",
            "Iter 1847, loss = 1.0200498\n",
            "Iter 1848, loss = 0.88691604\n",
            "Iter 1849, loss = 0.69224185\n",
            "Iter 1850, loss = 0.7126931\n",
            "Iter 1851, loss = 1.0300841\n",
            "Iter 1852, loss = 0.79555595\n",
            "Iter 1853, loss = 0.81040466\n",
            "Iter 1854, loss = 1.3061037\n",
            "Iter 1855, loss = 0.4799512\n",
            "Iter 1856, loss = 0.8527651\n",
            "Iter 1857, loss = 1.0169294\n",
            "Iter 1858, loss = 0.63469195\n",
            "Iter 1859, loss = 0.9848399\n",
            "Iter 1860, loss = 0.72359556\n",
            "Iter 1861, loss = 0.7227392\n",
            "Iter 1862, loss = 0.9819131\n",
            "Iter 1863, loss = 0.8227195\n",
            "Iter 1864, loss = 0.89517665\n",
            "Iter 1865, loss = 1.1064978\n",
            "Iter 1866, loss = 0.8983189\n",
            "Iter 1867, loss = 0.5662701\n",
            "Iter 1868, loss = 1.0044159\n",
            "Iter 1869, loss = 0.8415117\n",
            "Iter 1870, loss = 0.7239551\n",
            "Iter 1871, loss = 0.8590487\n",
            "Iter 1872, loss = 0.741742\n",
            "Iter 1873, loss = 1.0038373\n",
            "Iter 1874, loss = 0.9674084\n",
            "Iter 1875, loss = 0.5332235\n",
            "Iter 1876, loss = 1.1224015\n",
            "Iter 1877, loss = 0.7589532\n",
            "Iter 1878, loss = 0.6822697\n",
            "Iter 1879, loss = 0.8593533\n",
            "Iter 1880, loss = 0.9894833\n",
            "Iter 1881, loss = 0.71406645\n",
            "Iter 1882, loss = 0.8454162\n",
            "Iter 1883, loss = 1.3245397\n",
            "Iter 1884, loss = 0.6491559\n",
            "Iter 1885, loss = 0.7834547\n",
            "Iter 1886, loss = 0.9517528\n",
            "Iter 1887, loss = 0.9578445\n",
            "Iter 1888, loss = 0.67532426\n",
            "Iter 1889, loss = 0.84922093\n",
            "Iter 1890, loss = 0.86713755\n",
            "Iter 1891, loss = 0.9636589\n",
            "Iter 1892, loss = 0.74258184\n",
            "Iter 1893, loss = 0.8278806\n",
            "Iter 1894, loss = 0.80644584\n",
            "Iter 1895, loss = 0.69716704\n",
            "Iter 1896, loss = 0.8068284\n",
            "Iter 1897, loss = 0.8838593\n",
            "Iter 1898, loss = 1.1387815\n",
            "Iter 1899, loss = 0.9592281\n",
            "Iter 1900, loss = 0.69007623\n",
            "Iter 1901, loss = 0.9602077\n",
            "Iter 1902, loss = 0.7937599\n",
            "Iter 1903, loss = 0.9013368\n",
            "Iter 1904, loss = 0.8929068\n",
            "Iter 1905, loss = 0.993042\n",
            "Iter 1906, loss = 0.78398204\n",
            "Iter 1907, loss = 0.79595286\n",
            "Iter 1908, loss = 0.65365636\n",
            "Iter 1909, loss = 0.93983674\n",
            "Iter 1910, loss = 0.9805697\n",
            "Iter 1911, loss = 0.84769917\n",
            "Iter 1912, loss = 0.81114924\n",
            "Iter 1913, loss = 0.6081754\n",
            "Iter 1914, loss = 0.9881749\n",
            "Iter 1915, loss = 0.61090964\n",
            "Iter 1916, loss = 1.1252383\n",
            "Iter 1917, loss = 0.797732\n",
            "Iter 1918, loss = 0.8541161\n",
            "Iter 1919, loss = 0.8416376\n",
            "Iter 1920, loss = 0.886365\n",
            "Iter 1921, loss = 0.6581863\n",
            "Iter 1922, loss = 0.8261122\n",
            "Iter 1923, loss = 0.94149417\n",
            "Iter 1924, loss = 0.83403856\n",
            "Iter 1925, loss = 0.69618726\n",
            "Iter 1926, loss = 0.62625545\n",
            "Iter 1927, loss = 1.0222497\n",
            "Iter 1928, loss = 1.1878006\n",
            "Iter 1929, loss = 0.8474467\n",
            "Iter 1930, loss = 0.9198118\n",
            "Iter 1931, loss = 1.3103282\n",
            "Iter 1932, loss = 0.95999044\n",
            "Iter 1933, loss = 0.6947454\n",
            "Iter 1934, loss = 0.82283175\n",
            "Iter 1935, loss = 0.8461189\n",
            "Iter 1936, loss = 0.77997965\n",
            "Iter 1937, loss = 1.0606489\n",
            "Iter 1938, loss = 0.85478055\n",
            "Iter 1939, loss = 1.0430112\n",
            "Iter 1940, loss = 1.0856886\n",
            "Iter 1941, loss = 1.1205369\n",
            "Iter 1942, loss = 0.64349616\n",
            "Iter 1943, loss = 1.0800643\n",
            "Iter 1944, loss = 0.92074525\n",
            "Iter 1945, loss = 0.74204624\n",
            "Iter 1946, loss = 0.76981825\n",
            "Iter 1947, loss = 0.7532364\n",
            "Iter 1948, loss = 0.81123185\n",
            "Iter 1949, loss = 1.0445678\n",
            "Iter 1950, loss = 0.69895756\n",
            "Iter 1951, loss = 0.83894897\n",
            "Iter 1952, loss = 0.7586036\n",
            "Iter 1953, loss = 0.5946801\n",
            "Iter 1954, loss = 0.8162656\n",
            "Iter 1955, loss = 0.75335056\n",
            "Iter 1956, loss = 0.86973596\n",
            "Iter 1957, loss = 1.0263228\n",
            "Iter 1958, loss = 0.7806259\n",
            "Iter 1959, loss = 0.90686125\n",
            "Iter 1960, loss = 0.7533411\n",
            "Iter 1961, loss = 0.7376367\n",
            "Iter 1962, loss = 0.90063906\n",
            "Iter 1963, loss = 1.1020584\n",
            "Iter 1964, loss = 0.70079154\n",
            "Iter 1965, loss = 1.0139279\n",
            "Iter 1966, loss = 0.8646828\n",
            "Iter 1967, loss = 0.78286505\n",
            "Iter 1968, loss = 0.8855919\n",
            "Iter 1969, loss = 0.74806195\n",
            "Iter 1970, loss = 0.7988217\n",
            "Iter 1971, loss = 0.92883754\n",
            "Iter 1972, loss = 1.0671351\n",
            "Iter 1973, loss = 1.0054191\n",
            "Iter 1974, loss = 0.70045197\n",
            "Iter 1975, loss = 0.70663726\n",
            "Iter 1976, loss = 1.1247472\n",
            "Iter 1977, loss = 0.92097384\n",
            "Iter 1978, loss = 0.7509292\n",
            "Iter 1979, loss = 0.78026414\n",
            "Iter 1980, loss = 0.8668074\n",
            "Iter 1981, loss = 0.5673746\n",
            "Iter 1982, loss = 1.1827565\n",
            "Iter 1983, loss = 0.8380739\n",
            "Iter 1984, loss = 0.7628745\n",
            "Iter 1985, loss = 0.7756963\n",
            "Iter 1986, loss = 0.8478618\n",
            "Iter 1987, loss = 0.6040627\n",
            "Iter 1988, loss = 0.6615399\n",
            "Iter 1989, loss = 1.1303432\n",
            "Iter 1990, loss = 0.99620146\n",
            "Iter 1991, loss = 0.83875215\n",
            "Iter 1992, loss = 0.88090414\n",
            "Iter 1993, loss = 0.80805206\n",
            "Iter 1994, loss = 1.0550063\n",
            "Iter 1995, loss = 1.0068574\n",
            "Iter 1996, loss = 1.2514904\n",
            "Iter 1997, loss = 0.6066655\n",
            "Iter 1998, loss = 0.8916887\n",
            "Iter 1999, loss = 0.61676216\n",
            "Iter 2000, loss = 0.5597385\n",
            "Iter 2001, loss = 0.75435936\n",
            "Iter 2002, loss = 1.2338511\n",
            "Iter 2003, loss = 0.79367614\n",
            "Iter 2004, loss = 0.68350506\n",
            "Iter 2005, loss = 1.1678283\n",
            "Iter 2006, loss = 1.3424983\n",
            "Iter 2007, loss = 0.925126\n",
            "Iter 2008, loss = 0.8822293\n",
            "Iter 2009, loss = 0.74874127\n",
            "Iter 2010, loss = 0.7424625\n",
            "Iter 2011, loss = 0.8941112\n",
            "Iter 2012, loss = 0.94348466\n",
            "Iter 2013, loss = 0.7385374\n",
            "Iter 2014, loss = 0.64809525\n",
            "Iter 2015, loss = 0.7257512\n",
            "Iter 2016, loss = 1.2707094\n",
            "Iter 2017, loss = 0.56948006\n",
            "Iter 2018, loss = 0.8166244\n",
            "Iter 2019, loss = 1.0545138\n",
            "Iter 2020, loss = 1.0867767\n",
            "Iter 2021, loss = 0.902022\n",
            "Iter 2022, loss = 0.84637374\n",
            "Iter 2023, loss = 1.0794789\n",
            "Iter 2024, loss = 0.6868522\n",
            "Iter 2025, loss = 0.8651735\n",
            "Iter 2026, loss = 1.1358081\n",
            "Iter 2027, loss = 0.696535\n",
            "Iter 2028, loss = 0.9812571\n",
            "Iter 2029, loss = 0.66388434\n",
            "Iter 2030, loss = 0.8656988\n",
            "Iter 2031, loss = 0.9979547\n",
            "Iter 2032, loss = 0.7447876\n",
            "Iter 2033, loss = 0.978289\n",
            "Iter 2034, loss = 0.5011845\n",
            "Iter 2035, loss = 1.1060792\n",
            "Iter 2036, loss = 0.8244245\n",
            "Iter 2037, loss = 0.87560105\n",
            "Iter 2038, loss = 0.74953824\n",
            "Iter 2039, loss = 0.8236953\n",
            "Iter 2040, loss = 0.9241022\n",
            "Iter 2041, loss = 1.0240741\n",
            "Iter 2042, loss = 0.6491884\n",
            "Iter 2043, loss = 0.68483186\n",
            "Iter 2044, loss = 0.82472485\n",
            "Iter 2045, loss = 0.658823\n",
            "Iter 2046, loss = 0.594074\n",
            "Iter 2047, loss = 1.0630573\n",
            "Iter 2048, loss = 0.8790799\n",
            "Iter 2049, loss = 0.93097657\n",
            "Iter 2050, loss = 0.8457665\n",
            "Iter 2051, loss = 0.89849734\n",
            "Iter 2052, loss = 0.6969838\n",
            "Iter 2053, loss = 0.6770074\n",
            "Iter 2054, loss = 1.0559778\n",
            "Iter 2055, loss = 0.84394395\n",
            "Iter 2056, loss = 0.65297645\n",
            "Iter 2057, loss = 0.85389686\n",
            "Iter 2058, loss = 1.1599712\n",
            "Iter 2059, loss = 0.5903691\n",
            "Iter 2060, loss = 0.7312109\n",
            "Iter 2061, loss = 0.5983247\n",
            "Iter 2062, loss = 1.0569253\n",
            "Iter 2063, loss = 0.71093524\n",
            "Iter 2064, loss = 0.6572542\n",
            "Iter 2065, loss = 0.96759427\n",
            "Iter 2066, loss = 1.2157259\n",
            "Iter 2067, loss = 0.92609733\n",
            "Iter 2068, loss = 0.8316629\n",
            "Iter 2069, loss = 0.63785774\n",
            "Iter 2070, loss = 1.1440557\n",
            "Iter 2071, loss = 0.9708154\n",
            "Iter 2072, loss = 0.7984548\n",
            "Iter 2073, loss = 0.69723165\n",
            "Iter 2074, loss = 0.7986157\n",
            "Iter 2075, loss = 0.9592491\n",
            "Iter 2076, loss = 0.8274317\n",
            "Iter 2077, loss = 0.9500933\n",
            "Iter 2078, loss = 0.9476204\n",
            "Iter 2079, loss = 1.1885448\n",
            "Iter 2080, loss = 0.88014245\n",
            "Iter 2081, loss = 0.77901125\n",
            "Iter 2082, loss = 0.8411408\n",
            "Iter 2083, loss = 0.90538436\n",
            "Iter 2084, loss = 0.8901727\n",
            "Iter 2085, loss = 0.94476914\n",
            "Iter 2086, loss = 0.8693986\n",
            "Iter 2087, loss = 0.7372419\n",
            "Iter 2088, loss = 0.8881016\n",
            "Iter 2089, loss = 0.65054095\n",
            "Iter 2090, loss = 0.5742966\n",
            "Iter 2091, loss = 0.6714096\n",
            "Iter 2092, loss = 0.52978796\n",
            "Iter 2093, loss = 0.47874576\n",
            "Iter 2094, loss = 1.257966\n",
            "Iter 2095, loss = 0.77108467\n",
            "Iter 2096, loss = 0.556785\n",
            "Iter 2097, loss = 0.9641977\n",
            "Iter 2098, loss = 0.718729\n",
            "Iter 2099, loss = 0.7949859\n",
            "Iter 2100, loss = 0.8639779\n",
            "Iter 2101, loss = 0.8050271\n",
            "Iter 2102, loss = 0.9595723\n",
            "Iter 2103, loss = 0.85302126\n",
            "Iter 2104, loss = 1.2247577\n",
            "Iter 2105, loss = 0.72182757\n",
            "Iter 2106, loss = 1.1148311\n",
            "Iter 2107, loss = 0.81968933\n",
            "Iter 2108, loss = 0.9649067\n",
            "Iter 2109, loss = 0.8459203\n",
            "Iter 2110, loss = 0.78165376\n",
            "Iter 2111, loss = 0.7936013\n",
            "Iter 2112, loss = 1.0522094\n",
            "Iter 2113, loss = 0.67623943\n",
            "Iter 2114, loss = 0.75274837\n",
            "Iter 2115, loss = 0.6721684\n",
            "Iter 2116, loss = 0.8665323\n",
            "Iter 2117, loss = 0.87324274\n",
            "Iter 2118, loss = 0.77630997\n",
            "Iter 2119, loss = 0.9941137\n",
            "Iter 2120, loss = 0.75809556\n",
            "Iter 2121, loss = 0.7667688\n",
            "Iter 2122, loss = 0.85435516\n",
            "Iter 2123, loss = 0.74054384\n",
            "Iter 2124, loss = 1.1670318\n",
            "Iter 2125, loss = 0.79928\n",
            "Iter 2126, loss = 1.02016\n",
            "Iter 2127, loss = 0.76010525\n",
            "Iter 2128, loss = 1.0434153\n",
            "Iter 2129, loss = 0.8088972\n",
            "Iter 2130, loss = 0.9788061\n",
            "Iter 2131, loss = 0.67542636\n",
            "Iter 2132, loss = 0.8454627\n",
            "Iter 2133, loss = 0.8799325\n",
            "Iter 2134, loss = 0.7377895\n",
            "Iter 2135, loss = 0.704501\n",
            "Iter 2136, loss = 0.994874\n",
            "Iter 2137, loss = 1.0803485\n",
            "Iter 2138, loss = 0.7409907\n",
            "Iter 2139, loss = 0.7018449\n",
            "Iter 2140, loss = 1.094414\n",
            "Iter 2141, loss = 0.7354949\n",
            "Iter 2142, loss = 0.8056446\n",
            "Iter 2143, loss = 1.1231612\n",
            "Iter 2144, loss = 0.8239172\n",
            "Iter 2145, loss = 0.9375887\n",
            "Iter 2146, loss = 0.74966574\n",
            "Iter 2147, loss = 0.7513292\n",
            "Iter 2148, loss = 0.63154614\n",
            "Iter 2149, loss = 0.78084356\n",
            "Iter 2150, loss = 0.6027992\n",
            "Iter 2151, loss = 1.1712513\n",
            "Iter 2152, loss = 0.83341193\n",
            "Iter 2153, loss = 0.9659604\n",
            "Iter 2154, loss = 0.8063099\n",
            "Iter 2155, loss = 0.9382126\n",
            "Iter 2156, loss = 0.78527105\n",
            "Iter 2157, loss = 0.709207\n",
            "Iter 2158, loss = 0.84534657\n",
            "Iter 2159, loss = 0.6664924\n",
            "Iter 2160, loss = 0.6488409\n",
            "Iter 2161, loss = 0.6313881\n",
            "Iter 2162, loss = 1.2895443\n",
            "Iter 2163, loss = 1.0186818\n",
            "Iter 2164, loss = 0.53991634\n",
            "Iter 2165, loss = 0.736193\n",
            "Iter 2166, loss = 1.3353176\n",
            "Iter 2167, loss = 0.8382158\n",
            "Iter 2168, loss = 1.1925783\n",
            "Iter 2169, loss = 0.80133224\n",
            "Iter 2170, loss = 0.67628336\n",
            "Iter 2171, loss = 1.2979288\n",
            "Iter 2172, loss = 0.7668905\n",
            "Iter 2173, loss = 0.76773965\n",
            "Iter 2174, loss = 0.72231174\n",
            "Iter 2175, loss = 1.0108558\n",
            "Iter 2176, loss = 0.7320405\n",
            "Iter 2177, loss = 1.0279963\n",
            "Iter 2178, loss = 0.83457303\n",
            "Iter 2179, loss = 0.79911876\n",
            "Iter 2180, loss = 0.624804\n",
            "Iter 2181, loss = 0.839689\n",
            "Iter 2182, loss = 0.84699905\n",
            "Iter 2183, loss = 0.63774806\n",
            "Iter 2184, loss = 0.86849105\n",
            "Iter 2185, loss = 1.3572261\n",
            "Iter 2186, loss = 0.96981645\n",
            "Iter 2187, loss = 0.9603486\n",
            "Iter 2188, loss = 0.7324141\n",
            "Iter 2189, loss = 0.716667\n",
            "Iter 2190, loss = 0.6625816\n",
            "Iter 2191, loss = 1.2401481\n",
            "Iter 2192, loss = 1.0156033\n",
            "Iter 2193, loss = 0.8639022\n",
            "Iter 2194, loss = 1.0920005\n",
            "Iter 2195, loss = 0.9948351\n",
            "Iter 2196, loss = 0.9310956\n",
            "Iter 2197, loss = 0.89401144\n",
            "Iter 2198, loss = 0.8765134\n",
            "Iter 2199, loss = 0.98066175\n",
            "Iter 2200, loss = 0.7847033\n",
            "Iter 2201, loss = 1.1100881\n",
            "Iter 2202, loss = 0.86229324\n",
            "Iter 2203, loss = 0.9292331\n",
            "Iter 2204, loss = 0.86775696\n",
            "Iter 2205, loss = 0.8340397\n",
            "Iter 2206, loss = 0.8270427\n",
            "Iter 2207, loss = 0.7967714\n",
            "Iter 2208, loss = 0.8372991\n",
            "Iter 2209, loss = 1.0340887\n",
            "Iter 2210, loss = 0.795539\n",
            "Iter 2211, loss = 0.82922024\n",
            "Iter 2212, loss = 0.9790163\n",
            "Iter 2213, loss = 0.8589275\n",
            "Iter 2214, loss = 0.7979791\n",
            "Iter 2215, loss = 0.8148924\n",
            "Iter 2216, loss = 0.99832\n",
            "Iter 2217, loss = 0.92381954\n",
            "Iter 2218, loss = 0.7033146\n",
            "Iter 2219, loss = 0.73287433\n",
            "Iter 2220, loss = 0.72758955\n",
            "Iter 2221, loss = 0.75205076\n",
            "Iter 2222, loss = 0.84413016\n",
            "Iter 2223, loss = 0.77627265\n",
            "Iter 2224, loss = 0.73830223\n",
            "Iter 2225, loss = 0.7776861\n",
            "Iter 2226, loss = 0.60891175\n",
            "Iter 2227, loss = 0.4661061\n",
            "Iter 2228, loss = 0.72704005\n",
            "Iter 2229, loss = 1.201387\n",
            "Iter 2230, loss = 0.9183414\n",
            "Iter 2231, loss = 0.99947715\n",
            "Iter 2232, loss = 1.0560118\n",
            "Iter 2233, loss = 1.1591911\n",
            "Iter 2234, loss = 0.79987395\n",
            "Iter 2235, loss = 0.690239\n",
            "Iter 2236, loss = 1.148982\n",
            "Iter 2237, loss = 0.9093468\n",
            "Iter 2238, loss = 0.82510054\n",
            "Iter 2239, loss = 0.74285686\n",
            "Iter 2240, loss = 0.8057341\n",
            "Iter 2241, loss = 0.79909676\n",
            "Iter 2242, loss = 0.73740596\n",
            "Iter 2243, loss = 0.86524546\n",
            "Iter 2244, loss = 0.96570367\n",
            "Iter 2245, loss = 0.6516856\n",
            "Iter 2246, loss = 1.0062766\n",
            "Iter 2247, loss = 1.1045365\n",
            "Iter 2248, loss = 0.93129396\n",
            "Iter 2249, loss = 0.63323814\n",
            "Iter 2250, loss = 0.71820474\n",
            "Iter 2251, loss = 1.0808854\n",
            "Iter 2252, loss = 1.0693729\n",
            "Iter 2253, loss = 0.7979636\n",
            "Iter 2254, loss = 0.84775877\n",
            "Iter 2255, loss = 0.75755346\n",
            "Iter 2256, loss = 1.0439341\n",
            "Iter 2257, loss = 0.9094707\n",
            "Iter 2258, loss = 0.8954716\n",
            "Iter 2259, loss = 0.78814036\n",
            "Iter 2260, loss = 0.86218464\n",
            "Iter 2261, loss = 0.7285952\n",
            "Iter 2262, loss = 0.831005\n",
            "Iter 2263, loss = 0.8732234\n",
            "Iter 2264, loss = 0.51706314\n",
            "Iter 2265, loss = 0.9717806\n",
            "Iter 2266, loss = 0.5659634\n",
            "Iter 2267, loss = 1.4400392\n",
            "Iter 2268, loss = 1.2911166\n",
            "Iter 2269, loss = 0.82860386\n",
            "Iter 2270, loss = 0.95672464\n",
            "Iter 2271, loss = 0.8538796\n",
            "Iter 2272, loss = 0.76660764\n",
            "Iter 2273, loss = 0.72872114\n",
            "Iter 2274, loss = 0.7827672\n",
            "Iter 2275, loss = 0.9430173\n",
            "Iter 2276, loss = 0.96753347\n",
            "Iter 2277, loss = 0.94156396\n",
            "Iter 2278, loss = 0.86592853\n",
            "Iter 2279, loss = 0.74759245\n",
            "Iter 2280, loss = 0.87548536\n",
            "Iter 2281, loss = 0.9278702\n",
            "Iter 2282, loss = 1.0425993\n",
            "Iter 2283, loss = 0.7027353\n",
            "Iter 2284, loss = 1.0403457\n",
            "Iter 2285, loss = 0.8178255\n",
            "Iter 2286, loss = 0.90608597\n",
            "Iter 2287, loss = 0.5049931\n",
            "Iter 2288, loss = 0.49387237\n",
            "Iter 2289, loss = 0.9658277\n",
            "Iter 2290, loss = 0.8657452\n",
            "Iter 2291, loss = 0.7444433\n",
            "Iter 2292, loss = 1.288003\n",
            "Iter 2293, loss = 0.73799247\n",
            "Iter 2294, loss = 0.9012222\n",
            "Iter 2295, loss = 1.0036306\n",
            "Iter 2296, loss = 0.6249206\n",
            "Iter 2297, loss = 0.95823056\n",
            "Iter 2298, loss = 0.87711513\n",
            "Iter 2299, loss = 0.7762128\n",
            "Iter 2300, loss = 0.8323074\n",
            "Iter 2301, loss = 0.7499502\n",
            "Iter 2302, loss = 0.9431674\n",
            "Iter 2303, loss = 0.9330824\n",
            "Iter 2304, loss = 0.9410131\n",
            "Iter 2305, loss = 0.54449886\n",
            "Iter 2306, loss = 0.907549\n",
            "Iter 2307, loss = 0.9349965\n",
            "Iter 2308, loss = 0.78048384\n",
            "Iter 2309, loss = 0.85901666\n",
            "Iter 2310, loss = 0.81256425\n",
            "Iter 2311, loss = 0.8718008\n",
            "Iter 2312, loss = 0.60576093\n",
            "Iter 2313, loss = 1.1103749\n",
            "Iter 2314, loss = 0.78863657\n",
            "Iter 2315, loss = 0.88535905\n",
            "Iter 2316, loss = 0.85460186\n",
            "Iter 2317, loss = 0.86648536\n",
            "Iter 2318, loss = 1.049156\n",
            "Iter 2319, loss = 0.9008182\n",
            "Iter 2320, loss = 0.91784656\n",
            "Iter 2321, loss = 0.6719649\n",
            "Iter 2322, loss = 0.8551148\n",
            "Iter 2323, loss = 0.8857125\n",
            "Iter 2324, loss = 0.9858979\n",
            "Iter 2325, loss = 0.7587142\n",
            "Iter 2326, loss = 0.49684507\n",
            "Iter 2327, loss = 0.77454\n",
            "Iter 2328, loss = 0.8317529\n",
            "Iter 2329, loss = 1.0515293\n",
            "Iter 2330, loss = 0.8759004\n",
            "Iter 2331, loss = 0.9684903\n",
            "Iter 2332, loss = 0.90541184\n",
            "Iter 2333, loss = 0.8559371\n",
            "Iter 2334, loss = 0.72622633\n",
            "Iter 2335, loss = 0.6961094\n",
            "Iter 2336, loss = 0.9456921\n",
            "Iter 2337, loss = 0.8308511\n",
            "Iter 2338, loss = 0.89247096\n",
            "Iter 2339, loss = 0.8947707\n",
            "Iter 2340, loss = 0.7206179\n",
            "Iter 2341, loss = 1.0143685\n",
            "Iter 2342, loss = 0.7926911\n",
            "Iter 2343, loss = 0.7231293\n",
            "Iter 2344, loss = 0.91872114\n",
            "Iter 2345, loss = 0.79078937\n",
            "Iter 2346, loss = 0.8421564\n",
            "Iter 2347, loss = 1.0265138\n",
            "Iter 2348, loss = 0.92622554\n",
            "Iter 2349, loss = 0.7076466\n",
            "Iter 2350, loss = 0.6254809\n",
            "Iter 2351, loss = 0.95119447\n",
            "Iter 2352, loss = 0.6251844\n",
            "Iter 2353, loss = 1.0127406\n",
            "Iter 2354, loss = 0.92704606\n",
            "Iter 2355, loss = 0.7521501\n",
            "Iter 2356, loss = 1.0727187\n",
            "Iter 2357, loss = 0.7930288\n",
            "Iter 2358, loss = 0.8168886\n",
            "Iter 2359, loss = 0.8951426\n",
            "Iter 2360, loss = 0.97682315\n",
            "Iter 2361, loss = 0.6735624\n",
            "Iter 2362, loss = 1.1436471\n",
            "Iter 2363, loss = 0.61517763\n",
            "Iter 2364, loss = 0.7663443\n",
            "Iter 2365, loss = 0.8544906\n",
            "Iter 2366, loss = 1.0899929\n",
            "Iter 2367, loss = 0.6906338\n",
            "Iter 2368, loss = 0.6186544\n",
            "Iter 2369, loss = 1.0582626\n",
            "Iter 2370, loss = 0.6945485\n",
            "Iter 2371, loss = 0.97242856\n",
            "Iter 2372, loss = 1.1188927\n",
            "Iter 2373, loss = 1.0415099\n",
            "Iter 2374, loss = 0.8382333\n",
            "Iter 2375, loss = 0.8408469\n",
            "Iter 2376, loss = 0.81463677\n",
            "Iter 2377, loss = 0.76189363\n",
            "Iter 2378, loss = 0.6653259\n",
            "Iter 2379, loss = 0.8488196\n",
            "Iter 2380, loss = 0.8317614\n",
            "Iter 2381, loss = 0.71952397\n",
            "Iter 2382, loss = 0.7429041\n",
            "Iter 2383, loss = 0.617139\n",
            "Iter 2384, loss = 0.5520879\n",
            "Iter 2385, loss = 0.7962079\n",
            "Iter 2386, loss = 1.3421323\n",
            "Iter 2387, loss = 1.1129042\n",
            "Iter 2388, loss = 0.6212094\n",
            "Iter 2389, loss = 1.0759232\n",
            "Iter 2390, loss = 0.669754\n",
            "Iter 2391, loss = 0.7525658\n",
            "Iter 2392, loss = 1.1633011\n",
            "Iter 2393, loss = 0.6211585\n",
            "Iter 2394, loss = 0.8274225\n",
            "Iter 2395, loss = 0.94831514\n",
            "Iter 2396, loss = 1.1941296\n",
            "Iter 2397, loss = 0.9589528\n",
            "Iter 2398, loss = 0.75004274\n",
            "Iter 2399, loss = 0.7772354\n",
            "Iter 2400, loss = 0.9071883\n",
            "Iter 2401, loss = 0.94782543\n",
            "Iter 2402, loss = 1.1134584\n",
            "Iter 2403, loss = 0.7449037\n",
            "Iter 2404, loss = 1.054426\n",
            "Iter 2405, loss = 0.7123455\n",
            "Iter 2406, loss = 0.8291392\n",
            "Iter 2407, loss = 1.0063717\n",
            "Iter 2408, loss = 0.825766\n",
            "Iter 2409, loss = 0.7625965\n",
            "Iter 2410, loss = 0.63329434\n",
            "Iter 2411, loss = 0.736656\n",
            "Iter 2412, loss = 0.8750289\n",
            "Iter 2413, loss = 0.90716994\n",
            "Iter 2414, loss = 0.97752815\n",
            "Iter 2415, loss = 0.7431717\n",
            "Iter 2416, loss = 0.7925223\n",
            "Iter 2417, loss = 0.50130635\n",
            "Iter 2418, loss = 0.95443994\n",
            "Iter 2419, loss = 0.68913084\n",
            "Iter 2420, loss = 1.1099466\n",
            "Iter 2421, loss = 0.970708\n",
            "Iter 2422, loss = 1.0978937\n",
            "Iter 2423, loss = 0.76717067\n",
            "Iter 2424, loss = 1.0823616\n",
            "Iter 2425, loss = 0.8359021\n",
            "Iter 2426, loss = 0.86847126\n",
            "Iter 2427, loss = 0.8756449\n",
            "Iter 2428, loss = 0.76611817\n",
            "Iter 2429, loss = 0.8578933\n",
            "Iter 2430, loss = 0.70982414\n",
            "Iter 2431, loss = 0.67112696\n",
            "Iter 2432, loss = 0.95710933\n",
            "Iter 2433, loss = 0.75860316\n",
            "Iter 2434, loss = 0.8921781\n",
            "Iter 2435, loss = 0.8205638\n",
            "Iter 2436, loss = 0.9123037\n",
            "Iter 2437, loss = 0.6821619\n",
            "Iter 2438, loss = 0.98222184\n",
            "Iter 2439, loss = 0.7890392\n",
            "Iter 2440, loss = 0.7285893\n",
            "Iter 2441, loss = 0.97280085\n",
            "Iter 2442, loss = 0.7225\n",
            "Iter 2443, loss = 0.9425373\n",
            "Iter 2444, loss = 0.77332747\n",
            "Iter 2445, loss = 0.9844959\n",
            "Iter 2446, loss = 0.83679986\n",
            "Iter 2447, loss = 0.8033041\n",
            "Iter 2448, loss = 0.7708521\n",
            "Iter 2449, loss = 0.91274065\n",
            "Iter 2450, loss = 0.9893303\n",
            "Iter 2451, loss = 0.66954947\n",
            "Iter 2452, loss = 1.0265745\n",
            "Iter 2453, loss = 0.9224758\n",
            "Iter 2454, loss = 0.7858937\n",
            "Iter 2455, loss = 1.1157093\n",
            "Iter 2456, loss = 0.8893016\n",
            "Iter 2457, loss = 0.6748766\n",
            "Iter 2458, loss = 0.7950475\n",
            "Iter 2459, loss = 0.94821054\n",
            "Iter 2460, loss = 0.77733433\n",
            "Iter 2461, loss = 0.8297356\n",
            "Iter 2462, loss = 0.84217685\n",
            "Iter 2463, loss = 0.7777102\n",
            "Iter 2464, loss = 1.152984\n",
            "Iter 2465, loss = 0.95946354\n",
            "Iter 2466, loss = 0.82101464\n",
            "Iter 2467, loss = 0.64358664\n",
            "Iter 2468, loss = 0.55420005\n",
            "Iter 2469, loss = 0.9926607\n",
            "Iter 2470, loss = 1.0452411\n",
            "Iter 2471, loss = 0.58957124\n",
            "Iter 2472, loss = 1.2984579\n",
            "Iter 2473, loss = 0.83747756\n",
            "Iter 2474, loss = 0.73829687\n",
            "Iter 2475, loss = 0.79087406\n",
            "Iter 2476, loss = 0.9953339\n",
            "Iter 2477, loss = 0.5317325\n",
            "Iter 2478, loss = 0.89512295\n",
            "Iter 2479, loss = 1.0907965\n",
            "Iter 2480, loss = 0.92185205\n",
            "Iter 2481, loss = 0.61343193\n",
            "Iter 2482, loss = 0.8322203\n",
            "Iter 2483, loss = 1.0076089\n",
            "Iter 2484, loss = 0.7062721\n",
            "Iter 2485, loss = 0.58394265\n",
            "Iter 2486, loss = 0.71822464\n",
            "Iter 2487, loss = 0.457893\n",
            "Iter 2488, loss = 0.6204088\n",
            "Iter 2489, loss = 1.1853633\n",
            "Iter 2490, loss = 0.7095735\n",
            "Iter 2491, loss = 0.9992378\n",
            "Iter 2492, loss = 0.8590799\n",
            "Iter 2493, loss = 0.8438036\n",
            "Iter 2494, loss = 0.8777944\n",
            "Iter 2495, loss = 0.7339763\n",
            "Iter 2496, loss = 1.0244696\n",
            "Iter 2497, loss = 0.7795446\n",
            "Iter 2498, loss = 0.7526992\n",
            "Iter 2499, loss = 0.77979255\n",
            "Iter 2500, loss = 0.91999483\n",
            "Iter 2501, loss = 0.76359355\n",
            "Iter 2502, loss = 0.9580457\n",
            "Iter 2503, loss = 0.6729735\n",
            "Iter 2504, loss = 0.91024566\n",
            "Iter 2505, loss = 0.92589927\n",
            "Iter 2506, loss = 0.9697498\n",
            "Iter 2507, loss = 0.9421324\n",
            "Iter 2508, loss = 0.82952046\n",
            "Iter 2509, loss = 0.83282423\n",
            "Iter 2510, loss = 0.9710068\n",
            "Iter 2511, loss = 0.8965888\n",
            "Iter 2512, loss = 0.77525175\n",
            "Iter 2513, loss = 0.85219043\n",
            "Iter 2514, loss = 0.75181687\n",
            "Iter 2515, loss = 0.54402626\n",
            "Iter 2516, loss = 0.77875555\n",
            "Iter 2517, loss = 0.7189086\n",
            "Iter 2518, loss = 0.66702944\n",
            "Iter 2519, loss = 1.0637639\n",
            "Iter 2520, loss = 0.983991\n",
            "Iter 2521, loss = 0.80836105\n",
            "Iter 2522, loss = 1.0688498\n",
            "Iter 2523, loss = 0.7572429\n",
            "Iter 2524, loss = 0.6584727\n",
            "Iter 2525, loss = 0.92893314\n",
            "Iter 2526, loss = 0.50270563\n",
            "Iter 2527, loss = 0.45504123\n",
            "Iter 2528, loss = 0.9226662\n",
            "Iter 2529, loss = 1.0228434\n",
            "Iter 2530, loss = 1.2388195\n",
            "Iter 2531, loss = 0.5087309\n",
            "Iter 2532, loss = 0.681543\n",
            "Iter 2533, loss = 0.9679897\n",
            "Iter 2534, loss = 0.59267133\n",
            "Iter 2535, loss = 0.91619086\n",
            "Iter 2536, loss = 0.69641334\n",
            "Iter 2537, loss = 0.54951704\n",
            "Iter 2538, loss = 1.3795122\n",
            "Iter 2539, loss = 0.894531\n",
            "Iter 2540, loss = 0.90731484\n",
            "Iter 2541, loss = 0.8883998\n",
            "Iter 2542, loss = 0.68732536\n",
            "Iter 2543, loss = 0.9378997\n",
            "Iter 2544, loss = 0.94339615\n",
            "Iter 2545, loss = 1.0669818\n",
            "Iter 2546, loss = 0.90365195\n",
            "Iter 2547, loss = 0.9128405\n",
            "Iter 2548, loss = 0.8626392\n",
            "Iter 2549, loss = 0.981436\n",
            "Iter 2550, loss = 0.92701143\n",
            "Iter 2551, loss = 0.5750894\n",
            "Iter 2552, loss = 1.0892748\n",
            "Iter 2553, loss = 0.56562567\n",
            "Iter 2554, loss = 0.9221987\n",
            "Iter 2555, loss = 0.856006\n",
            "Iter 2556, loss = 1.049509\n",
            "Iter 2557, loss = 0.7999824\n",
            "Iter 2558, loss = 1.0196259\n",
            "Iter 2559, loss = 1.0152516\n",
            "Iter 2560, loss = 0.968674\n",
            "Iter 2561, loss = 0.9547106\n",
            "Iter 2562, loss = 0.45816636\n",
            "Iter 2563, loss = 1.0618997\n",
            "Iter 2564, loss = 0.7852365\n",
            "Iter 2565, loss = 0.8305268\n",
            "Iter 2566, loss = 1.0092379\n",
            "Iter 2567, loss = 0.49456352\n",
            "Iter 2568, loss = 0.93472534\n",
            "Iter 2569, loss = 0.8825338\n",
            "Iter 2570, loss = 0.8613407\n",
            "Iter 2571, loss = 0.6407408\n",
            "Iter 2572, loss = 0.87885207\n",
            "Iter 2573, loss = 0.5905863\n",
            "Iter 2574, loss = 1.1000265\n",
            "Iter 2575, loss = 0.8883892\n",
            "Iter 2576, loss = 1.0529283\n",
            "Iter 2577, loss = 0.7504505\n",
            "Iter 2578, loss = 0.7447996\n",
            "Iter 2579, loss = 0.662232\n",
            "Iter 2580, loss = 0.75316465\n",
            "Iter 2581, loss = 1.4690859\n",
            "Iter 2582, loss = 0.83832693\n",
            "Iter 2583, loss = 1.052493\n",
            "Iter 2584, loss = 0.98628724\n",
            "Iter 2585, loss = 0.79613256\n",
            "Iter 2586, loss = 0.7687485\n",
            "Iter 2587, loss = 0.7723243\n",
            "Iter 2588, loss = 1.3299437\n",
            "Iter 2589, loss = 0.6357318\n",
            "Iter 2590, loss = 0.64424765\n",
            "Iter 2591, loss = 0.77566195\n",
            "Iter 2592, loss = 0.73885196\n",
            "Iter 2593, loss = 0.70153856\n",
            "Iter 2594, loss = 1.1007264\n",
            "Iter 2595, loss = 0.82206464\n",
            "Iter 2596, loss = 0.88241804\n",
            "Iter 2597, loss = 0.78469896\n",
            "Iter 2598, loss = 0.75663155\n",
            "Iter 2599, loss = 1.0626199\n",
            "Iter 2600, loss = 1.0184646\n",
            "Iter 2601, loss = 0.6125138\n",
            "Iter 2602, loss = 0.8730976\n",
            "Iter 2603, loss = 1.2189949\n",
            "Iter 2604, loss = 1.2161763\n",
            "Iter 2605, loss = 0.85646534\n",
            "Iter 2606, loss = 0.9591789\n",
            "Iter 2607, loss = 0.7423229\n",
            "Iter 2608, loss = 0.7805565\n",
            "Iter 2609, loss = 1.1478033\n",
            "Iter 2610, loss = 0.63220394\n",
            "Iter 2611, loss = 1.0729616\n",
            "Iter 2612, loss = 0.72474337\n",
            "Iter 2613, loss = 0.6099179\n",
            "Iter 2614, loss = 1.0495453\n",
            "Iter 2615, loss = 0.8363488\n",
            "Iter 2616, loss = 1.3629498\n",
            "Iter 2617, loss = 0.9154365\n",
            "Iter 2618, loss = 0.77939165\n",
            "Iter 2619, loss = 0.69036853\n",
            "Iter 2620, loss = 0.813516\n",
            "Iter 2621, loss = 0.88475144\n",
            "Iter 2622, loss = 0.8799695\n",
            "Iter 2623, loss = 0.8257473\n",
            "Iter 2624, loss = 0.72877824\n",
            "Iter 2625, loss = 0.64900744\n",
            "Iter 2626, loss = 0.91565907\n",
            "Iter 2627, loss = 0.81868064\n",
            "Iter 2628, loss = 0.8265507\n",
            "Iter 2629, loss = 1.0034974\n",
            "Iter 2630, loss = 1.1241571\n",
            "Iter 2631, loss = 0.685837\n",
            "Iter 2632, loss = 0.76083183\n",
            "Iter 2633, loss = 1.2268338\n",
            "Iter 2634, loss = 0.8949245\n",
            "Iter 2635, loss = 0.8242341\n",
            "Iter 2636, loss = 0.5998409\n",
            "Iter 2637, loss = 0.6968487\n",
            "Iter 2638, loss = 1.0549741\n",
            "Iter 2639, loss = 0.9547353\n",
            "Iter 2640, loss = 0.70430493\n",
            "Iter 2641, loss = 0.7769144\n",
            "Iter 2642, loss = 0.61571765\n",
            "Iter 2643, loss = 1.0095096\n",
            "Iter 2644, loss = 1.1014607\n",
            "Iter 2645, loss = 0.9398017\n",
            "Iter 2646, loss = 0.92847514\n",
            "Iter 2647, loss = 0.7676921\n",
            "Iter 2648, loss = 0.47155795\n",
            "Iter 2649, loss = 0.92662543\n",
            "Iter 2650, loss = 0.8258171\n",
            "Iter 2651, loss = 0.6939719\n",
            "Iter 2652, loss = 0.7784237\n",
            "Iter 2653, loss = 0.6932351\n",
            "Iter 2654, loss = 0.8720839\n",
            "Iter 2655, loss = 1.056046\n",
            "Iter 2656, loss = 0.78439265\n",
            "Iter 2657, loss = 0.9668144\n",
            "Iter 2658, loss = 0.96874523\n",
            "Iter 2659, loss = 0.6826719\n",
            "Iter 2660, loss = 0.66581285\n",
            "Iter 2661, loss = 0.60886383\n",
            "Iter 2662, loss = 0.79063785\n",
            "Iter 2663, loss = 0.9460446\n",
            "Iter 2664, loss = 0.7360313\n",
            "Iter 2665, loss = 0.68379956\n",
            "Iter 2666, loss = 0.8579737\n",
            "Iter 2667, loss = 0.85411024\n",
            "Iter 2668, loss = 0.8072609\n",
            "Iter 2669, loss = 0.81148714\n",
            "Iter 2670, loss = 0.93986094\n",
            "Iter 2671, loss = 0.5412892\n",
            "Iter 2672, loss = 0.6224072\n",
            "Iter 2673, loss = 1.0671431\n",
            "Iter 2674, loss = 0.8029743\n",
            "Iter 2675, loss = 1.0627707\n",
            "Iter 2676, loss = 0.5153662\n",
            "Iter 2677, loss = 0.7222751\n",
            "Iter 2678, loss = 0.8933592\n",
            "Iter 2679, loss = 0.71729124\n",
            "Iter 2680, loss = 1.0519524\n",
            "Iter 2681, loss = 0.8374412\n",
            "Iter 2682, loss = 0.6731398\n",
            "Iter 2683, loss = 0.6170926\n",
            "Iter 2684, loss = 1.0206597\n",
            "Iter 2685, loss = 0.8180847\n",
            "Iter 2686, loss = 1.0220609\n",
            "Iter 2687, loss = 0.78268117\n",
            "Iter 2688, loss = 0.94941556\n",
            "Iter 2689, loss = 0.9457072\n",
            "Iter 2690, loss = 0.8285005\n",
            "Iter 2691, loss = 0.89145505\n",
            "Iter 2692, loss = 0.9191184\n",
            "Iter 2693, loss = 0.7723081\n",
            "Iter 2694, loss = 0.9990567\n",
            "Iter 2695, loss = 0.83594143\n",
            "Iter 2696, loss = 0.8501579\n",
            "Iter 2697, loss = 0.9142768\n",
            "Iter 2698, loss = 0.52645653\n",
            "Iter 2699, loss = 1.1069446\n",
            "Iter 2700, loss = 0.70268977\n",
            "Iter 2701, loss = 0.6376757\n",
            "Iter 2702, loss = 0.8369353\n",
            "Iter 2703, loss = 1.2488129\n",
            "Iter 2704, loss = 0.68659735\n",
            "Iter 2705, loss = 0.9766408\n",
            "Iter 2706, loss = 1.1789807\n",
            "Iter 2707, loss = 0.8784969\n",
            "Iter 2708, loss = 0.8327707\n",
            "Iter 2709, loss = 0.7570358\n",
            "Iter 2710, loss = 0.923929\n",
            "Iter 2711, loss = 1.1338706\n",
            "Iter 2712, loss = 0.80269414\n",
            "Iter 2713, loss = 0.8180784\n",
            "Iter 2714, loss = 1.0442883\n",
            "Iter 2715, loss = 0.94747007\n",
            "Iter 2716, loss = 0.74391747\n",
            "Iter 2717, loss = 0.9832095\n",
            "Iter 2718, loss = 1.0726602\n",
            "Iter 2719, loss = 0.6930338\n",
            "Iter 2720, loss = 0.8851843\n",
            "Iter 2721, loss = 0.8646817\n",
            "Iter 2722, loss = 0.98042274\n",
            "Iter 2723, loss = 0.7292348\n",
            "Iter 2724, loss = 0.7066059\n",
            "Iter 2725, loss = 1.0306789\n",
            "Iter 2726, loss = 1.0050476\n",
            "Iter 2727, loss = 0.88252735\n",
            "Iter 2728, loss = 0.9214171\n",
            "Iter 2729, loss = 0.50902003\n",
            "Iter 2730, loss = 1.198754\n",
            "Iter 2731, loss = 0.7573907\n",
            "Iter 2732, loss = 0.7659633\n",
            "Iter 2733, loss = 0.8914008\n",
            "Iter 2734, loss = 0.6225438\n",
            "Iter 2735, loss = 0.88316226\n",
            "Iter 2736, loss = 1.0075886\n",
            "Iter 2737, loss = 1.0551791\n",
            "Iter 2738, loss = 0.82900846\n",
            "Iter 2739, loss = 0.6169945\n",
            "Iter 2740, loss = 0.89263856\n",
            "Iter 2741, loss = 0.57107615\n",
            "Iter 2742, loss = 0.6934815\n",
            "Iter 2743, loss = 1.1612194\n",
            "Iter 2744, loss = 0.6961342\n",
            "Iter 2745, loss = 0.67699695\n",
            "Iter 2746, loss = 0.6831213\n",
            "Iter 2747, loss = 0.90785956\n",
            "Iter 2748, loss = 0.8879999\n",
            "Iter 2749, loss = 0.8433394\n",
            "Iter 2750, loss = 1.0497949\n",
            "Iter 2751, loss = 0.92731714\n",
            "Iter 2752, loss = 0.8789387\n",
            "Iter 2753, loss = 0.855649\n",
            "Iter 2754, loss = 0.94924283\n",
            "Iter 2755, loss = 0.8029624\n",
            "Iter 2756, loss = 0.92051363\n",
            "Iter 2757, loss = 0.83463395\n",
            "Iter 2758, loss = 0.9803872\n",
            "Iter 2759, loss = 0.8184638\n",
            "Iter 2760, loss = 0.80683565\n",
            "Iter 2761, loss = 0.9703151\n",
            "Iter 2762, loss = 0.8828549\n",
            "Iter 2763, loss = 0.7844539\n",
            "Iter 2764, loss = 0.64152527\n",
            "Iter 2765, loss = 0.9690132\n",
            "Iter 2766, loss = 0.78713405\n",
            "Iter 2767, loss = 0.6575289\n",
            "Iter 2768, loss = 0.9836757\n",
            "Iter 2769, loss = 0.8472748\n",
            "Iter 2770, loss = 1.0573804\n",
            "Iter 2771, loss = 1.0117803\n",
            "Iter 2772, loss = 1.0682886\n",
            "Iter 2773, loss = 1.1011081\n",
            "Iter 2774, loss = 0.8592396\n",
            "Iter 2775, loss = 0.88962555\n",
            "Iter 2776, loss = 0.87514824\n",
            "Iter 2777, loss = 0.87042767\n",
            "Iter 2778, loss = 0.80164444\n",
            "Iter 2779, loss = 0.9738823\n",
            "Iter 2780, loss = 1.0714123\n",
            "Iter 2781, loss = 0.8074712\n",
            "Iter 2782, loss = 0.62788606\n",
            "Iter 2783, loss = 0.8449749\n",
            "Iter 2784, loss = 0.7286091\n",
            "Iter 2785, loss = 1.1359208\n",
            "Iter 2786, loss = 0.8010253\n",
            "Iter 2787, loss = 0.7712213\n",
            "Iter 2788, loss = 0.77000827\n",
            "Iter 2789, loss = 0.69193995\n",
            "Iter 2790, loss = 0.6436503\n",
            "Iter 2791, loss = 1.0315571\n",
            "Iter 2792, loss = 0.68288195\n",
            "Iter 2793, loss = 0.9718859\n",
            "Iter 2794, loss = 0.66020954\n",
            "Iter 2795, loss = 0.8922831\n",
            "Iter 2796, loss = 0.9600749\n",
            "Iter 2797, loss = 0.7614864\n",
            "Iter 2798, loss = 0.65795004\n",
            "Iter 2799, loss = 0.91442513\n",
            "Iter 2800, loss = 0.7766391\n",
            "Iter 2801, loss = 1.0408475\n",
            "Iter 2802, loss = 0.856811\n",
            "Iter 2803, loss = 0.7262954\n",
            "Iter 2804, loss = 0.8254138\n",
            "Iter 2805, loss = 0.69308007\n",
            "Iter 2806, loss = 0.6417171\n",
            "Iter 2807, loss = 1.0176296\n",
            "Iter 2808, loss = 0.76016766\n",
            "Iter 2809, loss = 0.71281546\n",
            "Iter 2810, loss = 0.85098445\n",
            "Iter 2811, loss = 0.9550836\n",
            "Iter 2812, loss = 0.71679705\n",
            "Iter 2813, loss = 0.82220334\n",
            "Iter 2814, loss = 0.70835936\n",
            "Iter 2815, loss = 0.8021506\n",
            "Iter 2816, loss = 0.80306214\n",
            "Iter 2817, loss = 0.7726752\n",
            "Iter 2818, loss = 0.5894136\n",
            "Iter 2819, loss = 0.9975198\n",
            "Iter 2820, loss = 0.80432415\n",
            "Iter 2821, loss = 0.8554269\n",
            "Iter 2822, loss = 0.68339074\n",
            "Iter 2823, loss = 0.7122581\n",
            "Iter 2824, loss = 0.63665247\n",
            "Iter 2825, loss = 0.80631435\n",
            "Iter 2826, loss = 0.82798207\n",
            "Iter 2827, loss = 0.97621506\n",
            "Iter 2828, loss = 1.0599139\n",
            "Iter 2829, loss = 1.0212076\n",
            "Iter 2830, loss = 0.65641123\n",
            "Iter 2831, loss = 0.43842733\n",
            "Iter 2832, loss = 0.9048051\n",
            "Iter 2833, loss = 1.1672591\n",
            "Iter 2834, loss = 0.9454129\n",
            "Iter 2835, loss = 0.53935325\n",
            "Iter 2836, loss = 0.82759655\n",
            "Iter 2837, loss = 0.9657233\n",
            "Iter 2838, loss = 0.8397163\n",
            "Iter 2839, loss = 0.7522594\n",
            "Iter 2840, loss = 0.9544423\n",
            "Iter 2841, loss = 0.9685083\n",
            "Iter 2842, loss = 0.7906904\n",
            "Iter 2843, loss = 0.85055125\n",
            "Iter 2844, loss = 0.930259\n",
            "Iter 2845, loss = 0.7848586\n",
            "Iter 2846, loss = 0.8210135\n",
            "Iter 2847, loss = 0.75285643\n",
            "Iter 2848, loss = 0.669569\n",
            "Iter 2849, loss = 1.1897547\n",
            "Iter 2850, loss = 0.5994635\n",
            "Iter 2851, loss = 0.61105514\n",
            "Iter 2852, loss = 0.92784154\n",
            "Iter 2853, loss = 0.6556703\n",
            "Iter 2854, loss = 0.8815346\n",
            "Iter 2855, loss = 0.54267323\n",
            "Iter 2856, loss = 0.78235495\n",
            "Iter 2857, loss = 1.1280893\n",
            "Iter 2858, loss = 0.82439005\n",
            "Iter 2859, loss = 0.6727044\n",
            "Iter 2860, loss = 0.8955188\n",
            "Iter 2861, loss = 0.68122137\n",
            "Iter 2862, loss = 0.92137957\n",
            "Iter 2863, loss = 0.6463229\n",
            "Iter 2864, loss = 0.65436083\n",
            "Iter 2865, loss = 0.7643075\n",
            "Iter 2866, loss = 0.9472045\n",
            "Iter 2867, loss = 0.95596313\n",
            "Iter 2868, loss = 0.7214825\n",
            "Iter 2869, loss = 0.7461047\n",
            "Iter 2870, loss = 0.7836096\n",
            "Iter 2871, loss = 0.9098141\n",
            "Iter 2872, loss = 1.0700948\n",
            "Iter 2873, loss = 0.66223997\n",
            "Iter 2874, loss = 0.70662427\n",
            "Iter 2875, loss = 0.9310303\n",
            "Iter 2876, loss = 0.8413097\n",
            "Iter 2877, loss = 0.9058466\n",
            "Iter 2878, loss = 0.7221378\n",
            "Iter 2879, loss = 0.7228831\n",
            "Iter 2880, loss = 0.8384868\n",
            "Iter 2881, loss = 1.599785\n",
            "Iter 2882, loss = 0.81996346\n",
            "Iter 2883, loss = 0.6442132\n",
            "Iter 2884, loss = 0.92700714\n",
            "Iter 2885, loss = 0.6826783\n",
            "Iter 2886, loss = 0.70073867\n",
            "Iter 2887, loss = 0.67758274\n",
            "Iter 2888, loss = 1.0140293\n",
            "Iter 2889, loss = 0.73451865\n",
            "Iter 2890, loss = 0.7884674\n",
            "Iter 2891, loss = 0.8636843\n",
            "Iter 2892, loss = 1.0449582\n",
            "Iter 2893, loss = 0.8863499\n",
            "Iter 2894, loss = 0.65109265\n",
            "Iter 2895, loss = 1.0034193\n",
            "Iter 2896, loss = 0.85148406\n",
            "Iter 2897, loss = 1.0701663\n",
            "Iter 2898, loss = 0.92937344\n",
            "Iter 2899, loss = 0.6371235\n",
            "Iter 2900, loss = 0.77199847\n",
            "Iter 2901, loss = 0.7292591\n",
            "Iter 2902, loss = 0.80144\n",
            "Iter 2903, loss = 0.8989817\n",
            "Iter 2904, loss = 0.95986646\n",
            "Iter 2905, loss = 0.8479898\n",
            "Iter 2906, loss = 1.1608133\n",
            "Iter 2907, loss = 0.6467354\n",
            "Iter 2908, loss = 0.6327264\n",
            "Iter 2909, loss = 0.8471178\n",
            "Iter 2910, loss = 0.9273942\n",
            "Iter 2911, loss = 1.193111\n",
            "Iter 2912, loss = 1.0211334\n",
            "Iter 2913, loss = 0.961542\n",
            "Iter 2914, loss = 0.8344737\n",
            "Iter 2915, loss = 0.8602246\n",
            "Iter 2916, loss = 0.98629725\n",
            "Iter 2917, loss = 0.8292618\n",
            "Iter 2918, loss = 0.95728755\n",
            "Iter 2919, loss = 0.8056328\n",
            "Iter 2920, loss = 0.8047124\n",
            "Iter 2921, loss = 0.97701555\n",
            "Iter 2922, loss = 0.93588454\n",
            "Iter 2923, loss = 0.85415757\n",
            "Iter 2924, loss = 1.0770359\n",
            "Iter 2925, loss = 0.66023654\n",
            "Iter 2926, loss = 0.9435665\n",
            "Iter 2927, loss = 0.5042689\n",
            "Iter 2928, loss = 1.1342491\n",
            "Iter 2929, loss = 0.87440735\n",
            "Iter 2930, loss = 1.1209112\n",
            "Iter 2931, loss = 0.7227901\n",
            "Iter 2932, loss = 0.9162544\n",
            "Iter 2933, loss = 1.1733577\n",
            "Iter 2934, loss = 0.7513271\n",
            "Iter 2935, loss = 0.6169894\n",
            "Iter 2936, loss = 0.52242875\n",
            "Iter 2937, loss = 0.7759794\n",
            "Iter 2938, loss = 0.9673643\n",
            "Iter 2939, loss = 1.3518064\n",
            "Iter 2940, loss = 0.6332707\n",
            "Iter 2941, loss = 0.8559351\n",
            "Iter 2942, loss = 0.85333073\n",
            "Iter 2943, loss = 1.0420072\n",
            "Iter 2944, loss = 0.64113265\n",
            "Iter 2945, loss = 1.1041671\n",
            "Iter 2946, loss = 0.8648882\n",
            "Iter 2947, loss = 0.93509173\n",
            "Iter 2948, loss = 0.8557918\n",
            "Iter 2949, loss = 0.7827598\n",
            "Iter 2950, loss = 0.8537363\n",
            "Iter 2951, loss = 0.83502614\n",
            "Iter 2952, loss = 0.7924123\n",
            "Iter 2953, loss = 0.5371529\n",
            "Iter 2954, loss = 0.7837655\n",
            "Iter 2955, loss = 0.8908049\n",
            "Iter 2956, loss = 0.6182779\n",
            "Iter 2957, loss = 0.6955326\n",
            "Iter 2958, loss = 1.0023509\n",
            "Iter 2959, loss = 0.681371\n",
            "Iter 2960, loss = 0.6309149\n",
            "Iter 2961, loss = 0.821241\n",
            "Iter 2962, loss = 0.6804471\n",
            "Iter 2963, loss = 1.0969677\n",
            "Iter 2964, loss = 0.8961625\n",
            "Iter 2965, loss = 0.9059755\n",
            "Iter 2966, loss = 0.706107\n",
            "Iter 2967, loss = 0.80731755\n",
            "Iter 2968, loss = 1.0923474\n",
            "Iter 2969, loss = 0.76814663\n",
            "Iter 2970, loss = 0.79703265\n",
            "Iter 2971, loss = 0.705453\n",
            "Iter 2972, loss = 1.3174303\n",
            "Iter 2973, loss = 1.0362874\n",
            "Iter 2974, loss = 0.8772141\n",
            "Iter 2975, loss = 0.76002157\n",
            "Iter 2976, loss = 1.1338212\n",
            "Iter 2977, loss = 0.6917897\n",
            "Iter 2978, loss = 0.7735757\n",
            "Iter 2979, loss = 1.0240452\n",
            "Iter 2980, loss = 0.8663359\n",
            "Iter 2981, loss = 1.0617846\n",
            "Iter 2982, loss = 0.8481719\n",
            "Iter 2983, loss = 0.7546739\n",
            "Iter 2984, loss = 0.7578652\n",
            "Iter 2985, loss = 0.6983001\n",
            "Iter 2986, loss = 0.8949119\n",
            "Iter 2987, loss = 0.97427565\n",
            "Iter 2988, loss = 0.8194381\n",
            "Iter 2989, loss = 0.9333247\n",
            "Iter 2990, loss = 0.797868\n",
            "Iter 2991, loss = 0.8237772\n",
            "Iter 2992, loss = 0.7871679\n",
            "Iter 2993, loss = 0.7553483\n",
            "Iter 2994, loss = 0.798723\n",
            "Iter 2995, loss = 0.66938436\n",
            "Iter 2996, loss = 0.68762517\n",
            "Iter 2997, loss = 0.80485433\n",
            "Iter 2998, loss = 0.67049295\n",
            "Iter 2999, loss = 0.9970583\n",
            "Iter 3000, loss = 0.8221549\n",
            "Iter 3001, loss = 0.8680309\n",
            "Iter 3002, loss = 0.72241765\n",
            "Iter 3003, loss = 0.7074489\n",
            "Iter 3004, loss = 0.56029195\n",
            "Iter 3005, loss = 0.9439869\n",
            "Iter 3006, loss = 0.96126425\n",
            "Iter 3007, loss = 0.943812\n",
            "Iter 3008, loss = 0.9074827\n",
            "Iter 3009, loss = 0.7634026\n",
            "Iter 3010, loss = 0.7886109\n",
            "Iter 3011, loss = 0.8513691\n",
            "Iter 3012, loss = 0.9187443\n",
            "Iter 3013, loss = 0.8469465\n",
            "Iter 3014, loss = 0.6129706\n",
            "Iter 3015, loss = 0.781857\n",
            "Iter 3016, loss = 0.76001704\n",
            "Iter 3017, loss = 0.7012708\n",
            "Iter 3018, loss = 0.7143444\n",
            "Iter 3019, loss = 0.8156832\n",
            "Iter 3020, loss = 0.93652177\n",
            "Iter 3021, loss = 1.0295111\n",
            "Iter 3022, loss = 0.93024284\n",
            "Iter 3023, loss = 0.5459037\n",
            "Iter 3024, loss = 1.000257\n",
            "Iter 3025, loss = 0.70057726\n",
            "Iter 3026, loss = 0.56396496\n",
            "Iter 3027, loss = 0.6992384\n",
            "Iter 3028, loss = 0.83984506\n",
            "Iter 3029, loss = 1.1083779\n",
            "Iter 3030, loss = 0.9018951\n",
            "Iter 3031, loss = 1.0050905\n",
            "Iter 3032, loss = 0.97152364\n",
            "Iter 3033, loss = 0.7718001\n",
            "Iter 3034, loss = 0.6273091\n",
            "Iter 3035, loss = 1.0493277\n",
            "Iter 3036, loss = 0.85168266\n",
            "Iter 3037, loss = 1.1375562\n",
            "Iter 3038, loss = 0.8817985\n",
            "Iter 3039, loss = 0.88213086\n",
            "Iter 3040, loss = 0.8644758\n",
            "Iter 3041, loss = 0.8762474\n",
            "Iter 3042, loss = 0.90114117\n",
            "Iter 3043, loss = 0.82559943\n",
            "Iter 3044, loss = 0.6786295\n",
            "Iter 3045, loss = 0.9059912\n",
            "Iter 3046, loss = 0.9739756\n",
            "Iter 3047, loss = 0.6898074\n",
            "Iter 3048, loss = 0.9351263\n",
            "Iter 3049, loss = 0.64531386\n",
            "Iter 3050, loss = 0.54374003\n",
            "Iter 3051, loss = 1.2374706\n",
            "Iter 3052, loss = 0.9488876\n",
            "Iter 3053, loss = 0.92497814\n",
            "Iter 3054, loss = 0.79460037\n",
            "Iter 3055, loss = 0.7574901\n",
            "Iter 3056, loss = 0.920452\n",
            "Iter 3057, loss = 0.73476326\n",
            "Iter 3058, loss = 0.82063496\n",
            "Iter 3059, loss = 1.0472667\n",
            "Iter 3060, loss = 0.7508823\n",
            "Iter 3061, loss = 0.8746171\n",
            "Iter 3062, loss = 1.0036124\n",
            "Iter 3063, loss = 0.87405837\n",
            "Iter 3064, loss = 0.7284982\n",
            "Iter 3065, loss = 1.0443095\n",
            "Iter 3066, loss = 0.774526\n",
            "Iter 3067, loss = 0.8130107\n",
            "Iter 3068, loss = 0.78930813\n",
            "Iter 3069, loss = 0.74165565\n",
            "Iter 3070, loss = 0.87256557\n",
            "Iter 3071, loss = 0.83556443\n",
            "Iter 3072, loss = 0.9997144\n",
            "Iter 3073, loss = 1.0226521\n",
            "Iter 3074, loss = 0.7779882\n",
            "Iter 3075, loss = 0.87538105\n",
            "Iter 3076, loss = 0.7972641\n",
            "Iter 3077, loss = 0.6905617\n",
            "Iter 3078, loss = 0.8100735\n",
            "Iter 3079, loss = 0.79320765\n",
            "Iter 3080, loss = 0.8377333\n",
            "Iter 3081, loss = 0.68726313\n",
            "Iter 3082, loss = 1.0815474\n",
            "Iter 3083, loss = 0.9195753\n",
            "Iter 3084, loss = 0.7751114\n",
            "Iter 3085, loss = 0.8560823\n",
            "Iter 3086, loss = 0.60638344\n",
            "Iter 3087, loss = 1.2517817\n",
            "Iter 3088, loss = 0.64250386\n",
            "Iter 3089, loss = 0.6578162\n",
            "Iter 3090, loss = 1.020879\n",
            "Iter 3091, loss = 0.7882868\n",
            "Iter 3092, loss = 0.5514999\n",
            "Iter 3093, loss = 0.50928205\n",
            "Iter 3094, loss = 0.7255107\n",
            "Iter 3095, loss = 1.1731474\n",
            "Iter 3096, loss = 1.1424352\n",
            "Iter 3097, loss = 0.6562953\n",
            "Iter 3098, loss = 0.9479747\n",
            "Iter 3099, loss = 0.8592249\n",
            "Iter 3100, loss = 0.73316497\n",
            "Iter 3101, loss = 1.0234549\n",
            "Iter 3102, loss = 0.7453178\n",
            "Iter 3103, loss = 0.5611944\n",
            "Iter 3104, loss = 0.8087677\n",
            "Iter 3105, loss = 0.78445166\n",
            "Iter 3106, loss = 0.77923536\n",
            "Iter 3107, loss = 1.1202755\n",
            "Iter 3108, loss = 0.85012174\n",
            "Iter 3109, loss = 1.0172492\n",
            "Iter 3110, loss = 0.74049675\n",
            "Iter 3111, loss = 0.62343657\n",
            "Iter 3112, loss = 0.7298867\n",
            "Iter 3113, loss = 0.62378705\n",
            "Iter 3114, loss = 1.0157068\n",
            "Iter 3115, loss = 0.80894315\n",
            "Iter 3116, loss = 0.8104731\n",
            "Iter 3117, loss = 0.79234946\n",
            "Iter 3118, loss = 1.0790384\n",
            "Iter 3119, loss = 0.9103529\n",
            "Iter 3120, loss = 1.0987532\n",
            "Iter 3121, loss = 0.665272\n",
            "Iter 3122, loss = 1.2083735\n",
            "Iter 3123, loss = 0.8665322\n",
            "Iter 3124, loss = 0.9965936\n",
            "Iter 3125, loss = 1.1020343\n",
            "Iter 3126, loss = 0.8102084\n",
            "Iter 3127, loss = 0.6367854\n",
            "Iter 3128, loss = 0.613019\n",
            "Iter 3129, loss = 1.1046093\n",
            "Iter 3130, loss = 0.48728865\n",
            "Iter 3131, loss = 1.0791979\n",
            "Iter 3132, loss = 1.0086488\n",
            "Iter 3133, loss = 0.6643102\n",
            "Iter 3134, loss = 0.9993522\n",
            "Iter 3135, loss = 0.8066596\n",
            "Iter 3136, loss = 0.40371922\n",
            "Iter 3137, loss = 0.98019946\n",
            "Iter 3138, loss = 0.6711526\n",
            "Iter 3139, loss = 0.8931366\n",
            "Iter 3140, loss = 0.8623489\n",
            "Iter 3141, loss = 0.95889974\n",
            "Iter 3142, loss = 0.8673329\n",
            "Iter 3143, loss = 0.9101562\n",
            "Iter 3144, loss = 0.99053514\n",
            "Iter 3145, loss = 0.8792824\n",
            "Iter 3146, loss = 0.5936698\n",
            "Iter 3147, loss = 0.7936007\n",
            "Iter 3148, loss = 0.78809834\n",
            "Iter 3149, loss = 0.8150859\n",
            "Iter 3150, loss = 0.7185364\n",
            "Iter 3151, loss = 1.0023029\n",
            "Iter 3152, loss = 1.0818372\n",
            "Iter 3153, loss = 0.6939899\n",
            "Iter 3154, loss = 0.7755649\n",
            "Iter 3155, loss = 0.5969544\n",
            "Iter 3156, loss = 1.0559084\n",
            "Iter 3157, loss = 0.875776\n",
            "Iter 3158, loss = 0.8221359\n",
            "Iter 3159, loss = 0.79643816\n",
            "Iter 3160, loss = 0.9568394\n",
            "Iter 3161, loss = 0.59686327\n",
            "Iter 3162, loss = 0.6806155\n",
            "Iter 3163, loss = 0.87895274\n",
            "Iter 3164, loss = 0.78687125\n",
            "Iter 3165, loss = 0.9808718\n",
            "Iter 3166, loss = 0.73463905\n",
            "Iter 3167, loss = 0.8207133\n",
            "Iter 3168, loss = 1.1581974\n",
            "Iter 3169, loss = 0.856736\n",
            "Iter 3170, loss = 0.6096116\n",
            "Iter 3171, loss = 0.957659\n",
            "Iter 3172, loss = 0.8368416\n",
            "Iter 3173, loss = 0.9235923\n",
            "Iter 3174, loss = 0.73509353\n",
            "Iter 3175, loss = 0.86073875\n",
            "Iter 3176, loss = 0.8735961\n",
            "Iter 3177, loss = 0.8302308\n",
            "Iter 3178, loss = 0.936735\n",
            "Iter 3179, loss = 0.79833066\n",
            "Iter 3180, loss = 0.7865958\n",
            "Iter 3181, loss = 0.83387893\n",
            "Iter 3182, loss = 0.64284754\n",
            "Iter 3183, loss = 0.9858258\n",
            "Iter 3184, loss = 0.6326119\n",
            "Iter 3185, loss = 0.80235595\n",
            "Iter 3186, loss = 0.6724604\n",
            "Iter 3187, loss = 0.75608265\n",
            "Iter 3188, loss = 0.6936128\n",
            "Iter 3189, loss = 0.84445804\n",
            "Iter 3190, loss = 0.8477943\n",
            "Iter 3191, loss = 0.72889453\n",
            "Iter 3192, loss = 0.68066037\n",
            "Iter 3193, loss = 0.99694216\n",
            "Iter 3194, loss = 0.98564863\n",
            "Iter 3195, loss = 0.87204456\n",
            "Iter 3196, loss = 0.75931627\n",
            "Iter 3197, loss = 0.71926934\n",
            "Iter 3198, loss = 1.0347692\n",
            "Iter 3199, loss = 0.92671967\n",
            "Iter 3200, loss = 0.94550705\n",
            "Iter 3201, loss = 0.90185165\n",
            "Iter 3202, loss = 0.82608366\n",
            "Iter 3203, loss = 0.6782598\n",
            "Iter 3204, loss = 0.9730632\n",
            "Iter 3205, loss = 1.0136722\n",
            "Iter 3206, loss = 0.7008278\n",
            "Iter 3207, loss = 0.89255536\n",
            "Iter 3208, loss = 0.6631117\n",
            "Iter 3209, loss = 0.721125\n",
            "Iter 3210, loss = 0.7088719\n",
            "Iter 3211, loss = 0.7078465\n",
            "Iter 3212, loss = 0.91039324\n",
            "Iter 3213, loss = 0.79575896\n",
            "Iter 3214, loss = 0.7419526\n",
            "Iter 3215, loss = 0.7266861\n",
            "Iter 3216, loss = 1.2169908\n",
            "Iter 3217, loss = 1.0261796\n",
            "Iter 3218, loss = 0.7866986\n",
            "Iter 3219, loss = 0.81816804\n",
            "Iter 3220, loss = 0.8767919\n",
            "Iter 3221, loss = 0.9803586\n",
            "Iter 3222, loss = 0.95383817\n",
            "Iter 3223, loss = 0.70021445\n",
            "Iter 3224, loss = 0.7369462\n",
            "Iter 3225, loss = 0.8154953\n",
            "Iter 3226, loss = 0.7960342\n",
            "Iter 3227, loss = 0.8548148\n",
            "Iter 3228, loss = 0.78379744\n",
            "Iter 3229, loss = 0.9039152\n",
            "Iter 3230, loss = 1.1350069\n",
            "Iter 3231, loss = 0.59839636\n",
            "Iter 3232, loss = 0.9955815\n",
            "Iter 3233, loss = 1.0750805\n",
            "Iter 3234, loss = 0.8090559\n",
            "Iter 3235, loss = 0.82975864\n",
            "Iter 3236, loss = 0.9049351\n",
            "Iter 3237, loss = 0.6603319\n",
            "Iter 3238, loss = 0.697437\n",
            "Iter 3239, loss = 1.146783\n",
            "Iter 3240, loss = 0.87170076\n",
            "Iter 3241, loss = 0.63388276\n",
            "Iter 3242, loss = 0.8176299\n",
            "Iter 3243, loss = 0.7063113\n",
            "Iter 3244, loss = 0.55379534\n",
            "Iter 3245, loss = 1.0090368\n",
            "Iter 3246, loss = 0.86492336\n",
            "Iter 3247, loss = 0.7164447\n",
            "Iter 3248, loss = 0.9575787\n",
            "Iter 3249, loss = 0.7510699\n",
            "Iter 3250, loss = 1.0022416\n",
            "Iter 3251, loss = 0.7636293\n",
            "Iter 3252, loss = 0.75898033\n",
            "Iter 3253, loss = 0.6656287\n",
            "Iter 3254, loss = 0.7652354\n",
            "Iter 3255, loss = 0.6632408\n",
            "Iter 3256, loss = 1.0154715\n",
            "Iter 3257, loss = 0.90848345\n",
            "Iter 3258, loss = 0.7384365\n",
            "Iter 3259, loss = 0.79835767\n",
            "Iter 3260, loss = 0.95636624\n",
            "Iter 3261, loss = 0.93276095\n",
            "Iter 3262, loss = 1.0324873\n",
            "Iter 3263, loss = 0.7917098\n",
            "Iter 3264, loss = 0.7381221\n",
            "Iter 3265, loss = 1.0065479\n",
            "Iter 3266, loss = 0.68008\n",
            "Iter 3267, loss = 0.6630392\n",
            "Iter 3268, loss = 0.68855155\n",
            "Iter 3269, loss = 0.8221446\n",
            "Iter 3270, loss = 0.6817841\n",
            "Iter 3271, loss = 0.65348226\n",
            "Iter 3272, loss = 1.0092375\n",
            "Iter 3273, loss = 0.7184453\n",
            "Iter 3274, loss = 0.813975\n",
            "Iter 3275, loss = 0.83481365\n",
            "Iter 3276, loss = 0.8772509\n",
            "Iter 3277, loss = 0.81070566\n",
            "Iter 3278, loss = 0.85329616\n",
            "Iter 3279, loss = 0.53343195\n",
            "Iter 3280, loss = 0.8118232\n",
            "Iter 3281, loss = 0.7825067\n",
            "Iter 3282, loss = 0.5138259\n",
            "Iter 3283, loss = 0.7230375\n",
            "Iter 3284, loss = 0.910415\n",
            "Iter 3285, loss = 0.7869901\n",
            "Iter 3286, loss = 1.0136787\n",
            "Iter 3287, loss = 0.9258678\n",
            "Iter 3288, loss = 1.0129608\n",
            "Iter 3289, loss = 0.789451\n",
            "Iter 3290, loss = 1.1297853\n",
            "Iter 3291, loss = 0.7309514\n",
            "Iter 3292, loss = 0.75998867\n",
            "Iter 3293, loss = 0.73543406\n",
            "Iter 3294, loss = 0.7695592\n",
            "Iter 3295, loss = 0.67553973\n",
            "Iter 3296, loss = 1.1254637\n",
            "Iter 3297, loss = 0.8426533\n",
            "Iter 3298, loss = 0.63337696\n",
            "Iter 3299, loss = 0.94320136\n",
            "Iter 3300, loss = 1.1477559\n",
            "Iter 3301, loss = 1.0622193\n",
            "Iter 3302, loss = 0.88958275\n",
            "Iter 3303, loss = 0.80728173\n",
            "Iter 3304, loss = 0.7580993\n",
            "Iter 3305, loss = 0.95745707\n",
            "Iter 3306, loss = 1.1305366\n",
            "Iter 3307, loss = 0.7747792\n",
            "Iter 3308, loss = 0.7517408\n",
            "Iter 3309, loss = 0.97356594\n",
            "Iter 3310, loss = 0.86349523\n",
            "Iter 3311, loss = 1.286993\n",
            "Iter 3312, loss = 0.77382106\n",
            "Iter 3313, loss = 0.8988216\n",
            "Iter 3314, loss = 0.86716545\n",
            "Iter 3315, loss = 0.8134502\n",
            "Iter 3316, loss = 0.9073818\n",
            "Iter 3317, loss = 0.880671\n",
            "Iter 3318, loss = 0.99368757\n",
            "Iter 3319, loss = 0.5812341\n",
            "Iter 3320, loss = 0.78186446\n",
            "Iter 3321, loss = 0.7767931\n",
            "Iter 3322, loss = 0.89645207\n",
            "Iter 3323, loss = 0.8844696\n",
            "Iter 3324, loss = 1.0411577\n",
            "Iter 3325, loss = 0.7020907\n",
            "Iter 3326, loss = 0.71375453\n",
            "Iter 3327, loss = 0.54980457\n",
            "Iter 3328, loss = 0.87798816\n",
            "Iter 3329, loss = 0.57347316\n",
            "Iter 3330, loss = 0.8928297\n",
            "Iter 3331, loss = 0.84713733\n",
            "Iter 3332, loss = 0.6557967\n",
            "Iter 3333, loss = 0.94265115\n",
            "Iter 3334, loss = 0.7006729\n",
            "Iter 3335, loss = 0.80228114\n",
            "Iter 3336, loss = 0.629201\n",
            "Iter 3337, loss = 0.86183846\n",
            "Iter 3338, loss = 0.4688349\n",
            "Iter 3339, loss = 1.0999432\n",
            "Iter 3340, loss = 0.8775579\n",
            "Iter 3341, loss = 0.92090106\n",
            "Iter 3342, loss = 0.71115625\n",
            "Iter 3343, loss = 0.7528988\n",
            "Iter 3344, loss = 0.9080435\n",
            "Iter 3345, loss = 0.7657293\n",
            "Iter 3346, loss = 0.57057357\n",
            "Iter 3347, loss = 0.8758557\n",
            "Iter 3348, loss = 0.8260041\n",
            "Iter 3349, loss = 0.7199948\n",
            "Iter 3350, loss = 0.7879483\n",
            "Iter 3351, loss = 1.0029294\n",
            "Iter 3352, loss = 0.97488344\n",
            "Iter 3353, loss = 0.82493854\n",
            "Iter 3354, loss = 1.08283\n",
            "Iter 3355, loss = 0.7723142\n",
            "Iter 3356, loss = 0.7287083\n",
            "Iter 3357, loss = 0.6450145\n",
            "Iter 3358, loss = 0.9185465\n",
            "Iter 3359, loss = 1.0446165\n",
            "Iter 3360, loss = 0.71140337\n",
            "Iter 3361, loss = 0.848717\n",
            "Iter 3362, loss = 0.98792684\n",
            "Iter 3363, loss = 0.60685503\n",
            "Iter 3364, loss = 0.6011903\n",
            "Iter 3365, loss = 0.9121307\n",
            "Iter 3366, loss = 0.7884224\n",
            "Iter 3367, loss = 0.8186498\n",
            "Iter 3368, loss = 0.6359072\n",
            "Iter 3369, loss = 1.0553594\n",
            "Iter 3370, loss = 0.8623425\n",
            "Iter 3371, loss = 0.785967\n",
            "Iter 3372, loss = 1.0590162\n",
            "Iter 3373, loss = 0.87164164\n",
            "Iter 3374, loss = 0.7746744\n",
            "Iter 3375, loss = 0.8175199\n",
            "Iter 3376, loss = 1.0127654\n",
            "Iter 3377, loss = 0.7708262\n",
            "Iter 3378, loss = 0.7829495\n",
            "Iter 3379, loss = 0.77701485\n",
            "Iter 3380, loss = 1.0156755\n",
            "Iter 3381, loss = 0.8617638\n",
            "Iter 3382, loss = 0.81812775\n",
            "Iter 3383, loss = 0.7186956\n",
            "Iter 3384, loss = 0.62886333\n",
            "Iter 3385, loss = 0.66013956\n",
            "Iter 3386, loss = 0.6722922\n",
            "Iter 3387, loss = 1.063633\n",
            "Iter 3388, loss = 1.2479988\n",
            "Iter 3389, loss = 0.8622078\n",
            "Iter 3390, loss = 0.95843333\n",
            "Iter 3391, loss = 0.8886012\n",
            "Iter 3392, loss = 0.8310827\n",
            "Iter 3393, loss = 1.0710641\n",
            "Iter 3394, loss = 0.9491514\n",
            "Iter 3395, loss = 0.94915575\n",
            "Iter 3396, loss = 0.8257854\n",
            "Iter 3397, loss = 0.63756824\n",
            "Iter 3398, loss = 0.6092158\n",
            "Iter 3399, loss = 1.0625688\n",
            "Iter 3400, loss = 0.6590718\n",
            "Iter 3401, loss = 1.1446888\n",
            "Iter 3402, loss = 0.772572\n",
            "Iter 3403, loss = 0.70866853\n",
            "Iter 3404, loss = 0.9762089\n",
            "Iter 3405, loss = 0.71640515\n",
            "Iter 3406, loss = 0.8770747\n",
            "Iter 3407, loss = 0.94319504\n",
            "Iter 3408, loss = 0.64154625\n",
            "Iter 3409, loss = 0.82464725\n",
            "Iter 3410, loss = 0.80648005\n",
            "Iter 3411, loss = 1.0664227\n",
            "Iter 3412, loss = 0.9138671\n",
            "Iter 3413, loss = 0.93602216\n",
            "Iter 3414, loss = 0.8921744\n",
            "Iter 3415, loss = 0.7572261\n",
            "Iter 3416, loss = 0.8584427\n",
            "Iter 3417, loss = 0.816695\n",
            "Iter 3418, loss = 0.83405817\n",
            "Iter 3419, loss = 0.54423624\n",
            "Iter 3420, loss = 0.7996429\n",
            "Iter 3421, loss = 1.1037519\n",
            "Iter 3422, loss = 1.1048911\n",
            "Iter 3423, loss = 0.6484213\n",
            "Iter 3424, loss = 0.86341894\n",
            "Iter 3425, loss = 0.78100157\n",
            "Iter 3426, loss = 0.88892937\n",
            "Iter 3427, loss = 0.63047045\n",
            "Iter 3428, loss = 0.75155336\n",
            "Iter 3429, loss = 0.80573374\n",
            "Iter 3430, loss = 0.7062431\n",
            "Iter 3431, loss = 0.66411495\n",
            "Iter 3432, loss = 0.9295646\n",
            "Iter 3433, loss = 0.8097693\n",
            "Iter 3434, loss = 0.63085353\n",
            "Iter 3435, loss = 0.69298625\n",
            "Iter 3436, loss = 0.72872394\n",
            "Iter 3437, loss = 0.6597162\n",
            "Iter 3438, loss = 0.72656894\n",
            "Iter 3439, loss = 1.270292\n",
            "Iter 3440, loss = 0.9227718\n",
            "Iter 3441, loss = 0.8168049\n",
            "Iter 3442, loss = 0.79190946\n",
            "Iter 3443, loss = 0.73349655\n",
            "Iter 3444, loss = 0.52262944\n",
            "Iter 3445, loss = 1.0096786\n",
            "Iter 3446, loss = 0.99263525\n",
            "Iter 3447, loss = 0.72596025\n",
            "Iter 3448, loss = 0.8707733\n",
            "Iter 3449, loss = 0.74426603\n",
            "Iter 3450, loss = 0.7349454\n",
            "Iter 3451, loss = 1.1648197\n",
            "Iter 3452, loss = 0.96677417\n",
            "Iter 3453, loss = 0.59393656\n",
            "Iter 3454, loss = 0.5164223\n",
            "Iter 3455, loss = 0.69835204\n",
            "Iter 3456, loss = 0.65891904\n",
            "Iter 3457, loss = 1.4549907\n",
            "Iter 3458, loss = 0.82643676\n",
            "Iter 3459, loss = 0.8499438\n",
            "Iter 3460, loss = 0.75976384\n",
            "Iter 3461, loss = 0.81573045\n",
            "Iter 3462, loss = 0.6799294\n",
            "Iter 3463, loss = 0.62881505\n",
            "Iter 3464, loss = 0.8503171\n",
            "Iter 3465, loss = 0.70827687\n",
            "Iter 3466, loss = 0.7229186\n",
            "Iter 3467, loss = 0.696846\n",
            "Iter 3468, loss = 1.0701493\n",
            "Iter 3469, loss = 0.84901154\n",
            "Iter 3470, loss = 0.9957849\n",
            "Iter 3471, loss = 0.89764297\n",
            "Iter 3472, loss = 0.6445068\n",
            "Iter 3473, loss = 0.9867901\n",
            "Iter 3474, loss = 0.5923386\n",
            "Iter 3475, loss = 0.6960531\n",
            "Iter 3476, loss = 0.68368804\n",
            "Iter 3477, loss = 0.8350301\n",
            "Iter 3478, loss = 1.0835894\n",
            "Iter 3479, loss = 0.74224687\n",
            "Iter 3480, loss = 0.76822317\n",
            "Iter 3481, loss = 0.97678286\n",
            "Iter 3482, loss = 1.0663064\n",
            "Iter 3483, loss = 1.0451154\n",
            "Iter 3484, loss = 0.74580663\n",
            "Iter 3485, loss = 0.78196144\n",
            "Iter 3486, loss = 0.9371711\n",
            "Iter 3487, loss = 0.960153\n",
            "Iter 3488, loss = 0.84717417\n",
            "Iter 3489, loss = 1.1745925\n",
            "Iter 3490, loss = 0.8245225\n",
            "Iter 3491, loss = 0.8056582\n",
            "Iter 3492, loss = 0.75441253\n",
            "Iter 3493, loss = 0.8854803\n",
            "Iter 3494, loss = 0.7363237\n",
            "Iter 3495, loss = 0.781524\n",
            "Iter 3496, loss = 0.900247\n",
            "Iter 3497, loss = 0.93638587\n",
            "Iter 3498, loss = 0.83736414\n",
            "Iter 3499, loss = 0.8387207\n",
            "Iter 3500, loss = 0.63535094\n",
            "Iter 3501, loss = 0.7641104\n",
            "Iter 3502, loss = 0.9289117\n",
            "Iter 3503, loss = 0.7187103\n",
            "Iter 3504, loss = 0.99582374\n",
            "Iter 3505, loss = 0.8611624\n",
            "Iter 3506, loss = 0.986392\n",
            "Iter 3507, loss = 0.8040862\n",
            "Iter 3508, loss = 0.8188143\n",
            "Iter 3509, loss = 0.6928863\n",
            "Iter 3510, loss = 0.8911749\n",
            "Iter 3511, loss = 0.7719604\n",
            "Iter 3512, loss = 0.6826256\n",
            "Iter 3513, loss = 0.76295227\n",
            "Iter 3514, loss = 0.7723464\n",
            "Iter 3515, loss = 1.0364673\n",
            "Iter 3516, loss = 0.8012383\n",
            "Iter 3517, loss = 1.0259525\n",
            "Iter 3518, loss = 0.7684671\n",
            "Iter 3519, loss = 0.9688296\n",
            "Iter 3520, loss = 0.6532965\n",
            "Iter 3521, loss = 0.79779\n",
            "Iter 3522, loss = 0.75878114\n",
            "Iter 3523, loss = 0.53820956\n",
            "Iter 3524, loss = 0.7797385\n",
            "Iter 3525, loss = 1.3978992\n",
            "Iter 3526, loss = 0.9510561\n",
            "Iter 3527, loss = 0.6325052\n",
            "Iter 3528, loss = 1.2508745\n",
            "Iter 3529, loss = 0.53217494\n",
            "Iter 3530, loss = 0.76194835\n",
            "Iter 3531, loss = 0.6242056\n",
            "Iter 3532, loss = 0.7732383\n",
            "Iter 3533, loss = 0.87894464\n",
            "Iter 3534, loss = 1.0190725\n",
            "Iter 3535, loss = 0.87068474\n",
            "Iter 3536, loss = 0.758402\n",
            "Iter 3537, loss = 0.7870344\n",
            "Iter 3538, loss = 1.080458\n",
            "Iter 3539, loss = 0.61423826\n",
            "Iter 3540, loss = 0.6340494\n",
            "Iter 3541, loss = 0.5350386\n",
            "Iter 3542, loss = 0.8317783\n",
            "Iter 3543, loss = 0.77151144\n",
            "Iter 3544, loss = 0.85576725\n",
            "Iter 3545, loss = 0.64983034\n",
            "Iter 3546, loss = 1.1497549\n",
            "Iter 3547, loss = 0.78843784\n",
            "Iter 3548, loss = 0.8861959\n",
            "Iter 3549, loss = 0.73555267\n",
            "Iter 3550, loss = 0.6552838\n",
            "Iter 3551, loss = 0.9009214\n",
            "Iter 3552, loss = 0.9544638\n",
            "Iter 3553, loss = 0.7548442\n",
            "Iter 3554, loss = 0.82253814\n",
            "Iter 3555, loss = 0.83542645\n",
            "Iter 3556, loss = 1.0513552\n",
            "Iter 3557, loss = 0.7562662\n",
            "Iter 3558, loss = 0.8706719\n",
            "Iter 3559, loss = 1.3615437\n",
            "Iter 3560, loss = 0.9188479\n",
            "Iter 3561, loss = 0.8077154\n",
            "Iter 3562, loss = 0.8303945\n",
            "Iter 3563, loss = 0.9070425\n",
            "Iter 3564, loss = 1.0483253\n",
            "Iter 3565, loss = 0.8154205\n",
            "Iter 3566, loss = 0.8142915\n",
            "Iter 3567, loss = 1.0873643\n",
            "Iter 3568, loss = 0.886884\n",
            "Iter 3569, loss = 0.80617774\n",
            "Iter 3570, loss = 1.1570318\n",
            "Iter 3571, loss = 0.6645803\n",
            "Iter 3572, loss = 0.6763139\n",
            "Iter 3573, loss = 0.62725383\n",
            "Iter 3574, loss = 0.95357215\n",
            "Iter 3575, loss = 0.9591633\n",
            "Iter 3576, loss = 0.6173595\n",
            "Iter 3577, loss = 0.6702392\n",
            "Iter 3578, loss = 0.96616644\n",
            "Iter 3579, loss = 0.87987924\n",
            "Iter 3580, loss = 0.71818984\n",
            "Iter 3581, loss = 0.87598324\n",
            "Iter 3582, loss = 0.9953964\n",
            "Iter 3583, loss = 0.74524796\n",
            "Iter 3584, loss = 1.1459377\n",
            "Iter 3585, loss = 0.7007396\n",
            "Iter 3586, loss = 0.8143428\n",
            "Iter 3587, loss = 1.1265503\n",
            "Iter 3588, loss = 0.9543016\n",
            "Iter 3589, loss = 0.8687662\n",
            "Iter 3590, loss = 0.8030998\n",
            "Iter 3591, loss = 0.75177383\n",
            "Iter 3592, loss = 1.2586586\n",
            "Iter 3593, loss = 0.85057884\n",
            "Iter 3594, loss = 0.7847471\n",
            "Iter 3595, loss = 0.6548379\n",
            "Iter 3596, loss = 0.8331351\n",
            "Iter 3597, loss = 0.62776285\n",
            "Iter 3598, loss = 0.9105168\n",
            "Iter 3599, loss = 0.7462079\n",
            "Iter 3600, loss = 0.8550168\n",
            "Iter 3601, loss = 0.8274901\n",
            "Iter 3602, loss = 0.6714473\n",
            "Iter 3603, loss = 0.73181885\n",
            "Iter 3604, loss = 0.646225\n",
            "Iter 3605, loss = 0.8167265\n",
            "Iter 3606, loss = 1.1141303\n",
            "Iter 3607, loss = 0.84870327\n",
            "Iter 3608, loss = 0.8503791\n",
            "Iter 3609, loss = 0.83480823\n",
            "Iter 3610, loss = 0.60355103\n",
            "Iter 3611, loss = 0.9331195\n",
            "Iter 3612, loss = 0.91691643\n",
            "Iter 3613, loss = 0.75585145\n",
            "Iter 3614, loss = 0.780686\n",
            "Iter 3615, loss = 0.66474164\n",
            "Iter 3616, loss = 0.7211571\n",
            "Iter 3617, loss = 0.79907924\n",
            "Iter 3618, loss = 1.1142453\n",
            "Iter 3619, loss = 1.0571429\n",
            "Iter 3620, loss = 0.61388326\n",
            "Iter 3621, loss = 0.8753552\n",
            "Iter 3622, loss = 1.0494428\n",
            "Iter 3623, loss = 0.7290321\n",
            "Iter 3624, loss = 1.0237181\n",
            "Iter 3625, loss = 0.8996248\n",
            "Iter 3626, loss = 1.2775223\n",
            "Iter 3627, loss = 0.7937305\n",
            "Iter 3628, loss = 0.68281895\n",
            "Iter 3629, loss = 0.48859265\n",
            "Iter 3630, loss = 0.9854891\n",
            "Iter 3631, loss = 0.8764638\n",
            "Iter 3632, loss = 0.8030717\n",
            "Iter 3633, loss = 0.479065\n",
            "Iter 3634, loss = 0.9319711\n",
            "Iter 3635, loss = 0.7516868\n",
            "Iter 3636, loss = 0.7298915\n",
            "Iter 3637, loss = 0.60542274\n",
            "Iter 3638, loss = 0.99675775\n",
            "Iter 3639, loss = 0.7308848\n",
            "Iter 3640, loss = 0.846375\n",
            "Iter 3641, loss = 0.8690238\n",
            "Iter 3642, loss = 1.0120406\n",
            "Iter 3643, loss = 0.6593279\n",
            "Iter 3644, loss = 0.6502915\n",
            "Iter 3645, loss = 0.968269\n",
            "Iter 3646, loss = 1.0815582\n",
            "Iter 3647, loss = 0.9129982\n",
            "Iter 3648, loss = 0.7840545\n",
            "Iter 3649, loss = 0.9459022\n",
            "Iter 3650, loss = 0.873193\n",
            "Iter 3651, loss = 0.7364639\n",
            "Iter 3652, loss = 0.6687317\n",
            "Iter 3653, loss = 0.7837417\n",
            "Iter 3654, loss = 0.9280283\n",
            "Iter 3655, loss = 0.92264926\n",
            "Iter 3656, loss = 0.6405541\n",
            "Iter 3657, loss = 0.7854737\n",
            "Iter 3658, loss = 0.7185043\n",
            "Iter 3659, loss = 0.93289876\n",
            "Iter 3660, loss = 0.9228189\n",
            "Iter 3661, loss = 1.0051391\n",
            "Iter 3662, loss = 0.8231218\n",
            "Iter 3663, loss = 0.5489236\n",
            "Iter 3664, loss = 1.2500361\n",
            "Iter 3665, loss = 0.73374724\n",
            "Iter 3666, loss = 0.9090974\n",
            "Iter 3667, loss = 0.9186424\n",
            "Iter 3668, loss = 0.77031183\n",
            "Iter 3669, loss = 0.9480431\n",
            "Iter 3670, loss = 0.8960877\n",
            "Iter 3671, loss = 0.86604863\n",
            "Iter 3672, loss = 0.66275215\n",
            "Iter 3673, loss = 1.0591491\n",
            "Iter 3674, loss = 0.85007\n",
            "Iter 3675, loss = 0.7417853\n",
            "Iter 3676, loss = 0.91627735\n",
            "Iter 3677, loss = 0.5480485\n",
            "Iter 3678, loss = 0.7325835\n",
            "Iter 3679, loss = 0.65987444\n",
            "Iter 3680, loss = 0.9075277\n",
            "Iter 3681, loss = 0.8439134\n",
            "Iter 3682, loss = 0.90214884\n",
            "Iter 3683, loss = 0.9766535\n",
            "Iter 3684, loss = 1.0797162\n",
            "Iter 3685, loss = 0.9026679\n",
            "Iter 3686, loss = 0.71055996\n",
            "Iter 3687, loss = 0.63249254\n",
            "Iter 3688, loss = 0.952235\n",
            "Iter 3689, loss = 0.9018483\n",
            "Iter 3690, loss = 0.80807304\n",
            "Iter 3691, loss = 0.69306386\n",
            "Iter 3692, loss = 0.58089983\n",
            "Iter 3693, loss = 0.75204444\n",
            "Iter 3694, loss = 1.0762309\n",
            "Iter 3695, loss = 0.8676475\n",
            "Iter 3696, loss = 1.0709968\n",
            "Iter 3697, loss = 0.81915617\n",
            "Iter 3698, loss = 0.96682155\n",
            "Iter 3699, loss = 0.5801674\n",
            "Iter 3700, loss = 0.9914537\n",
            "Iter 3701, loss = 0.980812\n",
            "Iter 3702, loss = 1.0923538\n",
            "Iter 3703, loss = 0.6695804\n",
            "Iter 3704, loss = 0.74312544\n",
            "Iter 3705, loss = 1.0359129\n",
            "Iter 3706, loss = 1.0147675\n",
            "Iter 3707, loss = 0.9531293\n",
            "Iter 3708, loss = 0.60133886\n",
            "Iter 3709, loss = 1.0103469\n",
            "Iter 3710, loss = 0.80035937\n",
            "Iter 3711, loss = 1.0123671\n",
            "Iter 3712, loss = 0.55733895\n",
            "Iter 3713, loss = 0.6152495\n",
            "Iter 3714, loss = 1.1063329\n",
            "Iter 3715, loss = 0.9809295\n",
            "Iter 3716, loss = 0.7661915\n",
            "Iter 3717, loss = 0.7351154\n",
            "Iter 3718, loss = 0.6750651\n",
            "Iter 3719, loss = 0.8810675\n",
            "Iter 3720, loss = 0.6599829\n",
            "Iter 3721, loss = 0.97932744\n",
            "Iter 3722, loss = 0.9039117\n",
            "Iter 3723, loss = 0.95293397\n",
            "Iter 3724, loss = 0.89353126\n",
            "Iter 3725, loss = 0.8863441\n",
            "Iter 3726, loss = 0.85336936\n",
            "Iter 3727, loss = 0.8116741\n",
            "Iter 3728, loss = 0.75559574\n",
            "Iter 3729, loss = 0.6871346\n",
            "Iter 3730, loss = 0.9475638\n",
            "Iter 3731, loss = 0.7802859\n",
            "Iter 3732, loss = 1.0824997\n",
            "Iter 3733, loss = 0.4850334\n",
            "Iter 3734, loss = 0.67855155\n",
            "Iter 3735, loss = 0.6389282\n",
            "Iter 3736, loss = 1.476731\n",
            "Iter 3737, loss = 0.8345157\n",
            "Iter 3738, loss = 0.7009443\n",
            "Iter 3739, loss = 0.8387841\n",
            "Iter 3740, loss = 0.5178822\n",
            "Iter 3741, loss = 0.94973874\n",
            "Iter 3742, loss = 0.5674859\n",
            "Iter 3743, loss = 1.0747482\n",
            "Iter 3744, loss = 0.9469613\n",
            "Iter 3745, loss = 0.73063207\n",
            "Iter 3746, loss = 0.85296667\n",
            "Iter 3747, loss = 0.88504124\n",
            "Iter 3748, loss = 0.9536192\n",
            "Iter 3749, loss = 0.71905446\n",
            "Iter 3750, loss = 0.7434492\n",
            "Iter 3751, loss = 0.6151971\n",
            "Iter 3752, loss = 0.76356757\n",
            "Iter 3753, loss = 0.539909\n",
            "Iter 3754, loss = 1.343249\n",
            "Iter 3755, loss = 0.73108405\n",
            "Iter 3756, loss = 0.615229\n",
            "Iter 3757, loss = 1.0425558\n",
            "Iter 3758, loss = 0.5451941\n",
            "Iter 3759, loss = 0.89775777\n",
            "Iter 3760, loss = 0.53507984\n",
            "Iter 3761, loss = 0.68850935\n",
            "Iter 3762, loss = 0.5924444\n",
            "Iter 3763, loss = 0.96346474\n",
            "Iter 3764, loss = 0.98875505\n",
            "Iter 3765, loss = 0.79143727\n",
            "Iter 3766, loss = 0.8767135\n",
            "Iter 3767, loss = 0.89275944\n",
            "Iter 3768, loss = 0.7165333\n",
            "Iter 3769, loss = 0.9689\n",
            "Iter 3770, loss = 1.2298518\n",
            "Iter 3771, loss = 0.7188041\n",
            "Iter 3772, loss = 0.964668\n",
            "Iter 3773, loss = 0.63552433\n",
            "Iter 3774, loss = 0.9292544\n",
            "Iter 3775, loss = 0.5953229\n",
            "Iter 3776, loss = 0.9705432\n",
            "Iter 3777, loss = 0.9174657\n",
            "Iter 3778, loss = 0.96333027\n",
            "Iter 3779, loss = 0.6966684\n",
            "Iter 3780, loss = 0.80535287\n",
            "Iter 3781, loss = 0.6950206\n",
            "Iter 3782, loss = 0.7986536\n",
            "Iter 3783, loss = 0.6336291\n",
            "Iter 3784, loss = 0.85108775\n",
            "Iter 3785, loss = 0.40528566\n",
            "Iter 3786, loss = 0.8084142\n",
            "Iter 3787, loss = 0.78275526\n",
            "Iter 3788, loss = 0.822227\n",
            "Iter 3789, loss = 0.6927296\n",
            "Iter 3790, loss = 0.58111954\n",
            "Iter 3791, loss = 0.5185392\n",
            "Iter 3792, loss = 1.1833208\n",
            "Iter 3793, loss = 0.9286349\n",
            "Iter 3794, loss = 0.9719483\n",
            "Iter 3795, loss = 0.93055284\n",
            "Iter 3796, loss = 0.8730315\n",
            "Iter 3797, loss = 0.667995\n",
            "Iter 3798, loss = 0.72083986\n",
            "Iter 3799, loss = 1.0782251\n",
            "Iter 3800, loss = 1.0821817\n",
            "Iter 3801, loss = 0.829527\n",
            "Iter 3802, loss = 0.7745683\n",
            "Iter 3803, loss = 1.01723\n",
            "Iter 3804, loss = 0.7980056\n",
            "Iter 3805, loss = 0.7391752\n",
            "Iter 3806, loss = 1.1856828\n",
            "Iter 3807, loss = 0.9985982\n",
            "Iter 3808, loss = 0.8083124\n",
            "Iter 3809, loss = 0.61210555\n",
            "Iter 3810, loss = 1.0825093\n",
            "Iter 3811, loss = 0.95412266\n",
            "Iter 3812, loss = 0.7068417\n",
            "Iter 3813, loss = 0.70919335\n",
            "Iter 3814, loss = 0.6643951\n",
            "Iter 3815, loss = 0.47454605\n",
            "Iter 3816, loss = 0.7805054\n",
            "Iter 3817, loss = 0.89732397\n",
            "Iter 3818, loss = 0.94570327\n",
            "Iter 3819, loss = 0.8110049\n",
            "Iter 3820, loss = 0.75685275\n",
            "Iter 3821, loss = 0.76260483\n",
            "Iter 3822, loss = 0.8839296\n",
            "Iter 3823, loss = 0.73079395\n",
            "Iter 3824, loss = 0.83628184\n",
            "Iter 3825, loss = 1.1912813\n",
            "Iter 3826, loss = 0.7893347\n",
            "Iter 3827, loss = 0.9387336\n",
            "Iter 3828, loss = 0.8291184\n",
            "Iter 3829, loss = 1.1075677\n",
            "Iter 3830, loss = 1.0441267\n",
            "Iter 3831, loss = 0.8198304\n",
            "Iter 3832, loss = 1.0803719\n",
            "Iter 3833, loss = 0.82268006\n",
            "Iter 3834, loss = 0.5046027\n",
            "Iter 3835, loss = 1.017102\n",
            "Iter 3836, loss = 0.94050664\n",
            "Iter 3837, loss = 0.6957168\n",
            "Iter 3838, loss = 0.8787899\n",
            "Iter 3839, loss = 0.66254884\n",
            "Iter 3840, loss = 0.95411247\n",
            "Iter 3841, loss = 0.6227591\n",
            "Iter 3842, loss = 0.7606057\n",
            "Iter 3843, loss = 0.7369322\n",
            "Iter 3844, loss = 0.71717983\n",
            "Iter 3845, loss = 0.7152625\n",
            "Iter 3846, loss = 1.2187684\n",
            "Iter 3847, loss = 0.9222933\n",
            "Iter 3848, loss = 0.73866963\n",
            "Iter 3849, loss = 0.60589755\n",
            "Iter 3850, loss = 1.05529\n",
            "Iter 3851, loss = 0.8676276\n",
            "Iter 3852, loss = 0.81056744\n",
            "Iter 3853, loss = 0.83730114\n",
            "Iter 3854, loss = 0.91174513\n",
            "Iter 3855, loss = 0.66599804\n",
            "Iter 3856, loss = 0.773638\n",
            "Iter 3857, loss = 0.83930814\n",
            "Iter 3858, loss = 0.55020577\n",
            "Iter 3859, loss = 0.52133465\n",
            "Iter 3860, loss = 0.8868186\n",
            "Iter 3861, loss = 0.77288365\n",
            "Iter 3862, loss = 0.7258119\n",
            "Iter 3863, loss = 1.0050244\n",
            "Iter 3864, loss = 1.2900164\n",
            "Iter 3865, loss = 0.77207804\n",
            "Iter 3866, loss = 1.0431418\n",
            "Iter 3867, loss = 0.96052\n",
            "Iter 3868, loss = 0.6874801\n",
            "Iter 3869, loss = 1.0907935\n",
            "Iter 3870, loss = 0.74429685\n",
            "Iter 3871, loss = 0.66475755\n",
            "Iter 3872, loss = 0.9023718\n",
            "Iter 3873, loss = 1.0912883\n",
            "Iter 3874, loss = 0.92366743\n",
            "Iter 3875, loss = 0.70338035\n",
            "Iter 3876, loss = 1.1146697\n",
            "Iter 3877, loss = 0.83441806\n",
            "Iter 3878, loss = 0.76647234\n",
            "Iter 3879, loss = 0.6307526\n",
            "Iter 3880, loss = 0.6725286\n",
            "Iter 3881, loss = 0.85731196\n",
            "Iter 3882, loss = 0.91842854\n",
            "Iter 3883, loss = 0.88490605\n",
            "Iter 3884, loss = 0.84290266\n",
            "Iter 3885, loss = 0.80167377\n",
            "Iter 3886, loss = 0.63258964\n",
            "Iter 3887, loss = 0.93103933\n",
            "Iter 3888, loss = 0.78565985\n",
            "Iter 3889, loss = 0.8544772\n",
            "Iter 3890, loss = 0.74446213\n",
            "Iter 3891, loss = 0.89617586\n",
            "Iter 3892, loss = 0.89661175\n",
            "Iter 3893, loss = 0.62422514\n",
            "Iter 3894, loss = 0.9969072\n",
            "Iter 3895, loss = 0.65729177\n",
            "Iter 3896, loss = 1.1284353\n",
            "Iter 3897, loss = 0.77979326\n",
            "Iter 3898, loss = 0.5600759\n",
            "Iter 3899, loss = 0.55350685\n",
            "Iter 3900, loss = 1.0452766\n",
            "Iter 3901, loss = 0.5595745\n",
            "Iter 3902, loss = 0.7225568\n",
            "Iter 3903, loss = 0.616079\n",
            "Iter 3904, loss = 0.6939975\n",
            "Iter 3905, loss = 1.0105062\n",
            "Iter 3906, loss = 0.86333895\n",
            "Iter 3907, loss = 0.7604028\n",
            "Iter 3908, loss = 0.7704667\n",
            "Iter 3909, loss = 0.78391135\n",
            "Iter 3910, loss = 0.5870574\n",
            "Iter 3911, loss = 1.0804633\n",
            "Iter 3912, loss = 0.74628836\n",
            "Iter 3913, loss = 0.87763023\n",
            "Iter 3914, loss = 0.7376441\n",
            "Iter 3915, loss = 0.8401233\n",
            "Iter 3916, loss = 0.61528826\n",
            "Iter 3917, loss = 0.53049034\n",
            "Iter 3918, loss = 0.62535596\n",
            "Iter 3919, loss = 1.3965681\n",
            "Iter 3920, loss = 0.76741827\n",
            "Iter 3921, loss = 1.1520557\n",
            "Iter 3922, loss = 0.9604252\n",
            "Iter 3923, loss = 0.89485174\n",
            "Iter 3924, loss = 0.9052658\n",
            "Iter 3925, loss = 0.6126834\n",
            "Iter 3926, loss = 0.6740135\n",
            "Iter 3927, loss = 0.97257286\n",
            "Iter 3928, loss = 1.0945988\n",
            "Iter 3929, loss = 0.67440206\n",
            "Iter 3930, loss = 0.76242137\n",
            "Iter 3931, loss = 0.79495764\n",
            "Iter 3932, loss = 0.79432094\n",
            "Iter 3933, loss = 0.8253668\n",
            "Iter 3934, loss = 0.81038225\n",
            "Iter 3935, loss = 0.80644137\n",
            "Iter 3936, loss = 0.7579055\n",
            "Iter 3937, loss = 0.6468301\n",
            "Iter 3938, loss = 0.8654622\n",
            "Iter 3939, loss = 1.250386\n",
            "Iter 3940, loss = 0.86324203\n",
            "Iter 3941, loss = 0.9838811\n",
            "Iter 3942, loss = 0.9686802\n",
            "Iter 3943, loss = 0.7438899\n",
            "Iter 3944, loss = 0.9259172\n",
            "Iter 3945, loss = 0.696298\n",
            "Iter 3946, loss = 0.7415899\n",
            "Iter 3947, loss = 0.7881268\n",
            "Iter 3948, loss = 0.52719676\n",
            "Iter 3949, loss = 0.7820668\n",
            "Iter 3950, loss = 0.79437166\n",
            "Iter 3951, loss = 0.8397288\n",
            "Iter 3952, loss = 0.7663349\n",
            "Iter 3953, loss = 1.1581854\n",
            "Iter 3954, loss = 0.9891052\n",
            "Iter 3955, loss = 0.9374186\n",
            "Iter 3956, loss = 0.7864911\n",
            "Iter 3957, loss = 0.72855806\n",
            "Iter 3958, loss = 1.0431756\n",
            "Iter 3959, loss = 0.9171814\n",
            "Iter 3960, loss = 1.0022067\n",
            "Iter 3961, loss = 0.8175615\n",
            "Iter 3962, loss = 0.979475\n",
            "Iter 3963, loss = 0.85179925\n",
            "Iter 3964, loss = 0.78813744\n",
            "Iter 3965, loss = 0.75849855\n",
            "Iter 3966, loss = 0.81410956\n",
            "Iter 3967, loss = 0.7990528\n",
            "Iter 3968, loss = 0.8153795\n",
            "Iter 3969, loss = 0.779899\n",
            "Iter 3970, loss = 1.2226385\n",
            "Iter 3971, loss = 1.1754308\n",
            "Iter 3972, loss = 0.681219\n",
            "Iter 3973, loss = 1.0361178\n",
            "Iter 3974, loss = 0.86618006\n",
            "Iter 3975, loss = 0.75892735\n",
            "Iter 3976, loss = 0.6452751\n",
            "Iter 3977, loss = 0.744576\n",
            "Iter 3978, loss = 0.88081884\n",
            "Iter 3979, loss = 0.5830749\n",
            "Iter 3980, loss = 1.139491\n",
            "Iter 3981, loss = 0.7592081\n",
            "Iter 3982, loss = 0.8505415\n",
            "Iter 3983, loss = 0.9391118\n",
            "Iter 3984, loss = 0.59324586\n",
            "Iter 3985, loss = 0.85013545\n",
            "Iter 3986, loss = 1.0488517\n",
            "Iter 3987, loss = 0.7774975\n",
            "Iter 3988, loss = 0.7463082\n",
            "Iter 3989, loss = 1.0048927\n",
            "Iter 3990, loss = 0.96600366\n",
            "Iter 3991, loss = 0.9825746\n",
            "Iter 3992, loss = 0.5458554\n",
            "Iter 3993, loss = 1.0667042\n",
            "Iter 3994, loss = 0.68288237\n",
            "Iter 3995, loss = 0.71924007\n",
            "Iter 3996, loss = 0.682192\n",
            "Iter 3997, loss = 0.8883443\n",
            "Iter 3998, loss = 0.9401205\n",
            "Iter 3999, loss = 0.77991086\n",
            "Iter 4000, loss = 0.6566387\n",
            "Iter 4001, loss = 0.8209175\n",
            "Iter 4002, loss = 0.8484612\n",
            "Iter 4003, loss = 0.49044153\n",
            "Iter 4004, loss = 0.7446968\n",
            "Iter 4005, loss = 0.91838485\n",
            "Iter 4006, loss = 0.79901147\n",
            "Iter 4007, loss = 0.8275793\n",
            "Iter 4008, loss = 0.6416973\n",
            "Iter 4009, loss = 0.8255039\n",
            "Iter 4010, loss = 0.6545129\n",
            "Iter 4011, loss = 0.8228493\n",
            "Iter 4012, loss = 0.86776984\n",
            "Iter 4013, loss = 0.8158451\n",
            "Iter 4014, loss = 0.73044485\n",
            "Iter 4015, loss = 0.6520101\n",
            "Iter 4016, loss = 0.7788191\n",
            "Iter 4017, loss = 0.68678087\n",
            "Iter 4018, loss = 0.9951511\n",
            "Iter 4019, loss = 0.8527224\n",
            "Iter 4020, loss = 0.72626936\n",
            "Iter 4021, loss = 1.3963895\n",
            "Iter 4022, loss = 0.7070649\n",
            "Iter 4023, loss = 0.5970042\n",
            "Iter 4024, loss = 0.7876386\n",
            "Iter 4025, loss = 0.89065725\n",
            "Iter 4026, loss = 0.57452536\n",
            "Iter 4027, loss = 0.8918352\n",
            "Iter 4028, loss = 0.8145591\n",
            "Iter 4029, loss = 0.9316235\n",
            "Iter 4030, loss = 0.55900955\n",
            "Iter 4031, loss = 0.8295143\n",
            "Iter 4032, loss = 0.9192665\n",
            "Iter 4033, loss = 0.8934252\n",
            "Iter 4034, loss = 0.676101\n",
            "Iter 4035, loss = 0.9806421\n",
            "Iter 4036, loss = 0.7616118\n",
            "Iter 4037, loss = 0.93363047\n",
            "Iter 4038, loss = 1.0089597\n",
            "Iter 4039, loss = 0.60327476\n",
            "Iter 4040, loss = 0.88069093\n",
            "Iter 4041, loss = 0.5562721\n",
            "Iter 4042, loss = 0.98989326\n",
            "Iter 4043, loss = 1.060575\n",
            "Iter 4044, loss = 0.62504256\n",
            "Iter 4045, loss = 0.7928599\n",
            "Iter 4046, loss = 0.97955275\n",
            "Iter 4047, loss = 0.9984673\n",
            "Iter 4048, loss = 0.5355136\n",
            "Iter 4049, loss = 1.0259566\n",
            "Iter 4050, loss = 0.90558565\n",
            "Iter 4051, loss = 0.7111579\n",
            "Iter 4052, loss = 0.8780128\n",
            "Iter 4053, loss = 0.82825613\n",
            "Iter 4054, loss = 0.83568716\n",
            "Iter 4055, loss = 0.5977949\n",
            "Iter 4056, loss = 0.6613956\n",
            "Iter 4057, loss = 0.6009909\n",
            "Iter 4058, loss = 0.5135121\n",
            "Iter 4059, loss = 1.0890927\n",
            "Iter 4060, loss = 0.5392403\n",
            "Iter 4061, loss = 0.6890862\n",
            "Iter 4062, loss = 1.2062583\n",
            "Iter 4063, loss = 0.71296525\n",
            "Iter 4064, loss = 0.75277346\n",
            "Iter 4065, loss = 0.8171586\n",
            "Iter 4066, loss = 0.9667167\n",
            "Iter 4067, loss = 0.75082505\n",
            "Iter 4068, loss = 0.8321433\n",
            "Iter 4069, loss = 0.8793494\n",
            "Iter 4070, loss = 0.79260254\n",
            "Iter 4071, loss = 0.69829804\n",
            "Iter 4072, loss = 0.96708393\n",
            "Iter 4073, loss = 0.9927094\n",
            "Iter 4074, loss = 0.71485645\n",
            "Iter 4075, loss = 0.6846689\n",
            "Iter 4076, loss = 0.9622483\n",
            "Iter 4077, loss = 0.78026235\n",
            "Iter 4078, loss = 0.82222307\n",
            "Iter 4079, loss = 0.8936621\n",
            "Iter 4080, loss = 0.87396365\n",
            "Iter 4081, loss = 0.96886885\n",
            "Iter 4082, loss = 1.0354614\n",
            "Iter 4083, loss = 0.69454706\n",
            "Iter 4084, loss = 0.79828197\n",
            "Iter 4085, loss = 0.73452556\n",
            "Iter 4086, loss = 0.9547821\n",
            "Iter 4087, loss = 0.64400685\n",
            "Iter 4088, loss = 0.9267192\n",
            "Iter 4089, loss = 1.1254776\n",
            "Iter 4090, loss = 0.8131211\n",
            "Iter 4091, loss = 0.7293566\n",
            "Iter 4092, loss = 0.85727084\n",
            "Iter 4093, loss = 0.9982697\n",
            "Iter 4094, loss = 0.6034714\n",
            "Iter 4095, loss = 0.6086763\n",
            "Iter 4096, loss = 0.79209536\n",
            "Iter 4097, loss = 0.99144113\n",
            "Iter 4098, loss = 0.74323726\n",
            "Iter 4099, loss = 0.9399396\n",
            "Iter 4100, loss = 0.8178905\n",
            "Iter 4101, loss = 0.7673918\n",
            "Iter 4102, loss = 0.59529495\n",
            "Iter 4103, loss = 0.8785294\n",
            "Iter 4104, loss = 0.7440736\n",
            "Iter 4105, loss = 0.8528075\n",
            "Iter 4106, loss = 0.7237768\n",
            "Iter 4107, loss = 1.0663456\n",
            "Iter 4108, loss = 0.9556886\n",
            "Iter 4109, loss = 0.89954484\n",
            "Iter 4110, loss = 0.8744414\n",
            "Iter 4111, loss = 0.7250613\n",
            "Iter 4112, loss = 0.88291997\n",
            "Iter 4113, loss = 0.6442113\n",
            "Iter 4114, loss = 0.868436\n",
            "Iter 4115, loss = 0.76621115\n",
            "Iter 4116, loss = 1.0668609\n",
            "Iter 4117, loss = 0.90692955\n",
            "Iter 4118, loss = 0.57384646\n",
            "Iter 4119, loss = 0.7000061\n",
            "Iter 4120, loss = 0.9282807\n",
            "Iter 4121, loss = 0.71425045\n",
            "Iter 4122, loss = 0.7784554\n",
            "Iter 4123, loss = 0.86670667\n",
            "Iter 4124, loss = 0.75720584\n",
            "Iter 4125, loss = 0.6990286\n",
            "Iter 4126, loss = 0.9010136\n",
            "Iter 4127, loss = 0.8899726\n",
            "Iter 4128, loss = 0.7192888\n",
            "Iter 4129, loss = 0.72104007\n",
            "Iter 4130, loss = 1.0565556\n",
            "Iter 4131, loss = 0.7571298\n",
            "Iter 4132, loss = 0.80738\n",
            "Iter 4133, loss = 0.9396688\n",
            "Iter 4134, loss = 0.9216735\n",
            "Iter 4135, loss = 0.71053797\n",
            "Iter 4136, loss = 1.0471852\n",
            "Iter 4137, loss = 0.7266701\n",
            "Iter 4138, loss = 0.6795169\n",
            "Iter 4139, loss = 1.0093982\n",
            "Iter 4140, loss = 0.7845123\n",
            "Iter 4141, loss = 0.83473766\n",
            "Iter 4142, loss = 1.0703431\n",
            "Iter 4143, loss = 0.85967934\n",
            "Iter 4144, loss = 0.83233225\n",
            "Iter 4145, loss = 0.6373787\n",
            "Iter 4146, loss = 0.6109623\n",
            "Iter 4147, loss = 1.078717\n",
            "Iter 4148, loss = 0.88567007\n",
            "Iter 4149, loss = 0.69946146\n",
            "Iter 4150, loss = 0.87820876\n",
            "Iter 4151, loss = 0.8626678\n",
            "Iter 4152, loss = 0.63881326\n",
            "Iter 4153, loss = 0.6337578\n",
            "Iter 4154, loss = 0.5953909\n",
            "Iter 4155, loss = 1.2562026\n",
            "Iter 4156, loss = 0.7764585\n",
            "Iter 4157, loss = 1.0270281\n",
            "Iter 4158, loss = 0.8613956\n",
            "Iter 4159, loss = 0.7862313\n",
            "Iter 4160, loss = 0.7997629\n",
            "Iter 4161, loss = 0.7734748\n",
            "Iter 4162, loss = 0.88967735\n",
            "Iter 4163, loss = 0.89824176\n",
            "Iter 4164, loss = 0.5883328\n",
            "Iter 4165, loss = 0.7108258\n",
            "Iter 4166, loss = 0.4882082\n",
            "Iter 4167, loss = 1.1648569\n",
            "Iter 4168, loss = 1.0557361\n",
            "Iter 4169, loss = 1.0455269\n",
            "Iter 4170, loss = 0.7705845\n",
            "Iter 4171, loss = 0.91632193\n",
            "Iter 4172, loss = 0.7941072\n",
            "Iter 4173, loss = 0.75188416\n",
            "Iter 4174, loss = 0.75163996\n",
            "Iter 4175, loss = 0.8151988\n",
            "Iter 4176, loss = 0.79923105\n",
            "Iter 4177, loss = 0.95900667\n",
            "Iter 4178, loss = 0.8555351\n",
            "Iter 4179, loss = 0.7142817\n",
            "Iter 4180, loss = 0.9194171\n",
            "Iter 4181, loss = 1.0747325\n",
            "Iter 4182, loss = 0.8509752\n",
            "Iter 4183, loss = 0.7974407\n",
            "Iter 4184, loss = 0.626227\n",
            "Iter 4185, loss = 1.0422751\n",
            "Iter 4186, loss = 0.89058757\n",
            "Iter 4187, loss = 0.89575744\n",
            "Iter 4188, loss = 0.86669856\n",
            "Iter 4189, loss = 0.99458057\n",
            "Iter 4190, loss = 0.6656355\n",
            "Iter 4191, loss = 0.7312777\n",
            "Iter 4192, loss = 1.3837576\n",
            "Iter 4193, loss = 0.76489544\n",
            "Iter 4194, loss = 1.0941806\n",
            "Iter 4195, loss = 0.81426466\n",
            "Iter 4196, loss = 0.9163058\n",
            "Iter 4197, loss = 0.72662365\n",
            "Iter 4198, loss = 0.9420873\n",
            "Iter 4199, loss = 1.2433158\n",
            "Iter 4200, loss = 0.87623334\n",
            "Iter 4201, loss = 0.71647483\n",
            "Iter 4202, loss = 0.97838515\n",
            "Iter 4203, loss = 0.8645568\n",
            "Iter 4204, loss = 0.59660727\n",
            "Iter 4205, loss = 0.9299489\n",
            "Iter 4206, loss = 0.78272885\n",
            "Iter 4207, loss = 0.78449416\n",
            "Iter 4208, loss = 0.8792672\n",
            "Iter 4209, loss = 1.0010521\n",
            "Iter 4210, loss = 0.79333544\n",
            "Iter 4211, loss = 0.9782272\n",
            "Iter 4212, loss = 0.7364576\n",
            "Iter 4213, loss = 0.98041785\n",
            "Iter 4214, loss = 1.0201021\n",
            "Iter 4215, loss = 0.64381564\n",
            "Iter 4216, loss = 0.5339997\n",
            "Iter 4217, loss = 0.64114267\n",
            "Iter 4218, loss = 0.8205206\n",
            "Iter 4219, loss = 1.1312728\n",
            "Iter 4220, loss = 0.8702359\n",
            "Iter 4221, loss = 0.72938854\n",
            "Iter 4222, loss = 0.588696\n",
            "Iter 4223, loss = 0.8944792\n",
            "Iter 4224, loss = 0.8798571\n",
            "Iter 4225, loss = 0.8834182\n",
            "Iter 4226, loss = 0.9270514\n",
            "Iter 4227, loss = 0.7644127\n",
            "Iter 4228, loss = 0.7194498\n",
            "Iter 4229, loss = 0.88971305\n",
            "Iter 4230, loss = 1.0724924\n",
            "Iter 4231, loss = 0.74517465\n",
            "Iter 4232, loss = 0.9961995\n",
            "Iter 4233, loss = 0.8097596\n",
            "Iter 4234, loss = 0.809439\n",
            "Iter 4235, loss = 0.6968795\n",
            "Iter 4236, loss = 1.094997\n",
            "Iter 4237, loss = 0.6895667\n",
            "Iter 4238, loss = 0.7096933\n",
            "Iter 4239, loss = 0.74873614\n",
            "Iter 4240, loss = 0.7315129\n",
            "Iter 4241, loss = 0.4658567\n",
            "Iter 4242, loss = 0.7235555\n",
            "Iter 4243, loss = 1.0503626\n",
            "Iter 4244, loss = 1.1060541\n",
            "Iter 4245, loss = 0.92606246\n",
            "Iter 4246, loss = 0.80413795\n",
            "Iter 4247, loss = 0.72688997\n",
            "Iter 4248, loss = 0.6164086\n",
            "Iter 4249, loss = 0.8041496\n",
            "Iter 4250, loss = 0.78874195\n",
            "Iter 4251, loss = 1.2061033\n",
            "Iter 4252, loss = 0.79177356\n",
            "Iter 4253, loss = 0.600925\n",
            "Iter 4254, loss = 0.87855947\n",
            "Iter 4255, loss = 0.7968684\n",
            "Iter 4256, loss = 0.49202573\n",
            "Iter 4257, loss = 0.90415233\n",
            "Iter 4258, loss = 0.8978199\n",
            "Iter 4259, loss = 0.95818686\n",
            "Iter 4260, loss = 0.82572705\n",
            "Iter 4261, loss = 0.79829013\n",
            "Iter 4262, loss = 0.95095444\n",
            "Iter 4263, loss = 0.90325654\n",
            "Iter 4264, loss = 0.75219965\n",
            "Iter 4265, loss = 0.9042293\n",
            "Iter 4266, loss = 0.7950799\n",
            "Iter 4267, loss = 0.85324085\n",
            "Iter 4268, loss = 0.7328377\n",
            "Iter 4269, loss = 0.60867226\n",
            "Iter 4270, loss = 0.5095641\n",
            "Iter 4271, loss = 0.6648282\n",
            "Iter 4272, loss = 0.9273377\n",
            "Iter 4273, loss = 0.79887193\n",
            "Iter 4274, loss = 0.81766975\n",
            "Iter 4275, loss = 0.5068176\n",
            "Iter 4276, loss = 0.63449514\n",
            "Iter 4277, loss = 1.1888747\n",
            "Iter 4278, loss = 0.63579893\n",
            "Iter 4279, loss = 0.85739326\n",
            "Iter 4280, loss = 0.5842378\n",
            "Iter 4281, loss = 0.90360236\n",
            "Iter 4282, loss = 1.0632348\n",
            "Iter 4283, loss = 0.99434376\n",
            "Iter 4284, loss = 0.846609\n",
            "Iter 4285, loss = 1.0526705\n",
            "Iter 4286, loss = 0.87528\n",
            "Iter 4287, loss = 0.737306\n",
            "Iter 4288, loss = 0.77030003\n",
            "Iter 4289, loss = 0.94122887\n",
            "Iter 4290, loss = 1.0299227\n",
            "Iter 4291, loss = 0.7241095\n",
            "Iter 4292, loss = 0.7719529\n",
            "Iter 4293, loss = 0.5928731\n",
            "Iter 4294, loss = 1.0110492\n",
            "Iter 4295, loss = 0.8916024\n",
            "Iter 4296, loss = 0.9963199\n",
            "Iter 4297, loss = 0.9331958\n",
            "Iter 4298, loss = 0.8916205\n",
            "Iter 4299, loss = 0.84939\n",
            "Iter 4300, loss = 0.87671924\n",
            "Iter 4301, loss = 0.94125146\n",
            "Iter 4302, loss = 0.65024817\n",
            "Iter 4303, loss = 1.1472228\n",
            "Iter 4304, loss = 0.59783113\n",
            "Iter 4305, loss = 0.9477975\n",
            "Iter 4306, loss = 1.0790129\n",
            "Iter 4307, loss = 0.81957865\n",
            "Iter 4308, loss = 0.98047805\n",
            "Iter 4309, loss = 1.0287488\n",
            "Iter 4310, loss = 1.1106472\n",
            "Iter 4311, loss = 0.80002373\n",
            "Iter 4312, loss = 0.918015\n",
            "Iter 4313, loss = 0.7693112\n",
            "Iter 4314, loss = 0.5862171\n",
            "Iter 4315, loss = 0.78680074\n",
            "Iter 4316, loss = 0.9241959\n",
            "Iter 4317, loss = 1.210977\n",
            "Iter 4318, loss = 0.8921285\n",
            "Iter 4319, loss = 0.88597643\n",
            "Iter 4320, loss = 0.9506741\n",
            "Iter 4321, loss = 0.7725419\n",
            "Iter 4322, loss = 0.7765657\n",
            "Iter 4323, loss = 0.79037356\n",
            "Iter 4324, loss = 0.9955623\n",
            "Iter 4325, loss = 0.9335693\n",
            "Iter 4326, loss = 0.7786698\n",
            "Iter 4327, loss = 0.78709567\n",
            "Iter 4328, loss = 0.81971574\n",
            "Iter 4329, loss = 0.6253347\n",
            "Iter 4330, loss = 0.86319643\n",
            "Iter 4331, loss = 0.9314576\n",
            "Iter 4332, loss = 0.75822634\n",
            "Iter 4333, loss = 0.78476965\n",
            "Iter 4334, loss = 0.5742823\n",
            "Iter 4335, loss = 1.016973\n",
            "Iter 4336, loss = 0.83275104\n",
            "Iter 4337, loss = 0.71782446\n",
            "Iter 4338, loss = 0.86981493\n",
            "Iter 4339, loss = 0.8536736\n",
            "Iter 4340, loss = 0.7601838\n",
            "Iter 4341, loss = 0.89549726\n",
            "Iter 4342, loss = 0.8514777\n",
            "Iter 4343, loss = 0.7236296\n",
            "Iter 4344, loss = 0.50268286\n",
            "Iter 4345, loss = 0.99376994\n",
            "Iter 4346, loss = 0.99945045\n",
            "Iter 4347, loss = 0.6576279\n",
            "Iter 4348, loss = 0.9948801\n",
            "Iter 4349, loss = 0.5567999\n",
            "Iter 4350, loss = 0.89476275\n",
            "Iter 4351, loss = 0.90290976\n",
            "Iter 4352, loss = 0.8675907\n",
            "Iter 4353, loss = 0.98165965\n",
            "Iter 4354, loss = 0.73082346\n",
            "Iter 4355, loss = 0.8470671\n",
            "Iter 4356, loss = 1.0596694\n",
            "Iter 4357, loss = 0.68249714\n",
            "Iter 4358, loss = 0.7455473\n",
            "Iter 4359, loss = 0.9399667\n",
            "Iter 4360, loss = 0.52880317\n",
            "Iter 4361, loss = 0.59281003\n",
            "Iter 4362, loss = 1.0374875\n",
            "Iter 4363, loss = 0.90253055\n",
            "Iter 4364, loss = 0.70169723\n",
            "Iter 4365, loss = 0.9719817\n",
            "Iter 4366, loss = 1.0757127\n",
            "Iter 4367, loss = 0.78300244\n",
            "Iter 4368, loss = 0.8545114\n",
            "Iter 4369, loss = 1.1225436\n",
            "Iter 4370, loss = 0.9269004\n",
            "Iter 4371, loss = 0.9904516\n",
            "Iter 4372, loss = 0.7291837\n",
            "Iter 4373, loss = 0.5993235\n",
            "Iter 4374, loss = 0.77452004\n",
            "Iter 4375, loss = 0.6555998\n",
            "Iter 4376, loss = 1.0786904\n",
            "Iter 4377, loss = 1.0203066\n",
            "Iter 4378, loss = 0.7983698\n",
            "Iter 4379, loss = 0.7415564\n",
            "Iter 4380, loss = 1.0476518\n",
            "Iter 4381, loss = 0.7134979\n",
            "Iter 4382, loss = 0.5785217\n",
            "Iter 4383, loss = 0.6818858\n",
            "Iter 4384, loss = 0.7897933\n",
            "Iter 4385, loss = 0.7983481\n",
            "Iter 4386, loss = 0.7375847\n",
            "Iter 4387, loss = 0.9227898\n",
            "Iter 4388, loss = 0.92145807\n",
            "Iter 4389, loss = 0.70232594\n",
            "Iter 4390, loss = 0.66736054\n",
            "Iter 4391, loss = 0.89024675\n",
            "Iter 4392, loss = 0.80253243\n",
            "Iter 4393, loss = 0.9432429\n",
            "Iter 4394, loss = 0.60584354\n",
            "Iter 4395, loss = 1.0632224\n",
            "Iter 4396, loss = 0.70238733\n",
            "Iter 4397, loss = 0.7899299\n",
            "Iter 4398, loss = 0.6035291\n",
            "Iter 4399, loss = 0.8996649\n",
            "Iter 4400, loss = 0.6832483\n",
            "Iter 4401, loss = 0.83876026\n",
            "Iter 4402, loss = 0.8938628\n",
            "Iter 4403, loss = 0.93043137\n",
            "Iter 4404, loss = 0.7666953\n",
            "Iter 4405, loss = 0.6006943\n",
            "Iter 4406, loss = 0.85652345\n",
            "Iter 4407, loss = 0.8451736\n",
            "Iter 4408, loss = 0.88016754\n",
            "Iter 4409, loss = 0.91955405\n",
            "Iter 4410, loss = 0.602103\n",
            "Iter 4411, loss = 0.98553824\n",
            "Iter 4412, loss = 0.8114556\n",
            "Iter 4413, loss = 0.7948049\n",
            "Iter 4414, loss = 0.7848082\n",
            "Iter 4415, loss = 0.77228725\n",
            "Iter 4416, loss = 0.86827356\n",
            "Iter 4417, loss = 0.99323237\n",
            "Iter 4418, loss = 0.6373399\n",
            "Iter 4419, loss = 0.54383755\n",
            "Iter 4420, loss = 1.186058\n",
            "Iter 4421, loss = 1.2199305\n",
            "Iter 4422, loss = 0.86185086\n",
            "Iter 4423, loss = 0.87340206\n",
            "Iter 4424, loss = 0.76037425\n",
            "Iter 4425, loss = 0.83564484\n",
            "Iter 4426, loss = 0.59912765\n",
            "Iter 4427, loss = 0.57771456\n",
            "Iter 4428, loss = 0.9568075\n",
            "Iter 4429, loss = 0.5588434\n",
            "Iter 4430, loss = 0.72693205\n",
            "Iter 4431, loss = 1.0053546\n",
            "Iter 4432, loss = 0.77674246\n",
            "Iter 4433, loss = 0.70717496\n",
            "Iter 4434, loss = 0.779721\n",
            "Iter 4435, loss = 0.727569\n",
            "Iter 4436, loss = 0.86133784\n",
            "Iter 4437, loss = 0.87082666\n",
            "Iter 4438, loss = 0.8129311\n",
            "Iter 4439, loss = 0.83758557\n",
            "Iter 4440, loss = 0.98069024\n",
            "Iter 4441, loss = 1.0203898\n",
            "Iter 4442, loss = 0.8058561\n",
            "Iter 4443, loss = 0.8870676\n",
            "Iter 4444, loss = 0.6720028\n",
            "Iter 4445, loss = 0.8071987\n",
            "Iter 4446, loss = 0.9978031\n",
            "Iter 4447, loss = 1.1479686\n",
            "Iter 4448, loss = 0.9044616\n",
            "Iter 4449, loss = 0.8667836\n",
            "Iter 4450, loss = 0.6616173\n",
            "Iter 4451, loss = 1.0246261\n",
            "Iter 4452, loss = 0.93293333\n",
            "Iter 4453, loss = 0.7622421\n",
            "Iter 4454, loss = 0.61738336\n",
            "Iter 4455, loss = 0.7298323\n",
            "Iter 4456, loss = 0.8854997\n",
            "Iter 4457, loss = 0.7661851\n",
            "Iter 4458, loss = 0.85440016\n",
            "Iter 4459, loss = 0.61330116\n",
            "Iter 4460, loss = 0.74422836\n",
            "Iter 4461, loss = 1.2628729\n",
            "Iter 4462, loss = 0.7080729\n",
            "Iter 4463, loss = 0.89823896\n",
            "Iter 4464, loss = 0.7659323\n",
            "Iter 4465, loss = 0.57140505\n",
            "Iter 4466, loss = 0.88273966\n",
            "Iter 4467, loss = 0.98600626\n",
            "Iter 4468, loss = 0.8945509\n",
            "Iter 4469, loss = 0.6355269\n",
            "Iter 4470, loss = 0.580912\n",
            "Iter 4471, loss = 0.84079987\n",
            "Iter 4472, loss = 0.75630337\n",
            "Iter 4473, loss = 1.0744867\n",
            "Iter 4474, loss = 0.82282645\n",
            "Iter 4475, loss = 0.6083455\n",
            "Iter 4476, loss = 0.92858005\n",
            "Iter 4477, loss = 0.89867866\n",
            "Iter 4478, loss = 0.5850574\n",
            "Iter 4479, loss = 0.48757148\n",
            "Iter 4480, loss = 0.97321004\n",
            "Iter 4481, loss = 0.71974933\n",
            "Iter 4482, loss = 0.7512269\n",
            "Iter 4483, loss = 0.6671065\n",
            "Iter 4484, loss = 1.071487\n",
            "Iter 4485, loss = 1.2320058\n",
            "Iter 4486, loss = 0.80196893\n",
            "Iter 4487, loss = 0.5315709\n",
            "Iter 4488, loss = 0.909389\n",
            "Iter 4489, loss = 0.9526001\n",
            "Iter 4490, loss = 0.8418709\n",
            "Iter 4491, loss = 0.8459964\n",
            "Iter 4492, loss = 0.83260983\n",
            "Iter 4493, loss = 0.94288343\n",
            "Iter 4494, loss = 0.60136765\n",
            "Iter 4495, loss = 1.0306798\n",
            "Iter 4496, loss = 0.8260967\n",
            "Iter 4497, loss = 0.73845154\n",
            "Iter 4498, loss = 0.6786674\n",
            "Iter 4499, loss = 0.966516\n",
            "\n",
            "Optimization Finished\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL5FHi_de8rP",
        "colab_type": "text"
      },
      "source": [
        "# Creating Train, Validation, and Test set\n",
        "Randomly shuffling the complete dataset (not yet embedded with word2vec embeddings which was learned just now), and then splitting it into train, validation and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSwv7DbUpVff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffled_indices = np.arange(len(eng))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "shuffled_vectorized_eng = []\n",
        "shuffled_vectorized_beng = []\n",
        "\n",
        "for i in range(len(eng)):\n",
        "    shuffled_vectorized_eng.append(vectorized_eng[shuffled_indices[i]])\n",
        "    shuffled_vectorized_beng.append(vectorized_beng[shuffled_indices[i]])\n",
        "\n",
        "train_len = int(.75*len(eng))\n",
        "val_len = int(.15*len(eng))\n",
        "\n",
        "train_eng = shuffled_vectorized_eng[0:train_len]\n",
        "train_beng = shuffled_vectorized_beng[0:train_len]\n",
        "\n",
        "val_eng = shuffled_vectorized_eng[train_len:val_len]\n",
        "val_beng = shuffled_vectorized_beng[train_len:val_len]\n",
        "\n",
        "test_eng = shuffled_vectorized_eng[train_len+val_len:]\n",
        "test_beng = shuffled_vectorized_beng[train_len+val_len:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcmUwEYIfERy",
        "colab_type": "text"
      },
      "source": [
        "# Function for bucketing and generating batches\n",
        "Mini-batch training requires all lines in a batch to be of equal length. We have different lines of different lengths.\n",
        "\n",
        "A solution is to fill shorter sentences with PADs so that length of all sentences become equal. But, if one sentence in a batch has 20 words, and the same batch has another sentence with one word, then the latter sentence will have to be filled in by at least 19 pads. If most of the sentences start to have more PADs than actual content, training can be problematic.\n",
        "\n",
        "The solution to that is bucketing. First the sentences in the total list are sorted. After that sentences of similar lengths will be closer to each other. Batches are then formed with sentences of similar lengths. Much less padding will be required to turn sentences of similar lengths into sentences of equal lengths.\n",
        "\n",
        "Also while creating the batch, the input samples (the Engish lines) will have their words embedded using the recently trained embedding matrix for English. The output samples (the labels) will simply contain the index of the target Bengali word in the Bengali vocabulary list. The labels being in this format will be easier to train with sparse_softmax_cross_entropy cost function of Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzhNTsaPpczs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bucket_and_batch(x,y,batch_size):\n",
        "    \n",
        "    len_x= np.zeros((len(x)),np.int32)\n",
        "    \n",
        "    for i in range(len(x)):\n",
        "        len_x[i] = len(x[i])\n",
        "        \n",
        "    sorted_by_len_indices = np.flip(np.argsort(len_x),0)\n",
        "\n",
        "    sorted_x = []\n",
        "    sorted_y = []\n",
        "    \n",
        "    for i in range(len(x)):\n",
        "        sorted_x.append(x[sorted_by_len_indices[i]])\n",
        "        sorted_y.append(y[sorted_by_len_indices[i]])\n",
        "        \n",
        "    i=0\n",
        "    batches_x = []\n",
        "    batches_y = []\n",
        "    \n",
        "    while i<len(x):\n",
        "        \n",
        "        if i+batch_size>=len(x):\n",
        "            break\n",
        "        \n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "    \n",
        "        max_len_x = len(sorted_x[i])\n",
        "    \n",
        "        len_y= np.zeros((len(y)),np.int32)\n",
        "    \n",
        "        for j in range(i,i+batch_size):\n",
        "            len_y[j] = len(sorted_y[j])\n",
        "            \n",
        "        max_len_y = np.amax(len_y)\n",
        "        \n",
        "        for j in range(i,i+batch_size):\n",
        "            line=[]\n",
        "            for k1 in range(max_len_x+1): #+1 to include <EOS>\n",
        "                if k1==len(sorted_x[j]):\n",
        "                    line.append(np_embedding_eng[vocab_eng.index('<EOS>')])\n",
        "                elif k1>len(sorted_x[j]):\n",
        "                    line.append(np_embedding_eng[vocab_eng.index('<PAD>')])\n",
        "                else:\n",
        "                    line.append(np_embedding_eng[sorted_x[j][k1]])\n",
        "            batch_x.append(line)\n",
        "        \n",
        "            line=[]\n",
        "            for k2 in range(max_len_y+1): #+1 to include <EOS>\n",
        "                if k2>len(sorted_y[j]):\n",
        "                    line.append(vocab_beng.index('<PAD>'))\n",
        "                elif k2==len(sorted_y[j]):\n",
        "                    line.append(vocab_beng.index('<EOS>'))\n",
        "                else:\n",
        "                    line.append(sorted_y[j][k2])\n",
        "            batch_y.append(line)\n",
        "    \n",
        "        batch_x = np.asarray(batch_x,np.float32)\n",
        "        batch_y = np.asarray(batch_y,np.int32)\n",
        "\n",
        "        batches_x.append(batch_x)\n",
        "        batches_y.append(batch_y)\n",
        "    \n",
        "        i+=batch_size\n",
        "        \n",
        "    return batches_x,batches_y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSjEfoYOfMQ2",
        "colab_type": "text"
      },
      "source": [
        "# Creating train, validation, and test batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbbDdjsPpg88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_batch_eng,train_batch_beng = bucket_and_batch(train_eng,train_beng,batch_size)\n",
        "\n",
        "val_batch_eng,val_batch_beng = bucket_and_batch(val_eng,val_beng,batch_size)\n",
        "\n",
        "test_batch_eng,test_batch_beng = bucket_and_batch(test_eng,test_beng,batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukxh--acfRwc",
        "colab_type": "text"
      },
      "source": [
        "# Saving processed data in another file.\n",
        "a file name \"translationPICKLE\" will be created in the folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSwCHRb6EuL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving processed data in another file.\n",
        "\n",
        "import pickle\n",
        "\n",
        "PICK = [vocab_eng,vocab_beng,np_embedding_eng,np_embedding_beng,train_batch_eng,train_batch_beng,val_batch_eng,val_batch_beng,test_batch_eng,test_batch_beng]\n",
        "\n",
        "with open('translationPICKLE', 'wb') as fp:\n",
        "    pickle.dump(PICK, fp)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhdGEwAcP4pn",
        "colab_type": "text"
      },
      "source": [
        "# Loading Pre-processed Data\n",
        "(start of Machine Translation.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF6cCvk1P80s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "with open ('translationPICKLE', 'rb') as fp:\n",
        "    PICK = pickle.load(fp)\n",
        "\n",
        "vocab_eng = PICK[0] \n",
        "vocab_beng = PICK[1] \n",
        "vocab_len = len(vocab_beng)\n",
        "\n",
        "np_embedding_eng = PICK[2]\n",
        "np_embedding_beng = PICK[3]\n",
        "np_embedding_eng = np.asarray(np_embedding_eng,np.float32)\n",
        "np_embedding_beng = np.asarray(np_embedding_beng,np.float32)\n",
        "\n",
        "word_vec_dim = np_embedding_eng.shape[1] \n",
        "\n",
        "train_batch_x = PICK[4]\n",
        "train_batch_y = PICK[5]\n",
        "\n",
        "val_batch_x = PICK[6]\n",
        "val_batch_y = PICK[7]\n",
        "\n",
        "test_batch_x = PICK[8]\n",
        "test_batch_y = PICK[9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtdAobtwfoxe",
        "colab_type": "text"
      },
      "source": [
        "# Function for converting vector of size word_vec_dim into the closest representative english word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-CLg3cBQIDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_similar_eucli_eng(x):\n",
        "    xminusy = np.subtract(np_embedding_eng,x)\n",
        "    sq_xminusy = np.square(xminusy)\n",
        "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
        "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
        "    return np.argsort(eucli_dists)\n",
        "    \n",
        "def vec2word_eng(vec):   # converts a given vector representation into the represented word \n",
        "    most_similars = most_similar_eucli_eng(np.asarray(vec,np.float32))\n",
        "    return vocab_eng[most_similars[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5XrC9wbfv_d",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameters and Placeholders.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcGVgwsbQLXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#Hyperparamters\n",
        "\n",
        "h=8 #no. of heads\n",
        "N=1 #no. of decoder and encoder layers\n",
        "learning_rate=0.001\n",
        "epochs = 200\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "#Placeholders\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
        "y = tf.placeholder(tf.int32, [None,None])\n",
        "\n",
        "output_len = tf.placeholder(tf.int32)\n",
        "\n",
        "teacher_forcing = tf.placeholder(tf.bool)\n",
        "\n",
        "tf_pad_mask = tf.placeholder(tf.float32,[None,None])\n",
        "tf_illegal_position_masks = tf.placeholder(tf.float32,[None,None,None])\n",
        "\n",
        "tf_pe_out = tf.placeholder(tf.float32,[None,None,None]) #positional codes for output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlumsU7Cf0tE",
        "colab_type": "text"
      },
      "source": [
        "**Model Parameters.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQDkmbyXQPO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dimensions for Q (Query),K (Keys) and V (Values) for attention layers.\n",
        "\n",
        "dqkv = 32 \n",
        "    \n",
        "#Parameters for attention sub-layers for all n encoders\n",
        "\n",
        "Wq_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wk_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wv_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wo_enc = tf.Variable(tf.truncated_normal(shape=[N,h*dqkv,word_vec_dim],stddev=0.01))\n",
        "\n",
        "#Parameters for position-wise fully connected layers for n encoders\n",
        "\n",
        "d = 1024\n",
        "W1_enc = tf.Variable(tf.truncated_normal(shape=[N,1,1,word_vec_dim,d],stddev=0.01))\n",
        "b1_enc = tf.Variable(tf.constant(0,tf.float32,shape=[N,d]))\n",
        "W2_enc = tf.Variable(tf.truncated_normal(shape=[N,1,1,d,word_vec_dim],stddev=0.01))\n",
        "b2_enc = tf.Variable(tf.constant(0,tf.float32,shape=[N,word_vec_dim]))\n",
        "    \n",
        "#Parameters for 2 attention sub-layers for all n decoders\n",
        "\n",
        "Wq_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wk_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wv_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wo_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h*dqkv,word_vec_dim],stddev=0.01))\n",
        "Wq_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wk_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wv_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
        "Wo_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h*dqkv,word_vec_dim],stddev=0.01))\n",
        "    \n",
        "#Parameters for position-wise fully connected layers for n decoders\n",
        "\n",
        "d = 1024\n",
        "W1_dec = tf.Variable(tf.truncated_normal(shape=[N,1,1,word_vec_dim,d],stddev=0.01))\n",
        "b1_dec = tf.Variable(tf.constant(0,tf.float32,shape=[N,d]))\n",
        "W2_dec = tf.Variable(tf.truncated_normal(shape=[N,1,1,d,word_vec_dim],stddev=0.01))\n",
        "b2_dec = tf.Variable(tf.constant(0,tf.float32,shape=[N,word_vec_dim]))\n",
        "    \n",
        "#Layer Normalization parameters for encoder\n",
        "\n",
        "scale_enc_1 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "shift_enc_1 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "\n",
        "scale_enc_2 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "shift_enc_2 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "\n",
        "#Layer Normalization parameters for decoder   \n",
        "\n",
        "scale_dec_1 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "shift_dec_1 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "\n",
        "scale_dec_2 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "shift_dec_2 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "\n",
        "scale_dec_3 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
        "shift_dec_3 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQXWeinyf4_V",
        "colab_type": "text"
      },
      "source": [
        "**Function for generating a sequence of positional codes for positional encoding.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1afbai_nQTVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(seq_len,model_dimensions):\n",
        "    pe = np.zeros((seq_len,model_dimensions,),np.float32)\n",
        "    for pos in range(0,seq_len):\n",
        "        for i in range(0,model_dimensions):\n",
        "            pe[pos][i] = math.sin(pos/(10000**(2*i/model_dimensions)))\n",
        "    return pe.reshape((seq_len,model_dimensions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDAuoNjJf9xK",
        "colab_type": "text"
      },
      "source": [
        "**Function for Layer Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6cHA_9RQWQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer_norm(inputs,scale,shift,epsilon = 1e-5):\n",
        "\n",
        "    mean, var = tf.nn.moments(inputs, [1,2], keep_dims=True)\n",
        "\n",
        "    LN = tf.multiply((scale / tf.sqrt(var + epsilon)),(inputs - mean)) + shift\n",
        " \n",
        "    return LN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNdaQecNgIaM",
        "colab_type": "text"
      },
      "source": [
        " **Function to pre-generate masks for illegal positions.**\n",
        "These masks are to be used to fill illegal positions with -infinity (or a very low value eg. -2^30).\n",
        "\n",
        "Illegal positions are positions of the decoder input tokens that aren't predicted at a given timestep.\n",
        "\n",
        "{ In a transformer, the decoder input is of the same shape as the WHOLE decoder output sequence. One word for the sequence is predicted at each timestep (from left to right). So in most timesteps, the left side of the decoder input sequence will contain valid previously predicted output words, but the right side -the yet to be predicted side should contain some values that should be ignored and never attended. We make sure that they're ignored by masking it }\n",
        "\n",
        "So, the illegal positions depends on the total output length and the no. of predicted output tokens.\n",
        "\n",
        "The appropriate mask when i output tokens are predicted can be retrieved from mask[i-1] where mask is the return value from this function. The argument out_len that function takes, signifies the total length of the output.\n",
        "\n",
        "The masks are used to assign the value -2^30 to all positions in the tensor influenced by the illegal ones. After going through the softmax layer, these positions become close to 0, as it should be.\n",
        "\n",
        "Dynamically creating masks depending on the current position\\timestep (depending on which the program can know which positions are legal and which aren't) is, however, a bit troublesome with tensorflow tf_while_loop.\n",
        "\n",
        "I will be pre-generating all the masks with Python native code and feed the list of all required masks to the network at each training step (output length can be different at different training steps)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BRry9jaQZ2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_masks_for_illegal_positions(out_len):\n",
        "    \n",
        "    masks=np.zeros((out_len-1,out_len,out_len),dtype=np.float32)\n",
        "    \n",
        "    for i in range(1,out_len):\n",
        "        mask = np.zeros((out_len,out_len),dtype=np.float32)\n",
        "        mask[i:out_len,:] = -2**30\n",
        "        mask[:,i:out_len] = -2**30\n",
        "        masks[i-1] = mask\n",
        "        \n",
        "    return masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHGI5iGDgVV-",
        "colab_type": "text"
      },
      "source": [
        "# **Function for Multi-Headed Attention.**\n",
        "Details: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "Q = Query\n",
        "\n",
        "K = Key\n",
        "\n",
        "V = Value\n",
        "\n",
        "d is the dimension for Q, K and V."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1HvWqVQQdxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(Q,K,V,d,filled=0,mask=False):\n",
        "\n",
        "    K = tf.transpose(K,[0,2,1])\n",
        "    d = tf.cast(d,tf.float32)\n",
        "    \n",
        "    softmax_component = tf.div(tf.matmul(Q,K),tf.sqrt(d))\n",
        "    \n",
        "    if mask == True:\n",
        "        softmax_component = softmax_component + tf_illegal_position_masks[filled-1]\n",
        "        \n",
        "    result = tf.matmul(tf.nn.dropout(tf.nn.softmax(softmax_component),keep_prob),V)\n",
        " \n",
        "    return result\n",
        "       \n",
        "\n",
        "def multihead_attention(Q,K,V,d,weights,filled=0,mask=False):\n",
        "    \n",
        "    Q_ = tf.reshape(Q,[-1,tf.shape(Q)[2]])\n",
        "    K_ = tf.reshape(K,[-1,tf.shape(Q)[2]])\n",
        "    V_ = tf.reshape(V,[-1,tf.shape(Q)[2]])\n",
        "\n",
        "    heads = tf.TensorArray(size=h,dtype=tf.float32)\n",
        "    \n",
        "    Wq = weights['Wq']\n",
        "    Wk = weights['Wk']\n",
        "    Wv = weights['Wv']\n",
        "    Wo = weights['Wo']\n",
        "    \n",
        "    for i in range(0,h):\n",
        "        \n",
        "       Q_w = tf.matmul(Q_,Wq[i])\n",
        "       Q_w = tf.reshape(Q_w,[tf.shape(Q)[0],tf.shape(Q)[1],d])\n",
        "        \n",
        "       K_w = tf.matmul(K_,Wk[i])\n",
        "       K_w = tf.reshape(K_w,[tf.shape(K)[0],tf.shape(K)[1],d])\n",
        "        \n",
        "       V_w = tf.matmul(V_,Wv[i])\n",
        "       V_w = tf.reshape(V_w,[tf.shape(V)[0],tf.shape(V)[1],d])\n",
        "\n",
        "       head = attention(Q_w,K_w,V_w,d,filled,mask)\n",
        "            \n",
        "       heads = heads.write(i,head)\n",
        "        \n",
        "    heads = heads.stack()\n",
        "    \n",
        "    concated = heads[0]\n",
        "    \n",
        "    for i in range(1,h):\n",
        "        concated = tf.concat([concated,heads[i]],2)\n",
        "\n",
        "    concated = tf.reshape(concated,[-1,h*d])\n",
        "    out = tf.matmul(concated,Wo)\n",
        "    out = tf.reshape(out,[tf.shape(heads)[1],tf.shape(heads)[2],word_vec_dim])\n",
        "    \n",
        "    return out\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpTIkzQKgfCk",
        "colab_type": "text"
      },
      "source": [
        "# **Function for encoder**\n",
        "More details: https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwe2abYQQh8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(x,weights,attention_weights,dqkv):\n",
        "\n",
        "    W1 = weights['W1']\n",
        "    W2 = weights['W2']\n",
        "    b1 = weights['b1']\n",
        "    b2 = weights['b2']\n",
        "    \n",
        "    scale1 = weights['scale1']\n",
        "    shift1 = weights['shift1']\n",
        "    scale2 = weights['scale2']\n",
        "    shift2 = weights['shift2']\n",
        "    \n",
        "    # SUBLAYER 1 (MASKED MULTI HEADED SELF ATTENTION)\n",
        "    \n",
        "    sublayer1 = multihead_attention(x,x,x,dqkv,attention_weights)\n",
        "    sublayer1 = tf.nn.dropout(sublayer1,keep_prob)\n",
        "    sublayer1 = layer_norm(sublayer1 + x,scale1,shift1)\n",
        "    \n",
        "    sublayer1_ = tf.reshape(sublayer1,[tf.shape(sublayer1)[0],1,tf.shape(sublayer1)[1],word_vec_dim])\n",
        "    \n",
        "    # SUBLAYER 2 (TWO 1x1 CONVOLUTIONAL LAYERS AKA POSITION WISE FULLY CONNECTED NETWORKS)\n",
        "    \n",
        "    sublayer2 = tf.nn.conv2d(sublayer1_, W1, strides=[1,1,1,1], padding='SAME')\n",
        "    sublayer2 = tf.nn.bias_add(sublayer2,b1)\n",
        "    sublayer2 = tf.nn.relu(sublayer2)\n",
        "    \n",
        "    sublayer2 = tf.nn.conv2d(sublayer2, W2, strides=[1,1,1,1], padding='SAME')\n",
        "    sublayer2 = tf.nn.bias_add(sublayer2,b2)\n",
        "    \n",
        "    sublayer2 = tf.reshape(sublayer2,[tf.shape(sublayer2)[0],tf.shape(sublayer2)[2],word_vec_dim])\n",
        "    \n",
        "    sublayer2 = tf.nn.dropout(sublayer2,keep_prob)\n",
        "    sublayer2 = layer_norm(sublayer2 + sublayer1,scale2,shift2)\n",
        "    \n",
        "    return sublayer2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBo6egpWgj94",
        "colab_type": "text"
      },
      "source": [
        "# **Function for decoder**\n",
        "More details: https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaXH9GuaQlOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(y,enc_out,weights,masked_attention_weights,attention_weights,dqkv,mask=False,filled=0):\n",
        "\n",
        "    W1 = weights['W1']\n",
        "    W2 = weights['W2']\n",
        "    b1 = weights['b1']\n",
        "    b2 = weights['b2']\n",
        "    \n",
        "    scale1 = weights['scale1']\n",
        "    shift1 = weights['shift1']\n",
        "    scale2 = weights['scale2']\n",
        "    shift2 = weights['shift2']\n",
        "    scale3 = weights['scale3']\n",
        "    shift3 = weights['shift3']\n",
        "    \n",
        "    # SUBLAYER 1 (MASKED MULTI HEADED SELF ATTENTION)\n",
        "\n",
        "    sublayer1 = multihead_attention(y,y,y,dqkv,masked_attention_weights,filled,mask)\n",
        "    sublayer1 = tf.nn.dropout(sublayer1,keep_prob)\n",
        "    sublayer1 = layer_norm(sublayer1 + y,scale1,shift1)\n",
        "    \n",
        "    # SUBLAYER 2 (MULTIHEADED ENCODER-DECODER INTERLAYER ATTENTION)\n",
        "    \n",
        "    sublayer2 = multihead_attention(sublayer1,enc_out,enc_out,dqkv,attention_weights)\n",
        "    sublayer2 = tf.nn.dropout(sublayer2,keep_prob)\n",
        "    sublayer2 = layer_norm(sublayer2 + sublayer1,scale2,shift2)\n",
        "    \n",
        "    # SUBLAYER 3 (TWO 1x1 CONVOLUTIONAL LAYERS AKA POSITION WISE FULLY CONNECTED NETWORKS)\n",
        "    \n",
        "    sublayer2_ = tf.reshape(sublayer2,[tf.shape(sublayer2)[0],1,tf.shape(sublayer2)[1],word_vec_dim])\n",
        "    \n",
        "    sublayer3 = tf.nn.conv2d(sublayer2_, W1, strides=[1,1,1,1], padding='SAME')\n",
        "    sublayer3 = tf.nn.bias_add(sublayer3,b1)\n",
        "    sublayer3 = tf.nn.relu(sublayer3)\n",
        "    \n",
        "    sublayer3 = tf.nn.conv2d(sublayer3, W2, strides=[1,1,1,1], padding='SAME')\n",
        "    sublayer3 = tf.nn.bias_add(sublayer3,b2)\n",
        "    \n",
        "    sublayer3 = tf.reshape(sublayer3,[tf.shape(sublayer3)[0],tf.shape(sublayer3)[2],word_vec_dim])\n",
        "    \n",
        "    sublayer3 = tf.nn.dropout(sublayer3,keep_prob)\n",
        "    sublayer3 = layer_norm(sublayer3 + sublayer2,scale3,shift3)\n",
        "    \n",
        "    return sublayer3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf3T1VzEgtby",
        "colab_type": "text"
      },
      "source": [
        "# **Function for Stacking Encoders.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZlpUJdIQptZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stacked_encoders(layer_num,encoderin):\n",
        "    \n",
        "    for i in range(0,layer_num):\n",
        "        \n",
        "        encoder_weights = {\n",
        "            \n",
        "            'W1': W1_enc[i],\n",
        "            'W2': W2_enc[i],\n",
        "            'b1': b1_enc[i],\n",
        "            'b2': b2_enc[i],\n",
        "            'scale1': scale_enc_1[i],\n",
        "            'shift1': shift_enc_1[i],\n",
        "            'scale2': scale_enc_2[i],\n",
        "            'shift2': shift_enc_2[i],\n",
        "        }\n",
        "        \n",
        "        attention_weights = {\n",
        "            \n",
        "            'Wq': Wq_enc[i],\n",
        "            'Wk': Wk_enc[i],\n",
        "            'Wv': Wv_enc[i],\n",
        "            'Wo': Wo_enc[i],                       \n",
        "        }\n",
        "        \n",
        "        encoderin = encoder(encoderin,encoder_weights,attention_weights,dqkv)\n",
        "    \n",
        "    return encoderin\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc28lQdbgxt6",
        "colab_type": "text"
      },
      "source": [
        "# **Function for Stacking Decoders.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrQQVsuOQseN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stacked_decoders(layer_num,decoderin,encoderout,filled):\n",
        "    \n",
        "    for j in range(0,layer_num):\n",
        "        \n",
        "        decoder_weights = {\n",
        "            \n",
        "            'W1': W1_dec[j],\n",
        "            'W2': W2_dec[j],\n",
        "            'b1': b1_dec[j],\n",
        "            'b2': b2_dec[j],\n",
        "            'scale1': scale_dec_1[j],\n",
        "            'shift1': shift_dec_1[j],\n",
        "            'scale2': scale_dec_2[j],\n",
        "            'shift2': shift_dec_2[j],\n",
        "            'scale3': scale_dec_3[j],\n",
        "            'shift3': shift_dec_3[j],\n",
        "        }\n",
        "            \n",
        "        masked_attention_weights = {\n",
        "            \n",
        "            'Wq': Wq_dec_1[j],\n",
        "            'Wk': Wk_dec_1[j],\n",
        "            'Wv': Wv_dec_1[j],\n",
        "            'Wo': Wo_dec_1[j],                       \n",
        "        }\n",
        "        \n",
        "        attention_weights = {\n",
        "            \n",
        "            'Wq': Wq_dec_2[j],\n",
        "            'Wk': Wk_dec_2[j],\n",
        "            'Wv': Wv_dec_2[j],\n",
        "            'Wo': Wo_dec_2[j],                       \n",
        "        }\n",
        "            \n",
        "        decoderin = decoder(decoderin,encoderout,\n",
        "                            decoder_weights,\n",
        "                            masked_attention_weights,\n",
        "                            attention_weights,\n",
        "                            dqkv,\n",
        "                            mask=True,filled=filled)\n",
        "    return decoderin\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhI-yRa-g1J2",
        "colab_type": "text"
      },
      "source": [
        "# **Explanation**:\n",
        "**predicted_embedding():**\n",
        "Given a probability distribution and an embedding matrix, this function returns the embedding of the word with the maximum probability in the given distribution.\n",
        "\n",
        "**replaceSOS():**\n",
        "SOS signifies the start of sentence for the decoder. Also often represented as 'GO'. I am using an all ones vector as the first decoder input token. In the next time step, the SOS will be forgotten, and only the context of the previously predicted output (or the target output at the previous timestep, if teacher forcing is on) will be used.\n",
        "\n",
        "**add_pred_to_output_lists():**\n",
        "This function will concatenate the last predicted output into a tensor of concatenated sequence of output tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FYjyWlqQyt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predicted_embedding(out_prob_dist,tf_embd):\n",
        "    out_index = tf.cast(tf.argmax(out_prob_dist,1),tf.int32)\n",
        "    return tf.gather(tf_embd,out_index)\n",
        "\n",
        "def replaceSOS(output,out_prob_dist):\n",
        "    return output,tf.constant(1),tf.reshape(out_prob_dist,[tf.shape(x)[0],1,vocab_len])\n",
        "\n",
        "def add_pred_to_output_list(decoderin_part_1,output,filled,out_probs,out_prob_dist):\n",
        "    decoderin_part_1 = tf.concat([decoderin_part_1,output],1)\n",
        "    filled += 1\n",
        "    out_probs = tf.concat([out_probs,tf.reshape(out_prob_dist,[tf.shape(x)[0],1,vocab_len])],1)\n",
        "    return decoderin_part_1,filled,out_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KriQNgomhHmU",
        "colab_type": "text"
      },
      "source": [
        "# **Model Definition**\n",
        "It follows the encoder-decoder paradigm. The main exception from standard encoder-decoder paradigm, is that it uses 'transformers' instead of Reccurrent networks. The decoder undergoes a sequential processing, though.\n",
        "\n",
        "If teacher forcing is True, the decoder is made to guess the next output from the previous words in the actual target output, else the decoder predicts the next output from the previously predicted output of the decoder.\n",
        "\n",
        "Details about the model: https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE8yc3_rQ5sN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(x,y,teacher_forcing=True):\n",
        "    \n",
        "        \n",
        "    # NOTE: tf.shape(x)[0] == batch_size\n",
        "    \n",
        "    encoderin = x # (should be already positionally encoded) \n",
        "    encoderin = tf.nn.dropout(encoderin,keep_prob)\n",
        "\n",
        "    \n",
        "    # ENCODER LAYERS\n",
        "\n",
        "    encoderout = stacked_encoders(N,encoderin)\n",
        "    \n",
        "\n",
        "    # DECODER LAYERS\n",
        "\n",
        "    decoderin_part_1 = tf.ones([tf.shape(x)[0],1,word_vec_dim],dtype=tf.float32) #represents SOS\n",
        "    \n",
        "    filled = tf.constant(1) \n",
        "    # no. of output words that are filled i.e already predicted - are stored in 'filled'\n",
        "    # filled value is used to retrieve appropriate mask for illegal positions. \n",
        "    \n",
        "    \n",
        "    tf_embd = tf.convert_to_tensor(np_embedding_beng)\n",
        "    Wpd = tf.transpose(tf_embd)\n",
        "    # Wpd the transpose of the output embedding matrix will be used to convert the decoder output\n",
        "    # into a probability distribution over the output language vocabulary. \n",
        "    \n",
        "    out_probs = tf.zeros([tf.shape(x)[0],output_len,vocab_len],tf.float32)\n",
        "    # out_probs will contain the list of probability distributions.\n",
        "\n",
        "    #tf_while_loop since output_len will be dynamically defined during session run\n",
        "    \n",
        "    i=tf.constant(0)\n",
        "    \n",
        "    def cond(i,filled,decoderin_part_1,out_probs):\n",
        "        return i<output_len\n",
        "    \n",
        "    def body(i,filled,decoderin_part_1,out_probs):\n",
        "        \n",
        "        decoderin_part_2 = tf.zeros([tf.shape(x)[0],(output_len-filled),word_vec_dim],dtype=tf.float32)\n",
        "        \n",
        "        decoderin = tf.concat([decoderin_part_1,decoderin_part_2],1)\n",
        "        \n",
        "        decoderin = tf.nn.dropout(decoderin,keep_prob)\n",
        "        \n",
        "        decoderout = stacked_decoders(N,decoderin,encoderout,filled)\n",
        "        \n",
        "        # decoderout shape (now) = batch_size x seq_len x word_vec_dim\n",
        "\n",
        "        decoderout = tf.reduce_sum(decoderout,1) \n",
        "        \n",
        "        # summation over all the word_vec_dim dimensional vectors in the sequence to transform dimensions\n",
        "        # from batch_size x seq_len x word_vec_dim to batch_size x word_vec_dim.\n",
        "        # I suppose, a Linear layer can be alternatively used here too.\n",
        "        \n",
        "        # decoderout shape (now) = batch_size x word_vec_dim\n",
        "        \n",
        "        # converting decoderout to probability distributions\n",
        "        \n",
        "        out_prob_dist = tf.matmul(decoderout,Wpd)\n",
        "   \n",
        "        # If teacher forcing is false, initiate predicted_embedding(). It guesses the output embeddings\n",
        "        # to be that whose vocabulary index has maximum probability in out_prob_dist\n",
        "        # (the current output probability distribution). The embedding is used in the next\n",
        "        # iteration. \n",
        "        \n",
        "        # If teacher forcing is true, use the embedding of target index from y (laebls) \n",
        "        # for the next iteration.\n",
        "        \n",
        "        output = tf.cond(tf.equal(teacher_forcing,tf.convert_to_tensor(False)),\n",
        "                         lambda: predicted_embedding(out_prob_dist,tf_embd),\n",
        "                         lambda: tf.gather(tf_embd,y[:,i]))\n",
        "        \n",
        "        # Position Encoding the output\n",
        "        \n",
        "        output = output + tf_pe_out[i]\n",
        "        output = tf.reshape(output,[tf.shape(x)[0],1,word_vec_dim])\n",
        "                                \n",
        "        \n",
        "        #concatenate with list of previous predicted output tokens\n",
        "        \n",
        "        decoderin_part_1,filled,out_probs = tf.cond(tf.equal(i,0),\n",
        "                                        lambda:replaceSOS(output,out_prob_dist),\n",
        "                                        lambda:add_pred_to_output_list(decoderin_part_1,output,filled,out_probs,out_prob_dist))\n",
        "        \n",
        "        return i+1,filled,decoderin_part_1,out_probs\n",
        "            \n",
        "    _,_,_,out_probs = tf.while_loop(cond,body,[i,filled,decoderin_part_1,out_probs],\n",
        "                      shape_invariants=[i.get_shape(),\n",
        "                                        filled.get_shape(),\n",
        "                                        tf.TensorShape([None,None,word_vec_dim]),\n",
        "                                        tf.TensorShape([None,None,vocab_len])])\n",
        "\n",
        "    return out_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYPThrh_hfCM",
        "colab_type": "text"
      },
      "source": [
        "# **Setting up cost function and optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZmAeJmCQ_wH",
        "colab_type": "code",
        "outputId": "44e11cfc-5919-4c2c-c030-c776e2368d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Construct Model\n",
        "output = model(x,y,teacher_forcing)\n",
        "\n",
        "#OPTIMIZER\n",
        "\n",
        "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
        "cost = tf.multiply(cost,tf_pad_mask) #mask used to remove loss effect due to PADS\n",
        "cost = tf.reduce_mean(cost)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.9,beta2=0.98,epsilon=1e-9).minimize(cost)\n",
        "\n",
        "#wanna add some temperature?\n",
        "\n",
        "\"\"\"temperature = 0.7\n",
        "scaled_output = tf.log(output)/temperature\n",
        "softmax_output = tf.nn.softmax(scaled_output)\"\"\"\n",
        "\n",
        "#(^Use it with \"#prediction_int = np.random.choice(range(vocab_len), p=array.ravel())\")\n",
        "\n",
        "softmax_output = tf.nn.softmax(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-301-a08836c770db>:6: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn1u5qMHhlMg",
        "colab_type": "text"
      },
      "source": [
        "# **Function to create a Mask for pads' effect on cost.**\n",
        "The mask will have the same shape as the batch of labels but with the value 0 wherever there is a PAD. The mask will be element-wise multipled to the cost (before its averaged), so that any position in the cost tensor that is effected by the PAD will be multiplied by 0. This way, the effect of PADs (which we don't need to care about) on the cost (and therefore on the gradients) can be nullified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qQM7u9KR9WK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_pad_Mask(output_batch):\n",
        "    pad_index = vocab_beng.index('<PAD>')\n",
        "    mask = np.ones_like((output_batch),np.float32)\n",
        "    for i in range(len(mask)):\n",
        "        for j in range(len(mask[i])):\n",
        "            if output_batch[i,j]==pad_index:\n",
        "                mask[i,j]=0\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDU7ISHrhqqw",
        "colab_type": "text"
      },
      "source": [
        "## **Training .....**\n",
        "The input batch is positionally encoded before its fed to the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgCiC1r_SHH3",
        "colab_type": "code",
        "outputId": "c27451ea-fe4e-466a-ed54-4a9cdd20f206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import string\n",
        "import random\n",
        "from __future__ import print_function\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess: # Start Tensorflow Session\n",
        "    \n",
        "    saver = tf.train.Saver() \n",
        "    # Prepares variable for saving the model\n",
        "    sess.run(init) #initialize all variables\n",
        "    step = 0   \n",
        "    best_loss = 999\n",
        "    display_step = 1\n",
        "    warm_up_steps = 7000\n",
        "    \n",
        "    while step < epochs:\n",
        "        \n",
        "        batch_len = len(train_batch_x)\n",
        "        shuffled_indices = np.arange(batch_len)\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        \n",
        "        for i in range(0,batch_len):\n",
        "            \n",
        "            # Adaptive learning rate formula\n",
        "            #learning_rate = ((word_vec_dim)**(-0.5))*min((step*batch_len+i+1)**(-0.5),(step*batch_len+i+1)*warm_up_steps**(-1.5))\n",
        "\n",
        "            sample_no = np.random.randint(0,len(train_batch_x[0]))\n",
        "            print(\"\\nCHOSEN SAMPLE NO.: \"+str(sample_no))\n",
        "            \n",
        "            if i%display_step==0:\n",
        "                \n",
        "                print(\"\\nEpoch: \"+str(step+1)+\" Iteration: \"+str(i+1))\n",
        "                print(\"\\nSAMPLE TEXT:\")\n",
        "                for vec in train_batch_x[shuffled_indices[i]][sample_no]:\n",
        "                    print(vec2word_eng(vec),end=\" \")\n",
        "                print(\"\\n\")\n",
        "                \n",
        "            input_seq_len = len(train_batch_x[shuffled_indices[i]][0])\n",
        "            \n",
        "            pe_in = positional_encoding(input_seq_len,word_vec_dim)\n",
        "            pe_in = pe_in.reshape((1,input_seq_len,word_vec_dim))\n",
        "            \n",
        "            output_seq_len = len(train_batch_y[shuffled_indices[i]][0])\n",
        "            \n",
        "            \n",
        "            \n",
        "            illegal_position_masks = generate_masks_for_illegal_positions(output_seq_len)\n",
        "            \n",
        "            pe_out = positional_encoding(output_seq_len,word_vec_dim)\n",
        "            pe_out = pe_out.reshape((output_seq_len,1,word_vec_dim))\n",
        "    \n",
        "            \n",
        "            rand = random.randint(0,2) #determines chance of using Teacher Forcing\n",
        "            if rand==1:\n",
        "                random_bool = True\n",
        "            else:\n",
        "                random_bool = False\n",
        "            \n",
        "            pad_mask = create_pad_Mask(train_batch_y[shuffled_indices[i]])\n",
        "            \n",
        "            # Run optimization operation (backpropagation)\n",
        "            _,loss,out = sess.run([optimizer,cost,softmax_output],\n",
        "                                  feed_dict={x: (train_batch_x[shuffled_indices[i]]+pe_in), \n",
        "                                             y: train_batch_y[shuffled_indices[i]],\n",
        "                                             keep_prob: 0.9,\n",
        "                                             output_len: len(train_batch_y[shuffled_indices[i]][0]),\n",
        "                                             tf_pad_mask: pad_mask,\n",
        "                                             tf_illegal_position_masks: illegal_position_masks,\n",
        "                                             tf_pe_out: pe_out,\n",
        "                                             teacher_forcing: False #random_bool\n",
        "                                             # feed random bool for randomized teacher forcing. \n",
        "                                             })\n",
        "            \n",
        "            if i%display_step==0:\n",
        "                \n",
        "                print(\"\\nPREDICTED TRANSLATION OF THE SAMPLE:\\n\")\n",
        "                flag = 0\n",
        "                for array in out[sample_no]:\n",
        "                    \n",
        "                    #prediction_int = np.random.choice(range(vocab_len), p=array.ravel()) \n",
        "                    #(^use this if you want some variety)\n",
        "                    #(or use this what's below:)\n",
        "                    \n",
        "                    prediction_int = np.argmax(array)\n",
        "                    \n",
        "                    if vocab_beng[prediction_int] in string.punctuation or flag==0: \n",
        "                        print(vocab_beng[prediction_int],end='')\n",
        "                    else:\n",
        "                        print(\" \"+vocab_beng[prediction_int],end='')\n",
        "                    flag=1\n",
        "                print(\"\\n\")\n",
        "                \n",
        "                print(\"ACTUAL TRANSLATION OF THE SAMPLE:\\n\")\n",
        "                for index in train_batch_y[shuffled_indices[i]][sample_no]:\n",
        "                    print(vocab_beng[index],end=\" \")\n",
        "                print(\"\\n\")\n",
        "            \n",
        "            print(\"loss=\"+str(loss))\n",
        "                  \n",
        "            if(loss<best_loss):\n",
        "                best_loss = loss\n",
        "                saver.save(sess, 'Model_Backup/translation_model.ckpt')\n",
        "\n",
        "        step=step+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CHOSEN SAMPLE NO.: 56\n",
            "\n",
            "Epoch: 1 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he made her a bookshelf <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద ఇద ఇద ఇద ఇద ఇద ఇద ఇద ఇద ఇద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతను ఆమక పుసతకల అర తయరు చసడు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=341.22516\n",
            "\n",
            "CHOSEN SAMPLE NO.: 38\n",
            "\n",
            "Epoch: 2 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he dressed up as a woman <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వనలన వనలన వనలన వనలన వనలన వనలన వనలన వనలన వనలన వనలన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు సతరలగ వసతరలను ధరంచడు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=331.64352\n",
            "\n",
            "CHOSEN SAMPLE NO.: 19\n",
            "\n",
            "Epoch: 3 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she asked me how many languages i spoke <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "<EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఎనన భషలు మటలడుతనన తను అడగంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=201.12837\n",
            "\n",
            "CHOSEN SAMPLE NO.: 7\n",
            "\n",
            "Epoch: 4 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he was a great poet as well as a doctor <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మటలడంత మటలడంత మటలడంత మటలడంత మటలడంత మటలడంత మటలడంత మటలడంత మటలడంత మటలడంత\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను ఒక గపప కవ కదు మంచ వదయుడు కూడ <EOS> <PAD> \n",
            "\n",
            "loss=290.8291\n",
            "\n",
            "CHOSEN SAMPLE NO.: 2\n",
            "\n",
            "Epoch: 5 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 've made a mistake though i did not intend to <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద ఇద ఇద ఇద ఇద ఇద ఇద ఇద ఇద ఇద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన పరపటు చసను కవలన కకపయన <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=264.61212\n",
            "\n",
            "CHOSEN SAMPLE NO.: 16\n",
            "\n",
            "Epoch: 6 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "sitting down all day is bad for you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నమమదగన నమమదగన నమమదగన నమమదగన నమమదగన నమమదగన నమమదగన నమమదగన నమమదగన నమమదగన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజంత కూరచవడం నకు మంచద కదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=210.71724\n",
            "\n",
            "CHOSEN SAMPLE NO.: 17\n",
            "\n",
            "Epoch: 7 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we need to hire people we can trust <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "<EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం నమమదగన వళళన పనలక తసుకవల <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=77.905014\n",
            "\n",
            "CHOSEN SAMPLE NO.: 21\n",
            "\n",
            "Epoch: 8 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "there is no reason to be afraid <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "<EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన కరణం ఏమ లదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=88.269325\n",
            "\n",
            "CHOSEN SAMPLE NO.: 59\n",
            "\n",
            "Epoch: 9 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what else i can lose <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఆవడ ఆవడ ఆవడ ఆవడ ఆవడ ఆవడ ఆవడ ఆవడ ఆవడ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఇంకం కలపతను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=84.11668\n",
            "\n",
            "CHOSEN SAMPLE NO.: 6\n",
            "\n",
            "Epoch: 10 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is going to take all afternoon and maybe more <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తలుపు తలుపు తలుపు తలుపు తలుపు తలుపు తలుపు తలుపు తలుపు తలుపు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మధయహనం మతతం లద ఇంక ఎకకువ సమయం పటటచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=63.565533\n",
            "\n",
            "CHOSEN SAMPLE NO.: 30\n",
            "\n",
            "Epoch: 11 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he washed the blood off his hands <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఒక అవుతుంద అవుతుంద అవుతుంద అవుతుంద అవుతుంద అవుతుంద అవుతుంద అవుతుంద అవుతుంద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు తన చతులక అంటన రకతనన కడగసడు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=49.53126\n",
            "\n",
            "CHOSEN SAMPLE NO.: 26\n",
            "\n",
            "Epoch: 12 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she was on her way to school <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను కవల కవల తలయదు <EOS> తలయదు తలయదు తలయదు తలయదు తలయదు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను సకూలుకు వళళ దరల వుంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=38.73556\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 13 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను <EOS> <EOS> <EOS> <EOS> <EOS> కద <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=35.422123\n",
            "\n",
            "CHOSEN SAMPLE NO.: 23\n",
            "\n",
            "Epoch: 14 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'll let it go this time <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "దురదరశన కవల చసద కడగసడు కడగసడు ఒకట ఒకట చసద కవల వళళను\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఈసరక వదలసతను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=33.90157\n",
            "\n",
            "CHOSEN SAMPLE NO.: 32\n",
            "\n",
            "Epoch: 15 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "are you prepared to do this <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మటలడల నుండ నుండ తలలబయంద వళల కరణం నుండ తటటంద నుండ నుండ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద చయయడనక సదదంగ వుననవ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=32.301765\n",
            "\n",
            "CHOSEN SAMPLE NO.: 7\n",
            "\n",
            "Epoch: 16 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he was a great poet as well as a doctor <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కడగసడు వచచంద కవల వళల కరణం తటటంద కకపయన తటటంద కడగసడు కడగసడు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను ఒక గపప కవ కదు మంచ వదయుడు కూడ <EOS> <PAD> \n",
            "\n",
            "loss=31.188602\n",
            "\n",
            "CHOSEN SAMPLE NO.: 57\n",
            "\n",
            "Epoch: 17 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she scared the cat away <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను గదల ఒక పటటచచు వునన బస వునన పటటచచు చయగలగ తలక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ పలలన భయపటట తరమసంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=29.201458\n",
            "\n",
            "CHOSEN SAMPLE NO.: 51\n",
            "\n",
            "Epoch: 18 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you may go swimming or fishing <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అనుమత <EOS> లదన <EOS> మటలడంత పరశన తలక వచచంద పరశన బస\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఈత కటటడనక లక చపలు పటటడనక వళళచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=27.858545\n",
            "\n",
            "CHOSEN SAMPLE NO.: 58\n",
            "\n",
            "Epoch: 19 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "this made me very sad <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నజంగ మటలడంత ఈరజు <EOS> <EOS> ఉంటను <EOS> ఉంటను ఏడు వదయుడు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద ననను చల బధ పటటంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=27.256144\n",
            "\n",
            "CHOSEN SAMPLE NO.: 43\n",
            "\n",
            "Epoch: 20 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we need money to do anything <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు నరపుతుననడు టననసంట తలుసు నత <EOS> టననసంట ఆ రండు <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఏం చయయలనన మనక డబబులు కవల <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=26.446066\n",
            "\n",
            "CHOSEN SAMPLE NO.: 52\n",
            "\n",
            "Epoch: 21 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she is not at home now <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం <EOS> <EOS> సకస టననసంట ఆ <EOS> కుకకలు కుకకలు ఆ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను ఇపపుడు ఇంట దగగర లదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=25.902151\n",
            "\n",
            "CHOSEN SAMPLE NO.: 50\n",
            "\n",
            "Epoch: 22 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is not all your fault <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజంత ఆ చయగలగ తలుసు వళళు ఉంటను చపపు నన అంటన <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత న తపపు కదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=25.400806\n",
            "\n",
            "CHOSEN SAMPLE NO.: 16\n",
            "\n",
            "Epoch: 23 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "sitting down all day is bad for you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన సకస అంటన లక నజంగ కఫ తనన అంటన సమయం ఏమన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజంత కూరచవడం నకు మంచద కదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=24.548687\n",
            "\n",
            "CHOSEN SAMPLE NO.: 40\n",
            "\n",
            "Epoch: 24 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "tell me about your daily life <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము <EOS> కటటడనక వళళు ఈసరక ఫన ఈరజు నత నరపుతుననడు తను\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న రజువర జవతం గురంచ చపపు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=23.981874\n",
            "\n",
            "CHOSEN SAMPLE NO.: 8\n",
            "\n",
            "Epoch: 25 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not have time to talk right now <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వలువనద <EOS> పలల లదన కుకకలు అతను చయయడనక భయపడలసన <EOS> కటటడనక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు ఇపపుడు మటలడంత సమయం లదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=23.818035\n",
            "\n",
            "CHOSEN SAMPLE NO.: 18\n",
            "\n",
            "Epoch: 26 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do not hesitate to ask me for help <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అనుమత చతులక <EOS> <EOS> సలహ రండు <EOS> పటట చయగలగ రండు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననను సహయం అడగడనక ఏం సందహ పడదదు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=22.685549\n",
            "\n",
            "CHOSEN SAMPLE NO.: 5\n",
            "\n",
            "Epoch: 27 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i should have been able to do that by myself <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజుల ఇపపుడు అతను ఇపపుడు రజుల బయటక <EOS> తసుకుందం తనన చపలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న అంతట నన చయగలగ వుండలసంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=22.501719\n",
            "\n",
            "CHOSEN SAMPLE NO.: 27\n",
            "\n",
            "Epoch: 28 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not know where they are <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మంచ వళల ఇకకడ <EOS> <EOS> వుండలసంద <EOS> తసుకుందం అతను <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వళళు ఎకకడ ఉననర నకు తలయదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=22.045383\n",
            "\n",
            "CHOSEN SAMPLE NO.: 39\n",
            "\n",
            "Epoch: 29 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you must stay where you are <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మదలుపటటను <EOS> చపతవన కవలన పలల <EOS> ఏడు వదలయయడం <EOS> అకకడక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఎకకడ ఉననవ అకకడ ఉండు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=22.340254\n",
            "\n",
            "CHOSEN SAMPLE NO.: 49\n",
            "\n",
            "Epoch: 30 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we had better leave her alone <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు తనన కుకకలు అకకడక ఇంకం నకు చపలు నమమదగన నకు కుకకలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తనన ఒంటరగ వదలయయడం మంచద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=21.467468\n",
            "\n",
            "CHOSEN SAMPLE NO.: 50\n",
            "\n",
            "Epoch: 31 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is not all your fault <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మ ఉననయ మటలడుతనన తలుస తసుకుందం కుకకలు ఎకకువ చపతవన ఆమక తసుకుందం\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత న తపపు కదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=20.296337\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 32 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కపంగ ఉననయ ఆమక మనం చయయడనక వనలన <EOS> ఉననయ వదయురలు చయయడనక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=20.251673\n",
            "\n",
            "CHOSEN SAMPLE NO.: 60\n",
            "\n",
            "Epoch: 33 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i suggest you do that <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను తసంద ఎకకువ అకకడక చపలు <EOS> వదయురలు అతడు <EOS> ఎకకడ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు అద చయయమన న సలహ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=19.960693\n",
            "\n",
            "CHOSEN SAMPLE NO.: 13\n",
            "\n",
            "Epoch: 34 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i know you are going to say no <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన తసంద మంచద మటలడుతనన <EOS> వళళన సమయం మంచద సమయం అయయంద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు నువవు ఒదదన చపతవన తలుసు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=19.722044\n",
            "\n",
            "CHOSEN SAMPLE NO.: 55\n",
            "\n",
            "Epoch: 35 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "that was not easy you know <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద రండు <EOS> ఎకకడ తసుకుందం <EOS> ఈరజు <EOS> కరణం కరణం\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద అంత తలక కదు తలుస <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=19.433413\n",
            "\n",
            "CHOSEN SAMPLE NO.: 22\n",
            "\n",
            "Epoch: 36 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do you usually eat breakfast before seven <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజువర ఎకకడ ఉంటను మంచద తరమసంద అతడు పటట <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు మములుగ ఏడు కు ముంద టఫన చసతవ <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=19.094654\n",
            "\n",
            "CHOSEN SAMPLE NO.: 56\n",
            "\n",
            "Epoch: 37 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he made her a bookshelf <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు గదల వళళన <EOS> అరధం <EOS> కటటడనక <EOS> తసంద <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతను ఆమక పుసతకల అర తయరు చసడు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=19.340647\n",
            "\n",
            "CHOSEN SAMPLE NO.: 0\n",
            "\n",
            "Epoch: 38 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he has two pencils one is long and the other one is short <EOS> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను పటటంద చయయడనక తసంద గపప రతర వళల బయటకు పటటంద నన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన దగగర రండు పనసళళు వుననయ ఒకట పడుగు ఇంకట పటట <EOS> \n",
            "\n",
            "loss=18.57981\n",
            "\n",
            "CHOSEN SAMPLE NO.: 26\n",
            "\n",
            "Epoch: 39 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she was on her way to school <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న ఇపపుడు చయయడనక నమమదగన ఇకకడ చయయడనక <EOS> <EOS> <EOS> లదు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను సకూలుకు వళళ దరల వుంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=18.387737\n",
            "\n",
            "CHOSEN SAMPLE NO.: 59\n",
            "\n",
            "Epoch: 40 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what else i can lose <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న నమమదగన ఇషటం అడగడనక <EOS> కటటడనక ఇంట ఇద కరణం <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఇంకం కలపతను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=18.102081\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 41 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ల అడగంద <EOS> తసంద ఇపపుడు <EOS> ఈరజు తసంద లదు <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=18.155893\n",
            "\n",
            "CHOSEN SAMPLE NO.: 47\n",
            "\n",
            "Epoch: 42 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we have not slept in days <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న లదు <EOS> <EOS> పళళు ఇకకడ కవ పటటచచు <EOS> పనసళలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము రజుల తరబడ నదరపలదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=17.728552\n",
            "\n",
            "CHOSEN SAMPLE NO.: 63\n",
            "\n",
            "Epoch: 43 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'll be very careful <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను బయటకు సహయం కద చయగలగ <EOS> వసతుననవ నకు కద నకు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను చల జగరతతగ ఉంటను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=17.826336\n",
            "\n",
            "CHOSEN SAMPLE NO.: 51\n",
            "\n",
            "Epoch: 44 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you may go swimming or fishing <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సటషన <EOS> మటలడల దగగర <EOS> <EOS> వళళ <EOS> చూపుక <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఈత కటటడనక లక చపలు పటటడనక వళళచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=17.127644\n",
            "\n",
            "CHOSEN SAMPLE NO.: 59\n",
            "\n",
            "Epoch: 45 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what else i can lose <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న చయగలగ సహయం పటటంద ఆ చయగలగ <EOS> కంద మనం ఎకకువ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఇంకం కలపతను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=17.252598\n",
            "\n",
            "CHOSEN SAMPLE NO.: 23\n",
            "\n",
            "Epoch: 46 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'll let it go this time <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సకూలుకు ఎకకడ నమమదగన ఎకకువ తసుకుంటవ వళళన నకు చయగలగ ఎకకడ పటట\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఈసరక వదలసతను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=17.051323\n",
            "\n",
            "CHOSEN SAMPLE NO.: 20\n",
            "\n",
            "Epoch: 47 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'm not allowed to do that here <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను దగగర దగగర సమయం పటటంద కదు తన కద నదరపలదు సకూలుకు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద ఇకకడ చయయడనక నకు అనుమత లదు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=16.763538\n",
            "\n",
            "CHOSEN SAMPLE NO.: 17\n",
            "\n",
            "Epoch: 48 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we need to hire people we can trust <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఒక నన వళల ఈసరక వళళన ఈసరక <EOS> <EOS> రజువర <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం నమమదగన వళళన పనలక తసుకవల <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=16.235788\n",
            "\n",
            "CHOSEN SAMPLE NO.: 43\n",
            "\n",
            "Epoch: 49 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we need money to do anything <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తనన కవల అకకడక జవతం వళల దగగర మనం బయలుదర తరమసంద <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఏం చయయలనన మనక డబబులు కవల <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=16.809025\n",
            "\n",
            "CHOSEN SAMPLE NO.: 28\n",
            "\n",
            "Epoch: 50 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "how about going out to eat tonight <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను <EOS> లదు తను కదు తకడు అవుతుంద సందహ ఈరజు సందహ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ రతర బయటక వళల తందమ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=16.096792\n",
            "\n",
            "CHOSEN SAMPLE NO.: 20\n",
            "\n",
            "Epoch: 51 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'm not allowed to do that here <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం మనసుక వడన వదయురలు <EOS> నన పరపటు నుండ ఆ బల\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద ఇకకడ చయయడనక నకు అనుమత లదు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=16.588226\n",
            "\n",
            "CHOSEN SAMPLE NO.: 16\n",
            "\n",
            "Epoch: 52 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "sitting down all day is bad for you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం సహయం అంత ఉంటను <EOS> <EOS> వదలయయడం కవలన <EOS> కద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజంత కూరచవడం నకు మంచద కదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.929309\n",
            "\n",
            "CHOSEN SAMPLE NO.: 24\n",
            "\n",
            "Epoch: 53 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she refuses to say more about it <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఏడు నువవత ఉతకనపపుడు చయగలగ సహయం కదు ఉననయ తపపు చపపు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఇంక ఎకకువ చపపడనక ఒపపుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.676905\n",
            "\n",
            "CHOSEN SAMPLE NO.: 21\n",
            "\n",
            "Epoch: 54 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "there is no reason to be afraid <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు దగగర రజువర నువవత ఇకకడ <EOS> ఎల <EOS> ఎనన దగగర\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన కరణం ఏమ లదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.732465\n",
            "\n",
            "CHOSEN SAMPLE NO.: 63\n",
            "\n",
            "Epoch: 55 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'll be very careful <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "దగగరల నన అంగకరంచను మములుగ సలహ <EOS> <EOS> <EOS> <EOS> వచచంద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను చల జగరతతగ ఉంటను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=16.199896\n",
            "\n",
            "CHOSEN SAMPLE NO.: 24\n",
            "\n",
            "Epoch: 56 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she refuses to say more about it <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను <EOS> పటటంద ఉతకనపపుడు ఆ చపతవన అంక చయయడనక తలక మమడపండలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఇంక ఎకకువ చపపడనక ఒపపుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.54965\n",
            "\n",
            "CHOSEN SAMPLE NO.: 44\n",
            "\n",
            "Epoch: 57 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "these pencils are the same color <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఎనన <EOS> మనం రతరక అంత రజుల ఉననయ మదలుపటటను నమమదగన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ పనసళలు ఒక రంగుల ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.421789\n",
            "\n",
            "CHOSEN SAMPLE NO.: 50\n",
            "\n",
            "Epoch: 58 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is not all your fault <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద కటటుకుంటూన ఏం కటటుకుంటూన <EOS> గద <EOS> రజువర కవ చయగలగ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత న తపపు కదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.229303\n",
            "\n",
            "CHOSEN SAMPLE NO.: 51\n",
            "\n",
            "Epoch: 59 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you may go swimming or fishing <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను <EOS> తపపు ఎకకువ అరధం మములుగ ఒక అతడు వదయురలు సమయం\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఈత కటటడనక లక చపలు పటటడనక వళళచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=15.514822\n",
            "\n",
            "CHOSEN SAMPLE NO.: 33\n",
            "\n",
            "Epoch: 60 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "out of sight out of mind <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఎందుకు కరణం వనలన <EOS> చపలు గుండపటు ఏం <EOS> గదల పరశన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చూపుక వలుపల​ మనసుక వలుపల​ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.145624\n",
            "\n",
            "CHOSEN SAMPLE NO.: 2\n",
            "\n",
            "Epoch: 61 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 've made a mistake though i did not intend to <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతను అవుతుంద ఎకకడ తరమసంద నకు చయయడనక నన మదలుపటటను మంచద దగగర\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన పరపటు చసను కవలన కకపయన <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.132823\n",
            "\n",
            "CHOSEN SAMPLE NO.: 59\n",
            "\n",
            "Epoch: 62 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what else i can lose <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ కవ ఏడు వళల ఉంటను <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఇంకం కలపతను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.177241\n",
            "\n",
            "CHOSEN SAMPLE NO.: 1\n",
            "\n",
            "Epoch: 63 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is high time you left for school is not it <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఏడు అతడు పరశన చయయడనక అకకడక ఎకకువ పటట కటటడనక వుంద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సకూలుక బయలుదర సమయం అయయంద కద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.34343\n",
            "\n",
            "CHOSEN SAMPLE NO.: 52\n",
            "\n",
            "Epoch: 64 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she is not at home now <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననను ఏడు రజుల ఇకకడ ఈరజు పరశన వళల ఎకకువ <EOS> ఏడు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను ఇపపుడు ఇంట దగగర లదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=15.20282\n",
            "\n",
            "CHOSEN SAMPLE NO.: 29\n",
            "\n",
            "Epoch: 65 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "have you ever had a heart attack <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను అనుమత ఏడు గపప కటటుకుంటూన <EOS> దగగర <EOS> కఫ అతను\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు ఎపపుడన గుండపటు వచచంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.811765\n",
            "\n",
            "CHOSEN SAMPLE NO.: 33\n",
            "\n",
            "Epoch: 66 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "out of sight out of mind <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఎవరు <EOS> దగగర తనన <EOS> <EOS> ఈసరక <EOS> అవవబతుంద <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చూపుక వలుపల​ మనసుక వలుపల​ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.801944\n",
            "\n",
            "CHOSEN SAMPLE NO.: 37\n",
            "\n",
            "Epoch: 67 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not know who won <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము ననంక బల వసతరలను ఏడు సమయం <EOS> చయయడనక మటలడుతనన సకస\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఎవరు గలచర నకు తలయదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.477102\n",
            "\n",
            "CHOSEN SAMPLE NO.: 62\n",
            "\n",
            "Epoch: 68 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what is in the box <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు <EOS> వచచ సహయం <EOS> గపప పరపటు గపప <EOS> కుకకలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పటట ల ఏముంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.625818\n",
            "\n",
            "CHOSEN SAMPLE NO.: 28\n",
            "\n",
            "Epoch: 69 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "how about going out to eat tonight <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను అరధం తపపు బస <EOS> ఇషటం కవ ఇద రజుల మంచద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ రతర బయటక వళల తందమ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.973147\n",
            "\n",
            "CHOSEN SAMPLE NO.: 53\n",
            "\n",
            "Epoch: 70 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the movie 's about to start <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సకూలుక గలచర ఉననయ <EOS> కుకకలు పటటచచు తరమసంద తను మనసుక మనసుక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం మదలు అవవబతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.190073\n",
            "\n",
            "CHOSEN SAMPLE NO.: 54\n",
            "\n",
            "Epoch: 71 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "some of the dogs are alive <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు సమయం కదు వళళు లదు సమయం కరణం మదలుపటటను కరణం కవ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కనన కుకకలు బరతక ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.367024\n",
            "\n",
            "CHOSEN SAMPLE NO.: 3\n",
            "\n",
            "Epoch: 72 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do you know the reason why she is so angry <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఏద ఇద ఎకకడ ఏం మదలుపటటను ఆ తన ఇకకడ ఒక మదలుపటటను\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను ఎందుకు అంత కపంగ ఉంద నకు తలుస <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=14.480255\n",
            "\n",
            "CHOSEN SAMPLE NO.: 7\n",
            "\n",
            "Epoch: 73 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he was a great poet as well as a doctor <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన తలక సతరలగ ననంక బసకట <EOS> తపపు తరమసంద మనం ఈసరక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను ఒక గపప కవ కదు మంచ వదయుడు కూడ <EOS> <PAD> \n",
            "\n",
            "loss=13.999782\n",
            "\n",
            "CHOSEN SAMPLE NO.: 45\n",
            "\n",
            "Epoch: 74 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not trust that guy <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము నుండ ఒకట వచచంద ఇషటం సమయం బస రకతనన అరధం <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వడన నమమను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.252591\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 75 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు <EOS> అకకడ ఇకకడ ఏమ కఫ చపతవన కద <EOS> చూపుక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.13459\n",
            "\n",
            "CHOSEN SAMPLE NO.: 8\n",
            "\n",
            "Epoch: 76 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not have time to talk right now <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననను తను డసక ఏడు కదు <EOS> పటట <EOS> రతరక పళళు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు ఇపపుడు మటలడంత సమయం లదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.980158\n",
            "\n",
            "CHOSEN SAMPLE NO.: 36\n",
            "\n",
            "Epoch: 77 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it actually is not that simple <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వనలన వదయురలు సటషన కటటుకుంటూన ఎకకడ ముసలవళలు చయయడనక <EOS> పలల\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద అంత సులభం ఏం కదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.065143\n",
            "\n",
            "CHOSEN SAMPLE NO.: 42\n",
            "\n",
            "Epoch: 78 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "can i make a phone call <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తనన అకకడ ఇపపుడు <EOS> అర మదలుపటటను <EOS> <EOS> చపపు తసుకుందం\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఒక ఫన చసుకవచచ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=14.031164\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 79 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం చయయడనక గలచర దగగర ఉంటను <EOS> <EOS> పటట <EOS> మనం\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.899098\n",
            "\n",
            "CHOSEN SAMPLE NO.: 24\n",
            "\n",
            "Epoch: 80 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she refuses to say more about it <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను ఎకకువ రజువర రజువర సదదంగ భషలు ఎకకువ <EOS> నమమను మదలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఇంక ఎకకువ చపపడనక ఒపపుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.868274\n",
            "\n",
            "CHOSEN SAMPLE NO.: 28\n",
            "\n",
            "Epoch: 81 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "how about going out to eat tonight <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మకు <EOS> అర ఉననర కరణం ఏం <EOS> మనం సమయం ఎకకడ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ రతర బయటక వళల తందమ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.335523\n",
            "\n",
            "CHOSEN SAMPLE NO.: 7\n",
            "\n",
            "Epoch: 82 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he was a great poet as well as a doctor <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న ఎంత <EOS> బస మనసుక అతడు లదు <EOS> టఫన మనసుక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను ఒక గపప కవ కదు మంచ వదయుడు కూడ <EOS> <PAD> \n",
            "\n",
            "loss=13.456879\n",
            "\n",
            "CHOSEN SAMPLE NO.: 16\n",
            "\n",
            "Epoch: 83 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "sitting down all day is bad for you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతను అంక కుకకలు సహయం కవల నకు ఎకకువ ఎకకడ ననంక బయలుదర\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజంత కూరచవడం నకు మంచద కదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.646573\n",
            "\n",
            "CHOSEN SAMPLE NO.: 48\n",
            "\n",
            "Epoch: 84 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "how many mangoes do you want <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ వదయుడు ఉంటను కడగసడు ఒకట కదు <EOS> <EOS> <EOS> పటటంద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మకు ఎనన మమడపండలు కవల <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.357305\n",
            "\n",
            "CHOSEN SAMPLE NO.: 54\n",
            "\n",
            "Epoch: 85 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "some of the dogs are alive <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతను బరతక ఇపపుడు బయలుదర <EOS> <EOS> కదు <EOS> <EOS> పటటంద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కనన కుకకలు బరతక ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.617836\n",
            "\n",
            "CHOSEN SAMPLE NO.: 36\n",
            "\n",
            "Epoch: 86 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it actually is not that simple <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఇంటక తకడు <EOS> అర అర <EOS> ఇపపుడు <EOS> చపలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద అంత సులభం ఏం కదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.60345\n",
            "\n",
            "CHOSEN SAMPLE NO.: 17\n",
            "\n",
            "Epoch: 87 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we need to hire people we can trust <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత ఏడు అతడు అంతట కటటుకుంటూన అతడు కద సంతషంగ ఎంత ఆ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం నమమదగన వళళన పనలక తసుకవల <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.526396\n",
            "\n",
            "CHOSEN SAMPLE NO.: 15\n",
            "\n",
            "Epoch: 88 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "a cat came out from under the desk <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు లదు తసంద నకు చపలు సదదంగ రతరక తనన ఉననటలుననవ పడుగు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఒక పలల డసక కంద నుండ బయటకు వచచంద <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=13.343961\n",
            "\n",
            "CHOSEN SAMPLE NO.: 7\n",
            "\n",
            "Epoch: 89 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he was a great poet as well as a doctor <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను లదు <EOS> ఇకకడ ఉననయ <EOS> తసంద <EOS> చపలు ఎనన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను ఒక గపప కవ కదు మంచ వదయుడు కూడ <EOS> <PAD> \n",
            "\n",
            "loss=13.57001\n",
            "\n",
            "CHOSEN SAMPLE NO.: 7\n",
            "\n",
            "Epoch: 90 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he was a great poet as well as a doctor <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సకూలుక <EOS> రతర ననంక నమమను పనసళళు వదయురలు రతరక అంటన దగగర\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను ఒక గపప కవ కదు మంచ వదయుడు కూడ <EOS> <PAD> \n",
            "\n",
            "loss=13.244632\n",
            "\n",
            "CHOSEN SAMPLE NO.: 42\n",
            "\n",
            "Epoch: 91 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "can i make a phone call <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం లదు అకకడ భయపడడడు <EOS> పరశన <EOS> సదదంగ సందహ కఫ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఒక ఫన చసుకవచచ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.523893\n",
            "\n",
            "CHOSEN SAMPLE NO.: 32\n",
            "\n",
            "Epoch: 92 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "are you prepared to do this <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద నన అతను తలుస సతరలగ వదయుడు తలక ఇవవర దగుతను రంగుల\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద చయయడనక సదదంగ వుననవ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.201597\n",
            "\n",
            "CHOSEN SAMPLE NO.: 25\n",
            "\n",
            "Epoch: 93 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he is teaching spanish to the children <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం ఉండు కటటుకుంటూన <EOS> సకస చపతవన <EOS> అవకశం కవల ఎకకడ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతను పలలలక సపనష నరపుతుననడు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.040506\n",
            "\n",
            "CHOSEN SAMPLE NO.: 35\n",
            "\n",
            "Epoch: 94 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "where did you put my book <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ నదరపలదు సకూలుకు చయయమన అతడు వళళు సతరలగ వలుపల​ నమమను <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న పుసతకం ఎకకడ పటటవ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.878218\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 95 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మనం వుంద ఇంకమ టఫన సతరలగ సతరలగ <EOS> అంతట <EOS> అంగకరంచను\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.400656\n",
            "\n",
            "CHOSEN SAMPLE NO.: 62\n",
            "\n",
            "Epoch: 96 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what is in the box <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఎందుకు <EOS> <EOS> రజుల అంతట అనుమత ఒక <EOS> <EOS> పనలక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పటట ల ఏముంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.981944\n",
            "\n",
            "CHOSEN SAMPLE NO.: 5\n",
            "\n",
            "Epoch: 97 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i should have been able to do that by myself <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వళళు సతరలగ ఉననర కదు ఇపపుడు గదల చయయమన చూపుక <EOS> ఇపపుడు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న అంతట నన చయగలగ వుండలసంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.855583\n",
            "\n",
            "CHOSEN SAMPLE NO.: 53\n",
            "\n",
            "Epoch: 98 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the movie 's about to start <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ననంక వుననవ నరపసతుంద సహయం <EOS> <EOS> లదు సహయం <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం మదలు అవవబతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.822912\n",
            "\n",
            "CHOSEN SAMPLE NO.: 34\n",
            "\n",
            "Epoch: 99 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the baby is crying for milk <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మధయహనం ననూ రతర వదయుడు అనుమత వదయుడు మంచ లదు <EOS> పరశన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పసపప పల కసం ఏడుసతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.053049\n",
            "\n",
            "CHOSEN SAMPLE NO.: 45\n",
            "\n",
            "Epoch: 100 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not trust that guy <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను <EOS> ఈరజు <EOS> కదు <EOS> <EOS> పరశన ఎల పనసళలు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వడన నమమను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.811948\n",
            "\n",
            "CHOSEN SAMPLE NO.: 6\n",
            "\n",
            "Epoch: 101 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is going to take all afternoon and maybe more <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద పరశన సటషన <EOS> <EOS> <EOS> అంతట రండు <EOS> నరపసతుంద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మధయహనం మతతం లద ఇంక ఎకకువ సమయం పటటచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=12.2668705\n",
            "\n",
            "CHOSEN SAMPLE NO.: 31\n",
            "\n",
            "Epoch: 102 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you must speak in a loud voice <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు అకకడ ఎకకడ తయరు ఏడు తపపు మంచద అంతట ఏడు ఇద\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు గటటగ మటలడల <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=13.273905\n",
            "\n",
            "CHOSEN SAMPLE NO.: 47\n",
            "\n",
            "Epoch: 103 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we have not slept in days <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము లదు తయరు చయయమన మటలడంత బయలుదర లదు <EOS> సదరణంగ అంత\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము రజుల తరబడ నదరపలదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.714678\n",
            "\n",
            "CHOSEN SAMPLE NO.: 12\n",
            "\n",
            "Epoch: 104 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'm getting off at the next bus stop <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను దగగర వుంద కవ గుండపటు కవ అతను కదు ఎనన <PAD>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వచచ సటపు ల దగుతను <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.663434\n",
            "\n",
            "CHOSEN SAMPLE NO.: 45\n",
            "\n",
            "Epoch: 105 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not trust that guy <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను తయరు <EOS> తలుస <EOS> ననంక <EOS> దగుతను <EOS> సతరలగ\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వడన నమమను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.584286\n",
            "\n",
            "CHOSEN SAMPLE NO.: 1\n",
            "\n",
            "Epoch: 106 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is high time you left for school is not it <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తనన చయయడనక గలచర చయయడనక వుంద వుంద తలక వుంద ఎల చయయడనక\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సకూలుక బయలుదర సమయం అయయంద కద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.339836\n",
            "\n",
            "CHOSEN SAMPLE NO.: 9\n",
            "\n",
            "Epoch: 107 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not want to talk to you anymore <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఎవరు అడగడనక తరమసంద ఎల కూరచుంద <EOS> అతడు లదు సతరలగ <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననంక నత మటలడదలచుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.868582\n",
            "\n",
            "CHOSEN SAMPLE NO.: 53\n",
            "\n",
            "Epoch: 108 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the movie 's about to start <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ మములుగ ఇపపుడు <EOS> అంతట కుకకలు గపప వళళు అయపయంద బయలుదర\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం మదలు అవవబతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.285521\n",
            "\n",
            "CHOSEN SAMPLE NO.: 54\n",
            "\n",
            "Epoch: 109 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "some of the dogs are alive <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఉంటను <EOS> <EOS> <EOS> <EOS> దగగర <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కనన కుకకలు బరతక ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.24113\n",
            "\n",
            "CHOSEN SAMPLE NO.: 28\n",
            "\n",
            "Epoch: 110 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "how about going out to eat tonight <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ ఉతకనపపుడు <EOS> అంగకరంచను <EOS> <EOS> <EOS> గుండపటు <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ రతర బయటక వళల తందమ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.448713\n",
            "\n",
            "CHOSEN SAMPLE NO.: 58\n",
            "\n",
            "Epoch: 111 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "this made me very sad <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద అంత పటటచచు ఒక తలయదు చయయలనన వుననయ నమమదగన <EOS> తలయదు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద ననను చల బధ పటటంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.240739\n",
            "\n",
            "CHOSEN SAMPLE NO.: 41\n",
            "\n",
            "Epoch: 112 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "my sister is crazy about tennis <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననంక వుననవ <EOS> కవ పటటవ <EOS> ఇకకడ తలయదు <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మ అకకక టననసంట పచచ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=12.072345\n",
            "\n",
            "CHOSEN SAMPLE NO.: 60\n",
            "\n",
            "Epoch: 113 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i suggest you do that <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను తను వుండలసంద గదల <EOS> ఒకట <EOS> ఉండు <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు అద చయయమన న సలహ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.482763\n",
            "\n",
            "CHOSEN SAMPLE NO.: 43\n",
            "\n",
            "Epoch: 114 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we need money to do anything <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను అంత మనక పనసళలు సతరలగ అకకడక మదలుపటటను <EOS> మదలుపటటను <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఏం చయయలనన మనక డబబులు కవల <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.732572\n",
            "\n",
            "CHOSEN SAMPLE NO.: 4\n",
            "\n",
            "Epoch: 115 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i would not go there today if i were you <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను నువవత తటటంద <EOS> <EOS> పరపటు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను నువవత ఈరజు అకకడక వళళను <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.447704\n",
            "\n",
            "CHOSEN SAMPLE NO.: 15\n",
            "\n",
            "Epoch: 116 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "a cat came out from under the desk <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు రజుల <EOS> ఇపపుడు ఎకకడ <EOS> <EOS> <EOS> <EOS> ఏమన\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఒక పలల డసక కంద నుండ బయటకు వచచంద <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=11.907166\n",
            "\n",
            "CHOSEN SAMPLE NO.: 4\n",
            "\n",
            "Epoch: 117 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i would not go there today if i were you <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను టననసంట రంగుల ఏమన <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను నువవత ఈరజు అకకడక వళళను <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.820192\n",
            "\n",
            "CHOSEN SAMPLE NO.: 32\n",
            "\n",
            "Epoch: 118 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "are you prepared to do this <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద బయటక తలక సమయం అడగంద <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద చయయడనక సదదంగ వుననవ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.56693\n",
            "\n",
            "CHOSEN SAMPLE NO.: 19\n",
            "\n",
            "Epoch: 119 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she asked me how many languages i spoke <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఇపపుడు <EOS> లదు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఎనన భషలు మటలడుతనన తను అడగంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.576151\n",
            "\n",
            "CHOSEN SAMPLE NO.: 15\n",
            "\n",
            "Epoch: 120 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "a cat came out from under the desk <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చూపుక సటప సలహ నదరపలదు ఇంక దగుతను <EOS> పరపటు <EOS> పరపటు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఒక పలల డసక కంద నుండ బయటకు వచచంద <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=11.001973\n",
            "\n",
            "CHOSEN SAMPLE NO.: 22\n",
            "\n",
            "Epoch: 121 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do you usually eat breakfast before seven <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఇంక కదు దరల ఇంక <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు మములుగ ఏడు కు ముంద టఫన చసతవ <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=10.648509\n",
            "\n",
            "CHOSEN SAMPLE NO.: 12\n",
            "\n",
            "Epoch: 122 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'm getting off at the next bus stop <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను అంతట పలలలక మటలడంత <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వచచ సటపు ల దగుతను <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.423747\n",
            "\n",
            "CHOSEN SAMPLE NO.: 54\n",
            "\n",
            "Epoch: 123 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "some of the dogs are alive <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు తనన ఒక ల అంటన <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కనన కుకకలు బరతక ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.965663\n",
            "\n",
            "CHOSEN SAMPLE NO.: 11\n",
            "\n",
            "Epoch: 124 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "these socks do not stretch when you wash them <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ చపపు తసంద <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ సకస ఉతకనపపుడు సగవు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.977125\n",
            "\n",
            "CHOSEN SAMPLE NO.: 35\n",
            "\n",
            "Epoch: 125 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "where did you put my book <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న ఈసరక ఇంటక <EOS> లదు <EOS> ల <EOS> <EOS> అంగకరంచను\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న పుసతకం ఎకకడ పటటవ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.963375\n",
            "\n",
            "CHOSEN SAMPLE NO.: 24\n",
            "\n",
            "Epoch: 126 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she refuses to say more about it <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మ గుండపటు గలచర <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఇంక ఎకకువ చపపడనక ఒపపుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.905615\n",
            "\n",
            "CHOSEN SAMPLE NO.: 54\n",
            "\n",
            "Epoch: 127 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "some of the dogs are alive <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను <PAD> తను రంగుల <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కనన కుకకలు బరతక ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=11.615909\n",
            "\n",
            "CHOSEN SAMPLE NO.: 3\n",
            "\n",
            "Epoch: 128 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do you know the reason why she is so angry <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు నత తలవతకకువ ననను టఫన సహయం అంతట టఫన <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను ఎందుకు అంత కపంగ ఉంద నకు తలుస <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=11.35028\n",
            "\n",
            "CHOSEN SAMPLE NO.: 6\n",
            "\n",
            "Epoch: 129 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is going to take all afternoon and maybe more <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మధయహనం ఈత ఇద వళళ ఇపపుడు అవుతుంద <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మధయహనం మతతం లద ఇంక ఎకకువ సమయం పటటచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=10.815996\n",
            "\n",
            "CHOSEN SAMPLE NO.: 16\n",
            "\n",
            "Epoch: 130 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "sitting down all day is bad for you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు చతులక వలుపల​ రజువర తలక ఏం <EOS> <EOS> సలహ <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రజంత కూరచవడం నకు మంచద కదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.706658\n",
            "\n",
            "CHOSEN SAMPLE NO.: 13\n",
            "\n",
            "Epoch: 131 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i know you are going to say no <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు నదర మంచద చయగలగ ఇంక <EOS> తకడు <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు నువవు ఒదదన చపతవన తలుసు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.722165\n",
            "\n",
            "CHOSEN SAMPLE NO.: 13\n",
            "\n",
            "Epoch: 132 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i know you are going to say no <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద బరతక <PAD> టమ <EOS> పటటచచు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు నువవు ఒదదన చపతవన తలుసు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.161171\n",
            "\n",
            "CHOSEN SAMPLE NO.: 40\n",
            "\n",
            "Epoch: 133 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "tell me about your daily life <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న ఎవరు చపతుననవు తయరు సతరలగ పడుగు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న రజువర జవతం గురంచ చపపు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.771187\n",
            "\n",
            "CHOSEN SAMPLE NO.: 21\n",
            "\n",
            "Epoch: 134 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "there is no reason to be afraid <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన సతరలగ ఎకకడ ఉతకనపపుడు చపలు <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన కరణం ఏమ లదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.298238\n",
            "\n",
            "CHOSEN SAMPLE NO.: 47\n",
            "\n",
            "Epoch: 135 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we have not slept in days <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము సదదంగ నరపసతుంద వంటల కవలన <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మము రజుల తరబడ నదరపలదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.485974\n",
            "\n",
            "CHOSEN SAMPLE NO.: 21\n",
            "\n",
            "Epoch: 136 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "there is no reason to be afraid <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ తముక కదు <EOS> <EOS> ఉననయ <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన కరణం ఏమ లదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.251172\n",
            "\n",
            "CHOSEN SAMPLE NO.: 32\n",
            "\n",
            "Epoch: 137 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "are you prepared to do this <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద మటలడుతనన పనసళలు కవలన <EOS> మటలడుతనన <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద చయయడనక సదదంగ వుననవ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.343341\n",
            "\n",
            "CHOSEN SAMPLE NO.: 54\n",
            "\n",
            "Epoch: 138 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "some of the dogs are alive <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఒపపుకవటలదు లదు వళళన కవ ఏం ఒపపుకవటలదు <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "కనన కుకకలు బరతక ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.9542265\n",
            "\n",
            "CHOSEN SAMPLE NO.: 62\n",
            "\n",
            "Epoch: 139 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what is in the box <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పటట తపపు తపపు రండు బయటక <EOS> అవుతుంద <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పటట ల ఏముంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.672949\n",
            "\n",
            "CHOSEN SAMPLE NO.: 44\n",
            "\n",
            "Epoch: 140 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "these pencils are the same color <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ వచచంద ఇషటం తలవతకకువ <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ పనసళలు ఒక రంగుల ఉననయ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.939717\n",
            "\n",
            "CHOSEN SAMPLE NO.: 1\n",
            "\n",
            "Epoch: 141 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is high time you left for school is not it <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సకూలుక అతను ఇపపుడు <EOS> అవుతుంద నవససతుననవ అడగడనక బయలుదర <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "సకూలుక బయలుదర సమయం అయయంద కద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.960701\n",
            "\n",
            "CHOSEN SAMPLE NO.: 49\n",
            "\n",
            "Epoch: 142 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "we had better leave her alone <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను భయపడడడు అకకడ ఉననటలు రండు రంగుల <EOS> రండు <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తనన ఒంటరగ వదలయయడం మంచద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.439693\n",
            "\n",
            "CHOSEN SAMPLE NO.: 7\n",
            "\n",
            "Epoch: 143 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he was a great poet as well as a doctor <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను సమయం ఉననటలు సమయం తసుకవల పనసళళు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆటను ఒక గపప కవ కదు మంచ వదయుడు కూడ <EOS> <PAD> \n",
            "\n",
            "loss=10.488498\n",
            "\n",
            "CHOSEN SAMPLE NO.: 50\n",
            "\n",
            "Epoch: 144 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is not all your fault <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత నుండ చయయడనక డసక <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత న తపపు కదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.022692\n",
            "\n",
            "CHOSEN SAMPLE NO.: 24\n",
            "\n",
            "Epoch: 145 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she refuses to say more about it <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ సమయం సమయం <EOS> పటటంద సదదంగ <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఇంక ఎకకువ చపపడనక ఒపపుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=10.012466\n",
            "\n",
            "CHOSEN SAMPLE NO.: 5\n",
            "\n",
            "Epoch: 146 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i should have been able to do that by myself <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న మంచ వుండలసంద మంచ <EOS> నరపుతుననడు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న అంతట నన చయగలగ వుండలసంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.978274\n",
            "\n",
            "CHOSEN SAMPLE NO.: 18\n",
            "\n",
            "Epoch: 147 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do not hesitate to ask me for help <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననను తయరు ఏం కద <EOS> మనసుక వచచంద <EOS> మటలడుతనన <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననను సహయం అడగడనక ఏం సందహ పడదదు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.979073\n",
            "\n",
            "CHOSEN SAMPLE NO.: 4\n",
            "\n",
            "Epoch: 148 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i would not go there today if i were you <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను మటలడుతనన నదరపలదు ఉతకనపపుడు కవలన <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను నువవత ఈరజు అకకడక వళళను <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.943026\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 149 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన అవకశం నలమ ఏం <EOS> నుండ <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.739423\n",
            "\n",
            "CHOSEN SAMPLE NO.: 34\n",
            "\n",
            "Epoch: 150 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the baby is crying for milk <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పసపప పచచ నరపుతుననడు నరపుతుననడు రజువర తలుస <EOS> <EOS> <EOS> వదయుడు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పసపప పల కసం ఏడుసతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.582436\n",
            "\n",
            "CHOSEN SAMPLE NO.: 55\n",
            "\n",
            "Epoch: 151 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "that was not easy you know <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద ఇంకమ సతరలగ తలుస సపనష కదు వచచంద <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద అంత తలక కదు తలుస <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.818161\n",
            "\n",
            "CHOSEN SAMPLE NO.: 31\n",
            "\n",
            "Epoch: 152 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you must speak in a loud voice <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఆ వుంద దగగర తరమసంద <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు గటటగ మటలడల <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.538879\n",
            "\n",
            "CHOSEN SAMPLE NO.: 39\n",
            "\n",
            "Epoch: 153 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you must stay where you are <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు సమయం ఇంటక <EOS> <EOS> <EOS> <EOS> సపనష <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఎకకడ ఉననవ అకకడ ఉండు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.18197\n",
            "\n",
            "CHOSEN SAMPLE NO.: 46\n",
            "\n",
            "Epoch: 154 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "this is a pretty stupid question <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద చయయడనక వదయుడు కుకకలు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద చల తలవతకకువ పరశన <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.406533\n",
            "\n",
            "CHOSEN SAMPLE NO.: 53\n",
            "\n",
            "Epoch: 155 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the movie 's about to start <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం సకూలుకు అంటన రజువర సలహ <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం మదలు అవవబతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.621791\n",
            "\n",
            "CHOSEN SAMPLE NO.: 26\n",
            "\n",
            "Epoch: 156 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she was on her way to school <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను చయయడనక సటప <EOS> సకస తను <EOS> <EOS> దరల లదు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను సకూలుకు వళళ దరల వుంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.746812\n",
            "\n",
            "CHOSEN SAMPLE NO.: 32\n",
            "\n",
            "Epoch: 157 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "are you prepared to do this <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద వలుపల​ పలలలక తకడు వుంద అవుతుంద <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇద చయయడనక సదదంగ వుననవ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.504298\n",
            "\n",
            "CHOSEN SAMPLE NO.: 52\n",
            "\n",
            "Epoch: 158 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she is not at home now <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను తలుపు నకు అంగకరంచను తలయదు టఫన తను సలహ <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తను ఇపపుడు ఇంట దగగర లదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.620005\n",
            "\n",
            "CHOSEN SAMPLE NO.: 55\n",
            "\n",
            "Epoch: 159 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "that was not easy you know <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద చపపు ఉంద ఏడు అతను కదు <EOS> సలహ <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద అంత తలక కదు తలుస <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.150029\n",
            "\n",
            "CHOSEN SAMPLE NO.: 22\n",
            "\n",
            "Epoch: 160 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do you usually eat breakfast before seven <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు పనలక నకు కదు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు మములుగ ఏడు కు ముంద టఫన చసతవ <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=9.442065\n",
            "\n",
            "CHOSEN SAMPLE NO.: 57\n",
            "\n",
            "Epoch: 161 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she scared the cat away <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ అంత మమడపండలు ఒపపుకవటలదు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ పలలన భయపటట తరమసంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.971969\n",
            "\n",
            "CHOSEN SAMPLE NO.: 34\n",
            "\n",
            "Epoch: 162 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the baby is crying for milk <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పసపప ఉంద డసక కషమంచు <EOS> రతర <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పసపప పల కసం ఏడుసతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.818239\n",
            "\n",
            "CHOSEN SAMPLE NO.: 6\n",
            "\n",
            "Epoch: 163 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is going to take all afternoon and maybe more <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మధయహనం అంతట వుంద ల <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "మధయహనం మతతం లద ఇంక ఎకకువ సమయం పటటచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=9.084997\n",
            "\n",
            "CHOSEN SAMPLE NO.: 0\n",
            "\n",
            "Epoch: 164 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he has two pencils one is long and the other one is short <EOS> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన పడుగు రండు సదదంగ ఉంటను పడుగు వుననయ టఫన ఒకట పనసళళు\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన దగగర రండు పనసళళు వుననయ ఒకట పడుగు ఇంకట పటట <EOS> \n",
            "\n",
            "loss=8.943407\n",
            "\n",
            "CHOSEN SAMPLE NO.: 24\n",
            "\n",
            "Epoch: 165 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she refuses to say more about it <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఫన లదు పటటచచు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ ఇంక ఎకకువ చపపడనక ఒపపుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.84279\n",
            "\n",
            "CHOSEN SAMPLE NO.: 40\n",
            "\n",
            "Epoch: 166 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "tell me about your daily life <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న వచచ గురంచ చపపు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న రజువర జవతం గురంచ చపపు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.860262\n",
            "\n",
            "CHOSEN SAMPLE NO.: 42\n",
            "\n",
            "Epoch: 167 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "can i make a phone call <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఏడు తయరు రతర <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఒక ఫన చసుకవచచ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.720334\n",
            "\n",
            "CHOSEN SAMPLE NO.: 5\n",
            "\n",
            "Epoch: 168 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i should have been able to do that by myself <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న బసకట పడదదు అకకడ ఇపపుడు <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న అంతట నన చయగలగ వుండలసంద <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.792279\n",
            "\n",
            "CHOSEN SAMPLE NO.: 50\n",
            "\n",
            "Epoch: 169 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "it is not all your fault <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత మదలు సహయం ఇంకం <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఇదంత న తపపు కదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.352026\n",
            "\n",
            "CHOSEN SAMPLE NO.: 37\n",
            "\n",
            "Epoch: 170 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not know who won <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఎవరు పలల మదలుపటటను చయయమన ఆమక తయరు రకతనన <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఎవరు గలచర నకు తలయదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=9.274668\n",
            "\n",
            "CHOSEN SAMPLE NO.: 21\n",
            "\n",
            "Epoch: 171 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "there is no reason to be afraid <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన ఉంద బల నజంగ <EOS> <EOS> ఉననయ <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన కరణం ఏమ లదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.56763\n",
            "\n",
            "CHOSEN SAMPLE NO.: 27\n",
            "\n",
            "Epoch: 172 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not know where they are <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వళళు అతను పనసళలు తలుపు చపపు లదు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వళళు ఎకకడ ఉననర నకు తలయదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.909993\n",
            "\n",
            "CHOSEN SAMPLE NO.: 11\n",
            "\n",
            "Epoch: 173 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "these socks do not stretch when you wash them <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ మటలడుతనన దరల ఒపపుకవటలదు <EOS> తరమసంద <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఈ సకస ఉతకనపపుడు సగవు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.602931\n",
            "\n",
            "CHOSEN SAMPLE NO.: 60\n",
            "\n",
            "Epoch: 174 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i suggest you do that <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఒక గలచర తసంద తసంద ఇంట కవ <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు అద చయయమన న సలహ <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.298474\n",
            "\n",
            "CHOSEN SAMPLE NO.: 33\n",
            "\n",
            "Epoch: 175 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "out of sight out of mind <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చూపుక చయయడనక చయయడనక కటటుకుంటూన సలహ <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చూపుక వలుపల​ మనసుక వలుపల​ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.351072\n",
            "\n",
            "CHOSEN SAMPLE NO.: 21\n",
            "\n",
            "Epoch: 176 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "there is no reason to be afraid <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన లదు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "భయపడలసన కరణం ఏమ లదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.447447\n",
            "\n",
            "CHOSEN SAMPLE NO.: 53\n",
            "\n",
            "Epoch: 177 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "the movie 's about to start <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం మదలు వసతరలను చపలు <EOS> చపలు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చలనచతరం మదలు అవవబతుంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.177499\n",
            "\n",
            "CHOSEN SAMPLE NO.: 0\n",
            "\n",
            "Epoch: 178 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he has two pencils one is long and the other one is short <EOS> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన వుననయ పనసళళు పనసళళు పనసళళు వుననయ ఒకట ఒకట ఒకట ఒకట\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన దగగర రండు పనసళళు వుననయ ఒకట పడుగు ఇంకట పటట <EOS> \n",
            "\n",
            "loss=8.292189\n",
            "\n",
            "CHOSEN SAMPLE NO.: 31\n",
            "\n",
            "Epoch: 179 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you must speak in a loud voice <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు రలదు మటలడల ఇపపుడు చపపు <EOS> తలక <EOS> భషలు <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు గటటగ మటలడల <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.984665\n",
            "\n",
            "CHOSEN SAMPLE NO.: 57\n",
            "\n",
            "Epoch: 180 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "she scared the cat away <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ వళళను అవకశం <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ఆవడ పలలన భయపటట తరమసంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.400886\n",
            "\n",
            "CHOSEN SAMPLE NO.: 4\n",
            "\n",
            "Epoch: 181 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i would not go there today if i were you <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను అనుమత సమయం నమమదగన <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను నువవత ఈరజు అకకడక వళళను <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.319804\n",
            "\n",
            "CHOSEN SAMPLE NO.: 51\n",
            "\n",
            "Epoch: 182 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "you may go swimming or fishing <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు పలల అవకశం వచచంద నరపుతుననడు <EOS> వళళ <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఈత కటటడనక లక చపలు పటటడనక వళళచచు <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=8.24501\n",
            "\n",
            "CHOSEN SAMPLE NO.: 2\n",
            "\n",
            "Epoch: 183 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 've made a mistake though i did not intend to <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన నన అంతట కకపయన <EOS> పటటచచు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన పరపటు చసను కవలన కకపయన <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.020297\n",
            "\n",
            "CHOSEN SAMPLE NO.: 2\n",
            "\n",
            "Epoch: 184 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 've made a mistake though i did not intend to <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన చసను ఏం <EOS> లదు <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన పరపటు చసను కవలన కకపయన <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.6376357\n",
            "\n",
            "CHOSEN SAMPLE NO.: 62\n",
            "\n",
            "Epoch: 185 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what is in the box <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పటట సతరలగ తందమ <EOS> ఇపపుడు <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "పటట ల ఏముంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.625018\n",
            "\n",
            "CHOSEN SAMPLE NO.: 29\n",
            "\n",
            "Epoch: 186 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "have you ever had a heart attack <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు మములుగ మములుగ మములుగ రండు పరశన <PAD> కదు <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు ఎపపుడన గుండపటు వచచంద <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.985223\n",
            "\n",
            "CHOSEN SAMPLE NO.: 9\n",
            "\n",
            "Epoch: 187 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not want to talk to you anymore <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననంక అంతట మంచద సంతషంగ అకకడక <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "ననంక నత మటలడదలచుకవటలదు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=8.1405\n",
            "\n",
            "CHOSEN SAMPLE NO.: 30\n",
            "\n",
            "Epoch: 188 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he washed the blood off his hands <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు గలచర అతను కడగసడు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు తన చతులక అంటన రకతనన కడగసడు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.958524\n",
            "\n",
            "CHOSEN SAMPLE NO.: 27\n",
            "\n",
            "Epoch: 189 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not know where they are <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వళళు కకపయన ఈరజు కకపయన <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "వళళు ఎకకడ ఉననర నకు తలయదు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.558448\n",
            "\n",
            "CHOSEN SAMPLE NO.: 14\n",
            "\n",
            "Epoch: 190 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "with her heart pounding she opened the door <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన మంచ చపపు నరపుతుననడు రకతనన లదు లదు లదు <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన గుండ కటటుకుంటూన ఆమ తలుపు తసంద <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.3528748\n",
            "\n",
            "CHOSEN SAMPLE NO.: 45\n",
            "\n",
            "Epoch: 191 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i do not trust that guy <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఒక కవలన అడగంద లదు <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వడన నమమను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=6.914079\n",
            "\n",
            "CHOSEN SAMPLE NO.: 59\n",
            "\n",
            "Epoch: 192 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "what else i can lose <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను వడన అడగంద ఇంకం నమమను <EOS> ఇంకం <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నను ఇంకం కలపతను <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.180735\n",
            "\n",
            "CHOSEN SAMPLE NO.: 2\n",
            "\n",
            "Epoch: 193 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 've made a mistake though i did not intend to <EOS> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన అంగకరంచను కవలన అర <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నన పరపటు చసను కవలన కకపయన <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.2981997\n",
            "\n",
            "CHOSEN SAMPLE NO.: 40\n",
            "\n",
            "Epoch: 194 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "tell me about your daily life <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న ఎకకడ ఎకకడ గురంచ <EOS> ఏడు <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "న రజువర జవతం గురంచ చపపు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.049514\n",
            "\n",
            "CHOSEN SAMPLE NO.: 0\n",
            "\n",
            "Epoch: 195 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he has two pencils one is long and the other one is short <EOS> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన రండు ఇంకట చసను చసను చసను <EOS> రకతనన వుననయ ఒకట\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "తన దగగర రండు పనసళళు వుననయ ఒకట పడుగు ఇంకట పటట <EOS> \n",
            "\n",
            "loss=7.2822447\n",
            "\n",
            "CHOSEN SAMPLE NO.: 13\n",
            "\n",
            "Epoch: 196 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i know you are going to say no <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు వలుపల​ గలచర సపనష సపనష <EOS> అంటన <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నకు నువవు ఒదదన చపతవన తలుసు <EOS> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=6.5017824\n",
            "\n",
            "CHOSEN SAMPLE NO.: 38\n",
            "\n",
            "Epoch: 197 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "he dressed up as a woman <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు అతడు సకూలుకు వసతరలను ఏం టఫన వచచంద <EOS> వచచంద <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అతడు సతరలగ వసతరలను ధరంచడు <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=6.9392595\n",
            "\n",
            "CHOSEN SAMPLE NO.: 33\n",
            "\n",
            "Epoch: 198 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "out of sight out of mind <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "రలదు వలుపల​ ఏమ వలుపల​ కు <EOS> <EOS> వలుపల​ <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "చూపుక వలుపల​ మనసుక వలుపల​ <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=7.1539984\n",
            "\n",
            "CHOSEN SAMPLE NO.: 22\n",
            "\n",
            "Epoch: 199 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "do you usually eat breakfast before seven <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు ఎకకడ చపతవన పరశన <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "నువవు మములుగ ఏడు కు ముంద టఫన చసతవ <EOS> <PAD> <PAD> \n",
            "\n",
            "loss=7.017418\n",
            "\n",
            "CHOSEN SAMPLE NO.: 20\n",
            "\n",
            "Epoch: 200 Iteration: 1\n",
            "\n",
            "SAMPLE TEXT:\n",
            "i 'm not allowed to do that here <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "\n",
            "PREDICTED TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద నువవత నకు కదు <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "ACTUAL TRANSLATION OF THE SAMPLE:\n",
            "\n",
            "అద ఇకకడ చయయడనక నకు అనుమత లదు <EOS> <PAD> <PAD> <PAD> \n",
            "\n",
            "loss=6.8361497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVx-0cYCV4Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2vec(word):\n",
        "    if word in vocab_eng:\n",
        "        return np_embedding_eng[vocab_eng.index(word)]\n",
        "    else:\n",
        "        return np_embedding_eng[vocab_eng.index('<PAD>')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQwrDAwNh3YK",
        "colab_type": "text"
      },
      "source": [
        "# **Prediction:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn78L1RDV-Ql",
        "colab_type": "code",
        "outputId": "6de93302-4ba9-49e1-aa13-c7084c74367c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "with tf.Session() as sess: # Begin session\n",
        "    \n",
        "    print('Loading pre-trained weights for the model...')\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, 'Model_Backup/translation_model.ckpt')\n",
        "    sess.run(tf.global_variables())\n",
        "    print('\\nRESTORATION COMPLETE\\n')\n",
        "    \n",
        "    \n",
        "    test = ['who','are','you'] # Enter tokenized text here\n",
        "    test = list(map(word2vec,test))\n",
        "    test = np.asarray(test,np.float32)\n",
        "    test = test.reshape((1,test.shape[0],test.shape[1]))\n",
        "    \n",
        "    input_seq_len = test.shape[0]\n",
        "    pe_in = positional_encoding(input_seq_len,word_vec_dim)\n",
        "    pe_in = pe_in.reshape((1,input_seq_len,word_vec_dim))\n",
        "    test_pe = test+pe_in\n",
        "    \n",
        "    output_seq_len = int(input_seq_len+20)\n",
        "    illegal_position_masks = generate_masks_for_illegal_positions(output_seq_len)\n",
        "    pe_out = positional_encoding(output_seq_len,word_vec_dim) \n",
        "    pe_out = pe_out.reshape((output_seq_len,1,word_vec_dim))\n",
        "        \n",
        "    out = sess.run(softmax_output,\n",
        "                          feed_dict={x: test_pe,\n",
        "                                     y: np.zeros((1,1),np.int32), \n",
        "                                     # y value doesn't matter here.\n",
        "                                     # feeding y, because the network graph requires y.\n",
        "                                     # but its value won't actually be used in this case. \n",
        "                                     keep_prob: 1,\n",
        "                                     output_len: output_seq_len,\n",
        "                                     tf_pe_out: pe_out,\n",
        "                                     tf_illegal_position_masks: illegal_position_masks,\n",
        "                                     teacher_forcing: False\n",
        "                                    })\n",
        "\n",
        "    for array in out[0]:\n",
        "        if vocab_beng[np.argmax(array)] != '<EOS>':\n",
        "            print(vocab_beng[np.argmax(array)],end=' ')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pre-trained weights for the model...\n",
            "INFO:tensorflow:Restoring parameters from Model_Backup/translation_model.ckpt\n",
            "\n",
            "RESTORATION COMPLETE\n",
            "\n",
            "నువవు నువవు "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}